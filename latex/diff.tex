% Copyright (C) 2014-2017 by Thomas Auzinger <thomas@auzinger.name>
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL example.tex       Thu May 23 12:23:41 2019
%DIF ADD example_new.tex   Thu May 23 12:01:19 2019

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
%\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.

\usepackage{morewrites}

\usepackage[newfloat]{minted}
\usepackage{caption}

\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
\usepgfplotslibrary{statistics}

\newenvironment{code}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Listing}

%\usepackage{listings}
%\lstset{
%	language=bash,
%	basicstyle=\footnotesize, 
%	breakatwhitespace=false,
%	escapeinside={\%*}{*)}
%}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Bernhard Gößwein} % The author name without titles.
%DIF 48c48
%DIF < \newcommand{\thesistitle}{Designing a Framework gaining Repeatability for the OpenEO platform} % The title of the thesis. The English version should be used, if it exists.
%DIF -------
\newcommand{\thesistitle}{Designing a Framework gaining Repeatability for the openEO platform} % The title of the thesis. The English version should be used, if it exists. %DIF > 
%DIF -------

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {Subject},              % The document's subject in the document properties (optional).
    pdfkeywords     = {a, list, of, keywords} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{}{male}
\setadvisor{Ao.Univ.Prof. Dipl.-Ing. Dr.techn.}{Andreas Rauber}{}{male}

% For bachelor and master theses:
%\setfirstassistant{Ao.Univ.Prof. Dipl.-Ing. Dr.techn.}{Andreas Rauber}{}{male}
\setfirstassistant{Dr.}{Tomasz Miksa}{}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setaddress{Vorderer Ödhof 1, 3062 Kirchstetten}
\setregnumber{01026884}
\setdate{21}{12}{2018} % Set date with 3 arguments: {day}{month}{year}.
%DIF 92c92
%DIF < \settitle{\thesistitle}{Entwurf eines Frameworks zur Unterstützung von Reproduzierbarkeit für die OpenEO Plattform} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
%DIF -------
\settitle{\thesistitle}{Entwurf eines Frameworks zur Unterstützung von Reproduzierbarkeit für die openEO Plattform} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle. %DIF > 
%DIF -------
\setsubtitle{}{} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
%\setthesis{bachelor}
%
% Master:
\setthesis{master}
\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Software Engineering and Internet Computing}{Software Engineering and Internet Computing} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}
\newcounter{tmp@cnt}

\newcommand*\combine[1][2]{%
	\refstepcounter{enumi}
	\setcounter{tmp@cnt}{\value{enumi}}
	\addtocounter{enumi}{#1-1}
	\item[\thetmp@cnt--\theenumi\@labelpunc]}

\newcolumntype{L}[1]{>{\centering\arraybackslash}l{#1}}

\newacronym{api}{API}{Application Programming Interface}
\newacronym{gee}{GEE}{Google Earth Engine}
\newacronym{eodc}{EODC}{Earth Observation Data Centre}
\newacronym{soa}{SOA}{Service Oriented Architecture}
\newacronym{esa}{ESA}{European Space Agency}
\newacronym{ndvi}{NDVI}{Normalized Difference Vegetation Index}
\newacronym{pid}{PID}{Persistent Identifier}
%DIF 133c133
%DIF < \newacronym{openeo}{OpenEO}{Open Source Earth Observation Project}
%DIF -------
\newacronym{openeo}{openEO}{Open Source Earth Observation Project} %DIF > 
%DIF -------
\newacronym{rda}{RDA}{Research Data Alliance}
\newacronym{primad}{PRIMAD}{\textbf{P}latform \textbf{R}esearch \textbf{O}bjectives \textbf{I}mplementation \textbf{M}ethods \textbf{A}ctors \textbf{D}ata model}
\newacronym{wgdc}{WGDC}{Working Group on Data Citation}
\newacronym{vzj}{VZJ}{Vadose Zone Journal}
\newacronym{rr}{RR}{Reproducible Research}
\newacronym{gpf}{GPF}{Geoscience Paper of the Future}
\newacronym{ccca}{CCCA}{Climate Change Centre Austria}
\newacronym{netcdf}{NetCDF}{Network Common Data Form}
\newacronym{geotiff}{GeoTiff}{Georeferenced Tagged Image File Format}
\newacronym{tds}{TDS}{Thredds Data Server}
\newacronym{xml}{XML}{Extensible Markup Language}
\newacronym{geobia}{GEOBIA}{Geographic Object-Based Image Analysis}
\newacronym{obia}{OBIA}{Object-Based Image Analysis}
\newacronym{vcs}{VCS}{Version Control Systems}
\newacronym{sha}{SHA}{Secure Hash Algorithm}
\newacronym{rest}{REST}{Representational State Transfer}
\newacronym{json}{JSON}{JavaScript Object Notation}
\newacronym{ogc}{OGC}{Open Geospatial Consortium}
\newacronym{csw}{CSW}{Catalogue Service for the Web}
\newacronym{udf}{UDF}{User Defined Functions}
\newacronym{eo}{EO}{Earth Observation}
\newacronym{cli}{CLI}{Command Line Interface}
\newacronym{cpu}{CPU}{Central Processing Unit}
\newacronym{gpu}{GPU}{Graphics Processing Unit}
\newacronym{ram}{RAM}{Random Access Memory}
\newacronym{os}{OS}{Operating System}

%DIF 161c161
%DIF < \newglossaryentry{back end}
%DIF -------
\newglossaryentry{backend} %DIF > 
%DIF -------
{
	name={backend},
	description={A web service provider, capable of processing and providing geoscientific data.}
}

\newglossaryentry{job}
{
	name={job},
%DIF 170c170
%DIF < 	description={Definition of an execution on a back end, including the input data}
%DIF -------
	description={Definition of an execution on a backend, including the input data} %DIF > 
%DIF -------
}

\newglossaryentry{process}
{
	name={process},
	description={Algorithm that gets executed over earth observation data. May use the output of another process as input data.}
}

\newglossaryentry{processgraph}
{
	name={process graph},
%DIF 182c182
%DIF < 	description={OpenEO definition of a job in a JSON format.}
%DIF -------
	description={openEO definition of a job in a JSON format.} %DIF > 
%DIF -------
}
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
%DIF LISTINGS PREAMBLE %DIF PREAMBLE
\RequirePackage{listings} %DIF PREAMBLE
\RequirePackage{color} %DIF PREAMBLE
\lstdefinelanguage{DIFcode}{ %DIF PREAMBLE
%DIF DIFCODE_UNDERLINE %DIF PREAMBLE
  moredelim=[il][\color{red}\sout]{\%DIF\ <\ }, %DIF PREAMBLE
  moredelim=[il][\color{blue}\uwave]{\%DIF\ >\ } %DIF PREAMBLE
} %DIF PREAMBLE
\lstdefinestyle{DIFverbatimstyle}{ %DIF PREAMBLE
	language=DIFcode, %DIF PREAMBLE
	basicstyle=\ttfamily, %DIF PREAMBLE
	columns=fullflexible, %DIF PREAMBLE
	keepspaces=true %DIF PREAMBLE
} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim}{\lstset{style=DIFverbatimstyle}}{} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim*}{\lstset{style=DIFverbatimstyle,showspaces=true}}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

\frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to
%  http://www.informatik.tuwien.ac.at/dekanat

\addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
\addtitlepage{english} % English title page.
\addstatementpage

%\begin{danksagung*}


%\end{danksagung*}

\begin{acknowledgements*}
To my girlfriend Viola: Thank you very much for helping me through the stressful days of working on the thesis and for providing breaks and diversions when I needed them.\\ \\
To my family: Thank you for supporting me through my whole studying time and for motivating me to go on with the thesis. \\ \\
To my colleagues of the Remote sensing research group: Thank you for patiently waiting for me to finish my studies and letting me work on a thesis within the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project. \\ \\
To my colleagues at EODC: Thank you for providing me all resources I requested and letting me implement the solution on your system. \\ \\
Last but not least to my supervisors Andreas Rauber and Tomas Miksa for always quickly replying on questions and providing me with constructive feedback. 
\end{acknowledgements*}

\begin{kurzfassung}
Publikationen in den Bereichen der Geodäsie und Geoinformation sind oft, aus diversen Gründen, nicht reproduzierbar. Daher fand in den letzten Jahren ein Umdenken in Bezug auf reproduzierbare Publikationen in den computergestützten Geowissenschaften statt. Die Problematik für Wissenschaftler dieser Forschungsbereiche ist die große Anzahl an unterschiedlichen Anbietern, fehlende Datenversionierung und intransparente Anbieter Services. Existierende Literatur beschränkt sich auf konkrete Anbieter und Forschungsbereiche. Dabei wird der Fokus mehr auf die Ausführung des selben Auftrages als auf Reproduzierbarkeit gelegt. Um diese zu ermöglichen ist es notwendig die Daten und die Bearbeitung dieser zu identifizieren. Dabei fehlt es der Geodäsie und Geoinformation an einem allgemeinen Konzept dies sowohl für Wissenschaftler als auch für Anbieter zu ermöglichen. Das Ziel dieser Arbeit ist es eine allgemeine Lösung zur Reproduzierbarkeit in den Geowissenschaften zu erstellen, indem die Daten und deren Bearbeitung automatisch identifizierbar gemacht werden. Dafür wird eine vollständige Lösung innerhalb des Horizon 2020 Projektes \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend konzipiert, welche im Earth Observation Data Centre for Water Resources Monitoring (EODC) Anbieter implementiert wird. Die Research Data Allience (RDA) Empfehlungen bezüglich Datenzitierbarkeit werden in diesem Kontext angewendet. Um die Bearbeitung der Daten einzufangen wird das VFramework Konzept angewandt. Die Implementierung in EODC wird über vordefinierter Szenarien für Geowissenschaftler evaluiert.    
\end{kurzfassung}


\begin{abstract}
Studies show that many areas of earth observation sciences lack in creating reproducible research. In the last years\DIFdelbegin \DIFdel{there was an extensive }\DIFdelend \DIFaddbegin \DIFadd{, there was a great }\DIFaddend movement towards policies defining reproducibility for geoscientists. The diverse set of data providers, the lack of data versions and processing systems without transparency of environment \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend are responsible for scientists not being able to reproduce experiments. Existing solutions focus on re-execution and are specific for \DIFdelbegin \DIFdel{certain work-flows and back ends}\DIFdelend \DIFaddbegin \DIFadd{individual workflows and backends}\DIFaddend . To reproduce an experiment, the input data and the \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend have to be identifiable. The earth observation community has no general solution that can be easily applied by the providers and is simple to use for scientists. The aim of this thesis is a general concept of reproducibility in the context of the earth observation community \DIFdelbegin \DIFdel{, }\DIFdelend by applying automatic data identification and \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend capturing methods. Therefore, \DIFdelbegin \DIFdel{a complete system is implemented within }\DIFdelend the Horizon 2020 \DIFdelbegin \DIFdel{OpenEO project }\DIFdelend \DIFaddbegin \DIFadd{openEO project implements a complete system}\DIFaddend , which is a unified abstraction layer standard for earth observation data providers. In doing so, the Earth Observation Data Centre for Water Resources Monitoring (EODC) \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is extended applying the \DIFdelbegin \DIFdel{Resarch Data Allience }\DIFdelend \DIFaddbegin \DIFadd{Research Data Alliance }\DIFaddend (RDA) data identification recommendations. To capture the processing \DIFdelbegin \DIFdel{work-flow the VFramework is implemented }\DIFdelend \DIFaddbegin \DIFadd{workflow, we implement the VFramework }\DIFaddend into the solution. \DIFdelbegin \DIFdel{The prototype is evaluated }\DIFdelend \DIFaddbegin \DIFadd{We evaluate the prototype }\DIFaddend by predefined scenarios of scientists using EODC to examine the viability of the prototype.       
\end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
\mainmatter

\chapter{Introduction}\label{Introduction}
\section{Motivation}\label{Motivation}
Over the last decades\DIFaddbegin \DIFadd{, }\DIFaddend remote sensing agencies have increased the variations of data processing and therefore\DIFaddbegin \DIFadd{, }\DIFaddend the amount of resulting data. The complexity of the experiments leads researchers to use external services for the workflow. These circumstances \DIFdelbegin \DIFdel{makes }\DIFdelend \DIFaddbegin \DIFadd{make }\DIFaddend it hard for scientists to provide the necessary information to enable reproducibility. Scientists \DIFdelbegin \DIFdel{just }\DIFdelend create a description of the \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend and the used satellite identifier to describe experiments. \DIFdelbegin \DIFdel{To preserve the data for further usage in the future it }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend is necessary to have citable data and processes on the data to ensure long-term reproducibility \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{6352411}}\hspace{0pt}%DIFAUXCMD
. Reproducibility is discussed in many scientific areas}\DIFdelend \DIFaddbegin \DIFadd{to preserve the data for further usage in the future,  \mbox{%DIFAUXCMD
\cite{6352411}}\hspace{0pt}%DIFAUXCMD
. Many scientific areas, }\DIFaddend such as computer science, biology\DIFdelbegin \DIFdel{and in computational geoscience . In \mbox{%DIFAUXCMD
\cite{reprovsrepli} }\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{, and computational geoscience discuss reproducibility. The study on reproducibility in geoscience \mbox{%DIFAUXCMD
\cite{reprovsrepli} }\hspace{0pt}%DIFAUXCMD
defines the term }\DIFaddend "reproducing" \DIFdelbegin \DIFdel{an experiment is defined }\DIFdelend by running a second different experiment, which results in the same conclusion as the first experiment. The more the second experiment differs from the first, the more information is gained by it. It is needed to provide additional evidence about the outcome of an experiment. Replicability is the re-run of the same methods of the first experiment \DIFdelbegin \DIFdel{with the aim of checking }\DIFdelend \DIFaddbegin \DIFadd{to check }\DIFaddend if the described methods lead to the claimed results\cite{reprovsrepli}. According to \cite{Ostermann2017AdvancingSW} where the reproducibility and replicability of scientific papers in geoscience got tested, only half of the publications were replicable and none of them reproducible. In \cite{Thestateofreproducibility} a survey of geoscientific readers and authors of papers got executed, with the goal of finding reasons for the lack of reproducibility in earth observation sciences. One of the results is that even though 49\% of the participants responded that their publications are reproducible, only 12\% of them have linked the used code. The understanding of open\DIFaddbegin \DIFadd{, }\DIFaddend reproducible research is different among the participating scientists in a way that the interpretation is more related to repeatability and replicability. The main reasons for the lack of reproducibility in computational geoscience are:

\begin{enumerate}
	\item Insufficiently described methods 
	\item No persistent data identifier
	\item Legal concerns
	\item The impression that it is not necessary
	\item Too Time consuming
\end{enumerate} 

\DIFdelbegin \DIFdel{Reproducing geoscientific papers failed }\DIFdelend \DIFaddbegin \DIFadd{The reproduction of geoscientific papers fails }\DIFaddend by the different individual interpretation of the described approach in the publication. If the code is published with the publication, alternative software \DIFdelbegin \DIFdel{versions }\DIFdelend \DIFaddbegin \DIFadd{environments }\DIFaddend produced unequal results \DIFdelbegin \DIFdel{e.g. a }\DIFdelend \DIFaddbegin \DIFadd{in example }\DIFaddend different versions of the CRAN library in R resulted in a different result. The vast majority of issues on reproducing results are required changes of code and a deeper understanding of the procedures \DIFdelbegin \DIFdel{e.g. }\DIFdelend \DIFaddbegin \DIFadd{in example }\DIFaddend if functions are deprecated and the code \DIFdelbegin \DIFdel{has to be modified }\DIFdelend \DIFaddbegin \DIFadd{needs modifications }\DIFaddend for using new functions. In some cases \DIFaddbegin \DIFadd{system dependent }\DIFaddend issues occur, which are \DIFdelbegin \DIFdel{system dependent and }\DIFdelend related to the usage of random access memory and installation libraries of the operating system \cite{Thestateofreproducibility}. \DIFdelbegin \DIFdel{In \mbox{%DIFAUXCMD
\cite{Thestateofreproducibility} }\hspace{0pt}%DIFAUXCMD
resulting images of the research are reproduced and compared }\DIFdelend \DIFaddbegin \DIFadd{The study reproduces a resulting image of a publication and compares it }\DIFaddend with the result of the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution. Figure \ref{fig:motivation} shows an example of a result confrontation of the original experiment and the replicated experiment. The map \DIFdelbegin \DIFdel{shows }\DIFdelend \DIFaddbegin \DIFadd{showed }\DIFaddend spatially gridded biomass burning and was \DIFdelbegin \DIFdel{originally published }\DIFdelend \DIFaddbegin \DIFadd{published initially }\DIFaddend in \cite{bg-13-3225-2016}. Even though the resulting numbers of the reproduction remains the same as the original one, the different aspect ratio changes the appearance of the resulting image and therefore might lead to a different interpretation.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{motivation_example}
	\caption{Example of a comparison of the original result (a) and the replicated result (b). The boxes are highlighting the differences of the map. The blue box shows the misplacement of the legend, the purple box shows different colour of results, the red box shows a different data type of the legend numbers, the grey box shows a different labeling, the orange box highlights differences in the background map, the yellow box shows a different number of classes and the green box shows results that were not in the original figure \cite{Thestateofreproducibility}.}
	\label{fig:motivation} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure} 

\section{Problem Description}\label{Problem}
The vast majority of data used in earth observation sciences are retrieved and provided via \gls{soa} interfaces. Data providers like \gls{gee}\footnote{https://earthengine.google.com} and \gls{eodc}\footnote{https://www.eodc.eu} host a Web \gls{api} for data download and processing data. These services are used to define the experiment \DIFdelbegin \DIFdel{work-flow}\DIFdelend \DIFaddbegin \DIFadd{workflow}\DIFaddend , the definition of the \DIFdelbegin \DIFdel{used }\DIFdelend \DIFaddbegin \DIFadd{input }\DIFaddend data, the execution\DIFaddbegin \DIFadd{, }\DIFaddend and the retrieval of the results. Therefore, \DIFdelbegin \DIFdel{researcher are }\DIFdelend \DIFaddbegin \DIFadd{the researcher is }\DIFaddend not in full control of the code execution and the environment of their experiments, since the information about the \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend environment of the experiment is not accessible for the scientists. Used external dependencies for processing the earth observation data \DIFdelbegin \DIFdel{can not }\DIFdelend \DIFaddbegin \DIFadd{cannot }\DIFaddend be accessed. The services act as black boxes to the researchers with no possibility to get versions of the used packages to calculate the resulting images. The situation leads to the problem \DIFdelbegin \DIFdel{, }\DIFdelend that researchers are not capable of describing the experiments in a way that they \DIFdelbegin \DIFdel{can be reproduced}\DIFdelend \DIFaddbegin \DIFadd{are reproducible}\DIFaddend . \\
Input data of a typical remote sensing \DIFdelbegin \DIFdel{work-flow is defined by }\DIFdelend \DIFaddbegin \DIFadd{workflow is }\DIFaddend satellite data of a specified satellite type filtered by a temporal and a spatial extent. The raw satellite data is preprocessed by the \DIFdelbegin \DIFdel{back end provider , }\DIFdelend \DIFaddbegin \DIFadd{backend provider }\DIFaddend so that there is a global dataset. Geoscientists using the same satellite identifier \DIFdelbegin \DIFdel{can }\DIFdelend \DIFaddbegin \DIFadd{could }\DIFaddend have different results after executing the same experiment \DIFdelbegin \DIFdel{, }\DIFdelend if the provider updated the way the data is preprocessed. An \DIFdelbegin \DIFdel{examle }\DIFdelend \DIFaddbegin \DIFadd{example }\DIFaddend for this is the format update of \gls{esa} in 2017 of the Sentinel 1 dataset, which affected old data records, see \footnote{https://sentinel.esa.int/web/sentinel/missions/sentinel-1/news/-/article/sentinel-1-update-of-product-format}. When scientists use the dataset of a data provider, there is no possibility to see changes in the preprocessing algorithm. Therefore, data versions are not visible \DIFdelbegin \DIFdel{for }\DIFdelend \DIFaddbegin \DIFadd{to }\DIFaddend the researcher. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{The situation }\DIFaddend leads to the problem that scientists are not capable of identifying the \DIFdelbegin \DIFdel{used }\DIFdelend input data, which is a necessity for reproducibility.
%DIF < \todo{Talk about big data of Geoscience and how big it is...}
%DIF < \todo{maybe find better example of data update than format problems...}
%DIF < Due to a different range of functionality and a difference between the endpoints of the providers, it is great afford to create a workflow for more than one provider. The OpenEO project has the goal to be an abstraction layer above different EO data providers. Further information on the software architecture of the project is defined in the project proposal (\cite{openeo}) and in Section \ref{Related Work}. During the creation time of this thesis there is no consideration of repeatability in the OpenEO architecture. Verification of work-flows for users of OpenEO is not in the agenda of the OpenEO project. Generalized layers have the opportunity to be implemented in a way that makes processes and data scientifically verifiable and reproducible, as it handles data and processes on the data in a standardized way for different providers. Even though the range of functionality and the API endpoints are well-defined in the OpenEO core-API, the contributing content providers (OpenEO back ends) will have different underlying software execution environments. The used technology of an OpenEO back end will evolve in the future, hence it can lead to different results on the same workflow execution. Considering the following: A scientist runs an experiment using OpenEO as research tool and gets an arbitrary result. The same scientist runs the same experiment with the same input data some months later and gets a slightly different result. The question occurs, why are the results distinct? Has the used data changed, has the user accidentally submitted different code or has some underlying software inside the back end provider changed. Adding a possibility for the users of OpenEO to gain this information is an important feature for the scientific community. The aim of this thesis is to design an extension to OpenEO, so that users are able to retrieve provenance data about a job re-execution \cite{openeo}. 

%DIF > Due to a different range of functionality and a difference between the endpoints of the providers, it is great afford to create a workflow for more than one provider. The openEO project has the goal to be an abstraction layer above different EO data providers. Further information on the software architecture of the project is defined in the project proposal (\cite{openeo}) and in Section \ref{Related Work}. During the creation time of this thesis there is no consideration of repeatability in the openEO architecture. Verification of workflows for users of openEO is not in the agenda of the openEO project. Generalized layers have the opportunity to be implemented in a way that makes processes and data scientifically verifiable and reproducible, as it handles data and processes on the data in a standardized way for different providers. Even though the range of functionality and the API endpoints are well-defined in the openEO core-API, the contributing content providers (openEO backends) will have different underlying software execution environments. The used technology of an openEO backend will evolve in the future, hence it can lead to different results on the same workflow execution. Considering the following: A scientist runs an experiment using openEO as research tool and gets an arbitrary result. The same scientist runs the same experiment with the same input data some months later and gets a slightly different result. The question occurs, why are the results distinct? Has the used data changed, has the user accidentally submitted different code or has some underlying software inside the backend provider changed. Adding a possibility for the users of openEO to gain this information is an important feature for the scientific community. The aim of this thesis is to design an extension to openEO, so that users are able to retrieve provenance data about a job re-execution \cite{openeo}. 
\DIFaddbegin 

\DIFaddend \section{Aim of the Work}\label{Aim}\label{Use Cases}

The following sections define \DIFaddbegin \DIFadd{the }\DIFaddend use cases to validate the design proposed in this thesis. They describe scenarios focused on scientific \DIFdelbegin \DIFdel{work-flows }\DIFdelend \DIFaddbegin \DIFadd{workflows }\DIFaddend that are currently not achievable, but shall be accomplish-able with the solution of this thesis. This thesis uses the term "job" for the definition of \DIFdelbegin \DIFdel{a work-flow, }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend since it is common in earth observation science.

\subsection{Example Experiment}\label{example}

This section describes an example of an experiment that a remote sensing scientist wants to execute. The \DIFdelbegin \DIFdel{example will be used }\DIFdelend \DIFaddbegin \DIFadd{thesis uses the example }\DIFaddend throughout the thesis for \DIFaddbegin \DIFadd{a }\DIFaddend better illustration of the concepts. \DIFdelbegin \DIFdel{It is used in }\DIFdelend \DIFaddbegin \DIFadd{The following sections use it for }\DIFaddend the Use Cases\DIFdelbegin \DIFdel{in the following sections}\DIFdelend . \\
The input data of the experiment is Sentinel 2 data developed by the European Space Agency (ESA). The area of interest is the province \DIFdelbegin \DIFdel{south tyrol}\DIFdelend \DIFaddbegin \DIFadd{of South Tyrol}\DIFaddend . More specifically the bounding area in the "EPSG:4326" projection with the coordinates for \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend north-west corner of (10.288696, 46.905246) and \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend south-east corner of (12.189331, 45.935871). The time of interest is the month \DIFaddbegin \DIFadd{of }\DIFaddend May of the year 2017. In this example\DIFaddbegin \DIFadd{, }\DIFaddend the scientist is working on vegetation dynamics and wants to know what the state of the vegetation of \DIFdelbegin \DIFdel{south tyrol was in the beginning }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol was in May }\DIFaddend of 2017. Therefore the minimum of the  \gls{ndvi}\footnote{https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring\_vegetation\_2.php} is calculated on the data selection. It \DIFdelbegin \DIFdel{is derived }\DIFdelend \DIFaddbegin \DIFadd{derives }\DIFaddend from the difference between near-infrared (which reflects vegetation strongly) and red light (which vegetation absorbs). So for every pixel of the satellite image\DIFaddbegin \DIFadd{, }\DIFaddend the NDVI value is calculated for every day of May 2017. Then the 31 images for May 2017 are reduced to one by taking the minimum NDVI value of each pixel. Figure \ref{fig:example} shows the result of the running example execution.\\

The execution of the experiment consists of the following steps:

\begin{enumerate}
	\item Selecting the Sentinel 2 data records
	\item Filtering the Sentinel 2 records by the extent of south tyrol. \\(10.288696, 46.905246) - (12.189331, 45.935871) on "EPSG:4326" projection.
	\item Filtering the Sentinel 2 records by May 2017.
	\item Calculate NDVI for all days of May 2017.
	\item Reduce by the minimum value of May 2017.
	\item Interpret the resulting image.
\end{enumerate}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{openeo_example_output}
	\caption{Resulting image of the running example.}
	\label{fig:example} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}


\subsection{Use Case 1 – Re-use of input data}\label{UseCase1}
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{usecase1}
	\caption{Overview of the first Use Case.}
	\label{fig:usecase1} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}
The first use case is about the re-use of input data between job executions. Reproducible methods are especially important for the scientific community. Since scientists are likely to build on results and methods of publications, this is a scenario to make the re-use simpler. In this case\DIFaddbegin \DIFadd{, }\DIFaddend a scientist wants to create a publication by running the example, described in Section \ref{example} on an earth observation \DIFdelbegin \DIFdel{back end. The back end }\DIFdelend \DIFaddbegin \DIFadd{backend. The backend }\DIFaddend generates a \gls{pid} for the input data of the experiment. After that\DIFaddbegin \DIFadd{, }\DIFaddend the scientist publishes the results and cites the input data with the resulting PID. The PID redirects to a \DIFdelbegin \DIFdel{human readable }\DIFdelend \DIFaddbegin \DIFadd{human-readable }\DIFaddend landing page that provides meta information about the dataset. Another scientist also interested in the vegetation of \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend wants to use the same input data but chooses a different approach of processing it (\DIFdelbegin \DIFdel{e.g. }\DIFdelend \DIFaddbegin \DIFadd{for example the }\DIFaddend maximum instead of minimum). Hence, the input data PID can be used to re-use the same data. The \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend has to be capable of resolving the PID automatically to let the user work with the same input data for a new \DIFdelbegin \DIFdel{work-flow}\DIFdelend \DIFaddbegin \DIFadd{workflow}\DIFaddend . The data provider \DIFdelbegin \DIFdel{need }\DIFdelend \DIFaddbegin \DIFadd{needs }\DIFaddend to persist the data defined by a PID even if the preprocessing of the data changes in the future.   

\begin{itemize}
	\item \textbf{Input Data A}: Sentinel 2 data of the area of \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend in May 2017. 
	\item \textbf{Job A}: Taking the \textbf{minimum} NDVI of the area of \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend in May 2017. 
	\item \textbf{Job B}: Taking the \textbf{maximum} NDVI of the area of \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend in May 2017.
\end{itemize}

\DIFdelbegin \DIFdel{An }\DIFdelend \DIFaddbegin \DIFadd{Figure \ref{fig:usecase1} gives an }\DIFaddend overview of the use case\DIFdelbegin \DIFdel{can be viewed in Figure \ref{fig:usecase1}}\DIFdelend . \\

The scenario sequence of actions is summarized in the following steps: \\

\begin{enumerate}
	\item Researcher A runs job A at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .
	\item Researcher A retrieves the used input data PID of job A.
	\item Researcher A cites the input data with the PID in a publication.
	\item Researcher B uses the same input data, by applying the data PID of job A for job B.  
\end{enumerate}

\subsection{Use Case 2 – Providing job execution Information}\label{UseCase2}
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{usecase2}
	\caption{Overview of the second Use Case}
	\label{fig:usecase2} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{The second }\DIFaddend use case is like the first one \DIFdelbegin \DIFdel{, }\DIFdelend but is exclusively about job dependent environment information. The scientist can automatically get environment data about the job execution e.g. used software packages and their versions. \DIFdelbegin \DIFdel{Motivation }\DIFdelend \DIFaddbegin \DIFadd{The motivation }\DIFaddend for this is to add transparency \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{to }\DIFaddend the job processing for the users \DIFdelbegin \DIFdel{, }\DIFdelend so that researchers can describe their processes in more detail. It can help geoscientists to understand why results differ from executions in the past. \DIFdelbegin \DIFdel{An }\DIFdelend \DIFaddbegin \DIFadd{Figure \ref{fig:usecase2} gives an }\DIFaddend overview of the use case\DIFdelbegin \DIFdel{can be viewed in Figure \ref{fig:usecase2}. 
The }\DIFdelend \DIFaddbegin \DIFadd{. 
The following steps summarize the }\DIFaddend scenario sequence of actions\DIFdelbegin \DIFdel{is summarized in the following steps}\DIFdelend : \\
\begin{enumerate}
	\item Researcher runs an experiment (job A) at a \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .
	\item Researcher wants to describe the experiment environment.
%	\item Back End Developer releases a new version.   
%	\item Back End Developer runs test jobs to find differences.
\end{enumerate}

\subsection{Use Case 3 – Compare different job executions}\label{UseCase3}
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{usecase3}
	\caption{Overview of the third Use Case}
	\label{fig:usecase3} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}
The third use case \DIFdelbegin \DIFdel{is dedicated }\DIFdelend \DIFaddbegin \DIFadd{dedicates }\DIFaddend to the comparison of job executions. The goal is to add a possibility for geoscientists to compare different jobs not only by their results \DIFdelbegin \DIFdel{, }\DIFdelend but on the way they \DIFdelbegin \DIFdel{were }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend executed. The \DIFdelbegin \DIFdel{comparison is applied }\DIFdelend \DIFaddbegin \DIFadd{same backend applies the comparison }\DIFaddend between a job execution and another job execution\DIFdelbegin \DIFdel{on the same back end}\DIFdelend . Therefore, the processing implementation and the input data has to be identifiable. To make the comparison \DIFdelbegin \DIFdel{interesting }\DIFdelend \DIFaddbegin \DIFadd{enjoyable }\DIFaddend to the users\DIFdelbegin \DIFdel{additional meta-data }\DIFdelend \DIFaddbegin \DIFadd{, additional data }\DIFaddend is added to the job environment data e.g. an output checksum. For usability reasons\DIFaddbegin \DIFadd{, }\DIFaddend the comparison needs to be easy to apply and easy to understand, therefore accessible in the context of the user. In addition to the previous conditions\DIFaddbegin \DIFadd{, }\DIFaddend a visualization of the differences for the users can lower the access barrier for them to use the feature. \DIFdelbegin \DIFdel{An }\DIFdelend \DIFaddbegin \DIFadd{Figure \ref{fig:usecase3} gives an }\DIFaddend overview of this use case\DIFdelbegin \DIFdel{can be viewed in Figure \ref{fig:usecase3}.
The }\DIFdelend \DIFaddbegin \DIFadd{.
The following steps summarize the }\DIFaddend scenario sequence of actions\DIFdelbegin \DIFdel{is summarized in the following steps}\DIFdelend : \\

\begin{itemize}
	\item \textbf{Job A}: Taking the minimum NDVI of the area of \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend in May 2017. 
	\item \textbf{Job B}: Taking the minimum NDVI of the area of \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend in May 2017.
	\item \textbf{Job C}: Taking the maximum NDVI of the area of \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend in May 2017.
\end{itemize}

\begin{enumerate}
	\item Researcher runs an experiment (job A) at a \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .
	\item Researcher re-runs the same experiment (job B).
	\item Researcher runs a different experiment (job C).   
	\item Researcher receives a comparison of the jobs (A, B, C) by their environment and outcome.
\end{enumerate}

\subsection{Research Questions}\label{research question}

The expected outcome of this thesis is to discover and develop a framework for making repeatability conceivable in the earth observation community. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{The solution }\DIFaddend enables users to re-execute \DIFdelbegin \DIFdel{work-flows and validate }\DIFdelend \DIFaddbegin \DIFadd{workflows and validates }\DIFaddend the result, so that differences \DIFdelbegin \DIFdel{on }\DIFdelend \DIFaddbegin \DIFadd{in }\DIFaddend the processing or the data are accessible for the users. To achieve this goal\DIFaddbegin \DIFadd{, }\DIFaddend a model for capturing the environment of the \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend has to be discovered and implemented. Since the solution is implemented within the \gls{openeo} \DIFdelbegin \DIFdel{project }\DIFdelend (details see Section \DIFdelbegin \DIFdel{\ref{OpenEO}}\DIFdelend \DIFaddbegin \DIFadd{\ref{openEO}}\DIFaddend ), it shall conclude in recommendations for the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project on how to improve re-execution validation for the users and how it \DIFdelbegin \DIFdel{can be achieved}\DIFdelend \DIFaddbegin \DIFadd{is achievable}\DIFaddend . Considering the problem description and the scenarios above, the following research questions can be formulated:

\begin{itemize}
	\item \textbf{How can an earth observation job re-execution be applied like the initial execution?}
	\begin{itemize}
		\item How can the used data be identified after the initial execution?
		\item How can the used software of the initial execution be reproduced?
		\item What data has to be captured when?
		\item How can the result of a re-execution in future software versions be verified?
	\end{itemize}
	\item \textbf{How can the equality of a earth observation job re-execution results be validated?}
	\begin{itemize}
		\item What are the validation requirements?
		\item How can the data be compared?
		\item How can changes of the earth observation \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment be recognized?
		\item How can differences in the environment between the executions be discovered?
	\end{itemize}
\end{itemize}

\section{Methodological Approach}\label{Method}
The use cases defined in the previous section require specific changes to the components of an earth observation \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The following list describes the parts of the suggested solution briefly: 


\begin{enumerate}
	\item \textbf{Data Identification}
	\DIFdelbegin \DIFdel{In order to }\DIFdelend \DIFaddbegin \DIFadd{The input data has to be identifiable, to }\DIFaddend accomplish the capturing of processing workflows described in the use cases in Section \ref{Use Cases}\DIFdelbegin \DIFdel{, the input data has to be identifiable. In order to achieve this}\DIFdelend \DIFaddbegin \DIFadd{. To achieve this, }\DIFaddend the recommendations of the \gls{rda} (see \cite{rauber2016identification}) have to be implemented by adding versioning and query databases.

	\item \textbf{Process Versioning}
	\DIFdelbegin \DIFdel{To }\DIFdelend \DIFaddbegin \DIFadd{The process has to be identifiable, to }\DIFaddend accomplish the verification of different process executions\DIFdelbegin \DIFdel{, the process has to be identifiable}\DIFdelend . Therefore, \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend versioning of the process code can be used to persist \DIFdelbegin \DIFdel{different }\DIFdelend states of the code. The thesis is for an \DIFdelbegin \DIFdel{OpenSource project}\DIFdelend \DIFaddbegin \DIFadd{open source project, }\DIFaddend and the code of the \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend are published at Github, so \DIFdelbegin \DIFdel{Git is used }\DIFdelend \DIFaddbegin \DIFadd{we use Git }\DIFaddend as the tool to capture the code state. 

	\item \textbf{User Endpoints}
	The interface for users has to be implemented so that the use cases can be executed. Therefore, client applications have to be modified. The used programming language in this thesis is Python\footnote{https://www.python.org}.
\end{enumerate}

\section{Structure of Work}\label{Structure}
The following \DIFdelbegin \DIFdel{sections }\DIFdelend \DIFaddbegin \DIFadd{chapters }\DIFaddend are structured as followed:\\
Chapter 2 gives an overview of related scientific activities in the area of reproducibility in the earth observation sciences and reproducibility in other areas with similar objectives. \\
%DIF < Chapter 3  describes the technologies and concepts that are used in the implementation of this thesis. This chapter provides an overview of the EODC back end used for the implementation.\\
%DIF > Chapter 3  describes the technologies and concepts that are used in the implementation of this thesis. This chapter provides an overview of the EODC backend used for the implementation.\\
Chapter 3 provides the concept to address the research questions defined in Section \ref{Aim}.  This is the theoretical definition of what has to be implemented in the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project.\\
Chapter 4 has a detailed explanation to the proof of concept prototype implementation and how the concept of chapter 3 was realized for the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . \\
Chapter 5 dives into the evaluation of the implementation by applying \DIFdelbegin \DIFdel{the use }\DIFdelend \DIFaddbegin \DIFadd{test }\DIFaddend cases to the implementation of chapter 4.\\
Chapter 6 summarizes the outcome of the implementation and evaluation. It contains a discussion on results achieved, open issues and future work. \\

\chapter{Related Work}\label{Related Work}

This chapter describes the related work of the thesis. In the following\DIFaddbegin \DIFadd{, }\DIFaddend the reader gets informed about basic concepts related to the prototype implementation. The information is structured in subsections, where each represents different technologies or concepts. \\The first section presents the concepts behind reproducibility in computer science. \\
The second section describes the state of reproducibility in earth observation science. \\
The third section presents concepts related to data identification. \\ 
The fourth section consists of existing tools that can be used to achieve reproducibility. \\
The last section of this chapter describes the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project further and more specifically\DIFdelbegin \DIFdel{the EODC back end, which is used }\DIFdelend \DIFaddbegin \DIFadd{, the EODC backend, which we use }\DIFaddend for the proof of concept implementation in Chapter \ref{Implementation}.      


\section{Reproducibility}\label{Reproducibility}
\DIFdelbegin \DIFdel{According to \mbox{%DIFAUXCMD
\cite{6064509} }\hspace{0pt}%DIFAUXCMD
, reproducibility is defined }\DIFdelend \DIFaddbegin \DIFadd{This Publication\mbox{%DIFAUXCMD
\cite{6064509} }\hspace{0pt}%DIFAUXCMD
defines reproducibility }\DIFaddend as a new experiment based on an original experiment by an independent researcher in the manner of the original experiment.  \DIFdelbegin \DIFdel{The aim of reproducibility is }\DIFdelend \DIFaddbegin \DIFadd{Reproducibility aims }\DIFaddend to gain additional evidence on the result of the \DIFdelbegin \DIFdel{original result , }\DIFdelend \DIFaddbegin \DIFadd{first result }\DIFaddend by creating an independent experiment that shows similar results. Repetition is the re-run of the \DIFdelbegin \DIFdel{exact }\DIFdelend same experiment with the same method, same environment\DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{, and a }\DIFaddend very similar result. The \DIFdelbegin \DIFdel{aim of the repetition is }\DIFdelend \DIFaddbegin \DIFadd{repetition aims }\DIFaddend to check if the methods described in a publication are resulting in the purposed outcome. 
Achieving reproducibility is a common problem in all scientific areas. It is an issue of the whole scientific community to produce \DIFdelbegin \DIFdel{results that are reproducible }\DIFdelend \DIFaddbegin \DIFadd{reproducible results}\DIFaddend . Therefore in \cite{10.1371/journal.pcbi.1003285} there are ten rules defined to gain a common sense about reproducibility in all science areas. They consist of the basic idea that every result of interest has to be associated with a process and data used. \DIFdelbegin \DIFdel{External programs have to be persisted }\DIFdelend \DIFaddbegin \DIFadd{The researcher needs to provide external programs }\DIFaddend as well as custom script versions. The usage of version control is recommended. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend there is a rule defined to make the scripts and their results publicly available \cite{10.1371/journal.pcbi.1003285}. 
Reproducibility is the \DIFdelbegin \DIFdel{key }\DIFdelend \DIFaddbegin \DIFadd{crucial }\DIFaddend element of The Fourth Paradigm published in \cite{noauthororeditorfourth}. It leads to the term eScience, which has the aim of bringing science and computer technologies closer together. The general concept is to enable scientific procedures in the new information technologies used by \DIFdelbegin \DIFdel{data intensive }\DIFdelend \DIFaddbegin \DIFadd{data-intensive }\DIFaddend sciences. The expected result of eScience is to get all scientific papers publicly available\DIFaddbegin \DIFadd{, }\DIFaddend including the necessary data and \DIFdelbegin \DIFdel{work-flows }\DIFdelend \DIFaddbegin \DIFadd{workflows }\DIFaddend so that scientists \DIFdelbegin \DIFdel{are able to }\DIFdelend \DIFaddbegin \DIFadd{can }\DIFaddend have a fast interaction with each other \cite{noauthororeditorfourth}. 
eScience has the potential to enable a boost in scientific discovery by providing approaches to make digital data and \DIFdelbegin \DIFdel{work-flows }\DIFdelend \DIFaddbegin \DIFadd{workflows }\DIFaddend citable. The publication \cite{Rauber2015RepeatabilityAR} discusses a common way of reaching the goal formulated above. It describes an approach to look at whole research processes by introducing Process Management Plans, other than limiting it to data citation.  The \DIFaddbegin \DIFadd{paper \mbox{%DIFAUXCMD
\cite{Rauber2015RepeatabilityAR} }\hspace{0pt}%DIFAUXCMD
demonstrates the }\DIFaddend capturing, verification\DIFaddbegin \DIFadd{, }\DIFaddend and validation of the input data for a computational process\DIFdelbegin \DIFdel{is demonstrated within the paper \mbox{%DIFAUXCMD
\cite{Rauber2015RepeatabilityAR}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend .
Computer sciences have the issue of a high amount of published papers that do not provide enough information to make them reproducible. The problem \DIFdelbegin \DIFdel{will not be }\DIFdelend \DIFaddbegin \DIFadd{is not }\DIFaddend solved by the scientists that need to \DIFdelbegin \DIFdel{do }\DIFdelend \DIFaddbegin \DIFadd{make }\DIFaddend additional effort to make reproducibility doable, but by providing new tools for scientists that allow it automatically as stated in \cite{MIKSA201725}. There are some additional issues on reproducibility in computer science e.g. in the case that used software technologies are deprecated and not available \DIFdelbegin \DIFdel{any more}\DIFdelend \DIFaddbegin \DIFadd{anymore}\DIFaddend . Therefore, persisting the execution context is critical to achieve a re-execution of the experiment. One proposed concept of doing so is the VFramework described in more detail in \DIFdelbegin \DIFdel{the method Section \ref{Related Work}}\DIFdelend \DIFaddbegin \DIFadd{Section \ref{vframework}}\DIFaddend . 
The PRIMAD model defines a set of variables that define an experiment. The relationship of the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution to a re-execution \DIFdelbegin \DIFdel{can be }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend visualized by noticing changes \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{in }\DIFaddend the variables in the re-execution. The following variables of an experiment are used to describe the relationship to the second experiment\DIFaddbegin \DIFadd{, }\DIFaddend according to \cite{primad}:

\begin{itemize}
	\item \textbf{P} Platform / Execution Environment / Context (e.g. Python 2.7, Windows 10,\dots) \\
	\item \textbf{R} Research Objectives / Goals (e.g. sorting the input) \\
	\item \textbf{I} Implementation / Code / Source-Code (e.g. script in Python) \\
	\item \textbf{M} Methods / Algorithms (e.g. quick sort) \\
	\item \textbf{A} Actors / Persons (e.g. researcher that is executing the experiment) \\
	\item \textbf{D} Data (input data and parameter values)   (e.g. input data that is to be sorted) 
\end{itemize}
If there is another experiment that is different on every variable to the original one \DIFdelbegin \DIFdel{, }\DIFdelend but has the same method (M) and goal (R), then it is a reproducing experiment \cite{primad}. 
\\  
Data citation is a \DIFdelbegin \DIFdel{key }\DIFdelend \DIFaddbegin \DIFadd{vital }\DIFaddend issue of reproducing results of past experiments. Preserving the exact process without persisting the original data is no positive effect \DIFdelbegin \DIFdel{for }\DIFdelend \DIFaddbegin \DIFadd{on }\DIFaddend the scientific community. If the data used in an experiment is not available anymore, or not specified explicit enough\DIFaddbegin \DIFadd{, }\DIFaddend then there is no chance of reproducing it no matter how much information about the execution \DIFdelbegin \DIFdel{was persisted}\DIFdelend \DIFaddbegin \DIFadd{is known}\DIFaddend . In earth observation persisting the data for reproducibility in the future is an issue discussed in \cite{6352411}. Gaining data identification in digital sciences has an official working group named \gls{wgdc}, which created 14 recommendations on data citation further explained in Section \ref{Data Identification}.

 \subsection{PROV-O}\label{PROV}
In 2003 the World Wide Web Consortium published the PROV model as a standard for provenance definitions. \DIFdelbegin \DIFdel{It is defined by twelve different documents }\DIFdelend \DIFaddbegin \DIFadd{Twelve different documents define it}\DIFaddend . In the context of this thesis the PROV Ontology (PROV-O) is the most relevant \cite{733f89c65e4844f9aabcae1c276a5602}. 
PROV-O is a standard language using OWL2 Web Ontology. It is \DIFdelbegin \DIFdel{designed as }\DIFdelend a lightweight concept \DIFdelbegin \DIFdel{to be used in }\DIFdelend \DIFaddbegin \DIFadd{capable of }\DIFaddend a broad spectrum of applications.  

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{prov}
	\caption{Overview of the main components of PROV-O \cite{733f89c65e4844f9aabcae1c276a5602}}
	\label{fig:prov} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

In Figure \ref{fig:prov}\DIFaddbegin \DIFadd{, }\DIFaddend there is the basic setup of the PROV-O concept. It consists of three main elements. The Entity is any physical, digital, conceptual thing. Provenance records describe Entities that can consist of references to other Entities. Another element is the Agent, which is responsible for Activities and that they are taking place, e.g. software, persons\DIFaddbegin \DIFadd{, }\DIFaddend or organizations. The association of an Agent to an Activity defines the responsibility of the Agent for the Activity. An Activity describes what happened that the Entity has come to existence and how attributes of an Entity changed \cite{733f89c65e4844f9aabcae1c276a5602}. The \DIFaddbegin \DIFadd{design chapter \ref{Design} does not specify the }\DIFaddend representation of the context information\DIFdelbegin \DIFdel{is not specified in the design chapter \ref{Design}, therefore the }\DIFdelend \DIFaddbegin \DIFadd{. Therefore the }\DIFaddend PROV ontology can be used to represent the information. \DIFdelbegin \DIFdel{In this thesis }\DIFdelend \DIFaddbegin \DIFadd{This thesis is not implementing }\DIFaddend the PROV-O standard \DIFdelbegin \DIFdel{is not implemented, }\DIFdelend since the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend providers wanted their \DIFdelbegin \DIFdel{own }\DIFdelend representation of it, which is an extension of the existing \DIFdelbegin \DIFdel{meta-data}\DIFdelend \DIFaddbegin \DIFadd{data}\DIFaddend , but since it is a reasonable extension\DIFdelbegin \DIFdel{it is mentioned in the }\DIFdelend \DIFaddbegin \DIFadd{. The }\DIFaddend future work Section \ref{FutureWork} \DIFaddbegin \DIFadd{mentions it}\DIFaddend .

\subsection{VFramework}\DIFaddbegin \label{vframework}
\DIFaddend 

The basic concept of the VFramework is \DIFdelbegin \DIFdel{that the execution is done }\DIFdelend \DIFaddbegin \DIFadd{the workflow execution }\DIFaddend with parallel capturing of provenance data. During the \DIFdelbegin \DIFdel{original execution}\DIFdelend \DIFaddbegin \DIFadd{first execution, }\DIFaddend evidence gets collected into a repository e.g. logging of the needed \DIFdelbegin \DIFdel{meta-data. All of which is persisted in the }\DIFdelend \DIFaddbegin \DIFadd{data. The }\DIFaddend context model of the execution \DIFaddbegin \DIFadd{persists all of which }\DIFaddend e.g. a database record. A re-execution can then be verified and validated using the provided provenance data in the context model of the original execution and the context model of the re-execution. The provenance data \DIFdelbegin \DIFdel{is divided }\DIFdelend \DIFaddbegin \DIFadd{divides }\DIFaddend into static and dynamic data. Static data defined in the VFramework is data that is not dependent on the execution of the experiment e.g. the operating system and installed packages. The static environment information is\DIFdelbegin \DIFdel{therefore independent from the set up }\DIFdelend \DIFaddbegin \DIFadd{, therefore, independent of the setup }\DIFaddend and the configuration of the execution. Dynamic data\DIFaddbegin \DIFadd{, }\DIFaddend on the other hand\DIFaddbegin \DIFadd{, }\DIFaddend has to be captured during the execution of the original experiment e.g. python version of the execution or used input files. It describes the data that is specific for one \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend execution \cite{Miksa2013FrameworkFV}. 
In Figure \ref{fig:vframework}\DIFaddbegin \DIFadd{, }\DIFaddend there is an overview of the VFramework concept.   
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{vframework}
	\caption{Overview of the Concept of the VFramework \cite{Miksa2013FrameworkFV}}
	\label{fig:vframework} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}


\section{Earth Observation Science}\label{EOScience}

Reproducibility is a \DIFdelbegin \DIFdel{well discussed }\DIFdelend \DIFaddbegin \DIFadd{well-discussed }\DIFaddend topic of the computational geoscience. The high amount of earth observation data leads to complex research experiments and therefore\DIFaddbegin \DIFadd{, }\DIFaddend high complexity in the papers.  In \cite{Ostermann2017AdvancingSW} reproducibility and replicability of scientific papers in geoscience are tested by obtaining more than 400 papers. In \cite{Ostermann2017AdvancingSW} a reproduction is defined by an exact duplicate of an experiment, whereas \DIFdelbegin \DIFdel{a replication is defined }\DIFdelend \DIFaddbegin \DIFadd{it defines replication }\DIFaddend as a resemblance of the original execution, but allowing variation e.g. different scales. Table \ref{Tab:geoprimad} describes the difference \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{between }\DIFaddend reproduction and replicability in PRIMAD terms. Only half of the test group publications are replicable\DIFaddbegin \DIFadd{, }\DIFaddend and none of them reproducible. There are publications to address this lack of reproducibility in the earth observation science. The sections below summarize some examples of earth observation communities that are working on the topic of reproducibility.     

\begin{table}[]
	\caption{PRIMAD description of reproduction and replication according to \cite{Ostermann2017AdvancingSW}}
	\begin{tabular}{l|l|l}
		 & \textbf{Reproduction} & \textbf{Replication}  \\ \hline
		\textbf{P}latform & same & different \\ \hline 
		\textbf{R}esearch Objectives & same & same \\ \hline  
		\textbf{I}mplementation  & same & different  \\ \hline  
		\textbf{M}ethods & same & different \\ \hline 
		\textbf{A}ctors & different & different \\ \hline
		\textbf{D}ata & same & different, but similiar \\ \hline
	\end{tabular}
	\label{Tab:geoprimad}
\end{table}



\subsection{Vadose Zone Journal (VZJ)}\label{VZJ}
In order to face the issue of lacking reproducibility in geoscience the \gls{vzj} started a \gls{rr} program in 2015 \cite{doi:10.2136/vzj2015.06.0088}. 
The earth observation science is a big part of VZJ publications\DIFaddbegin \DIFadd{, }\DIFaddend and most of them are not applying the open computational science guidelines. The main reasons are behaviors of scientists that do not see the overall benefit of putting effort into documentation. Therefore, the VZJ started an RR program to publish alongside the scientific paper the code and data used by the scientists for evaluating the publication. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{The strategy }\DIFaddend shall lower the access barrier for scientists to publish their research work entirely. Aim of the project is to create a community of researchers with a \DIFdelbegin \DIFdel{common }\DIFdelend \DIFaddbegin \DIFadd{shared }\DIFaddend sense of reproducibility and data citation on the platform and to animate other scientists to join the approach. On \DIFdelbegin \DIFdel{the }\DIFdelend time this thesis is written, the service is still available\footnote{https://dl.sciencesocieties.org/publications/vzj/author-instructions-reproducible-research}, but there were no results available on how much it is used \cite{doi:10.2136/vzj2015.06.0088}.

\subsection{The Geoscience Paper of the Future (GPF)}\label{GPF}

The geoscience paper of the future (GPF) is according to \cite{Gil2016TowardTG} a proposed standard to help geographical scientists by making reproducible publications. It defines recommendations on data management and software management \DIFdelbegin \DIFdel{, }\DIFdelend by introducing the reader to available repositories. The gain of it is applying concepts of open science and reproducibility to earth observation papers. A GPF needs to apply the following requirements:

\begin{itemize}
	\item \textbf{Reusable data} in a public repositories and persistent identifiers.
	\item \textbf{Reusable software} (including software for preparing and post editing of the data) in a  public repositories and persistent identifiers.
	\item \textbf{Documenting the computational provenance of results} in a public repositories and persistent identifiers.  
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{gpf}
	\caption{Comparison between reproducible publications and geoscientific papers of the future \cite{Gil2016TowardTG}}
	\label{fig:gpf} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\DIFdelbegin \DIFdel{In Figure \ref{fig:gpf} }\DIFdelend \DIFaddbegin \DIFadd{Figure \ref{fig:gpf} visualizes }\DIFaddend the differences with a reproducible paper\DIFdelbegin \DIFdel{gets visualized}\DIFdelend . In addition to the characteristics of the reproducible paper\DIFaddbegin \DIFadd{, }\DIFaddend the GPF focuses on publishing the data publicly with open licenses with citable persistent identifiers.
The GPF consists of a set of 20 recommendations for geoscientists regarding data accessibility, software accessibility\DIFaddbegin \DIFadd{, }\DIFaddend and provenance information. GPF authors may find reasons \DIFdelbegin \DIFdel{to not }\DIFdelend \DIFaddbegin \DIFadd{not to }\DIFaddend be able to follow the rules and therefore\DIFaddbegin \DIFadd{, }\DIFaddend have to find workarounds and propose areas for future improvements. The strategy of the GPF community is to educate the scientist to make reproducible publications, by making training sessions for them\footnote{http://scientificpaperofthefuture.org/gpf/events.html}, \DIFdelbegin \DIFdel{rather then }\DIFdelend \DIFaddbegin \DIFadd{instead than }\DIFaddend providing tools to make it easier for scientists to use it. In comparison to this thesis\DIFaddbegin \DIFadd{, }\DIFaddend it is a different approach to achieve the same goal\DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{; }\DIFaddend in this thesis\DIFaddbegin \DIFadd{, }\DIFaddend the strategy is to achieve reproducibility in geoscience through technology. 

\subsection{Climate Change Centre Austria (CCCA)}

The \gls{ccca} is a research network for Austrian climate research online available since 2016 and has 28 members. The main tasks are the provision of \DIFdelbegin \DIFdel{climate relevant }\DIFdelend \DIFaddbegin \DIFadd{climate-relevant }\DIFaddend information, the inter-operable interfaces\DIFaddbegin \DIFadd{, }\DIFaddend and long term archiving of scientific data. In 2017 the \gls{netcdf} data citation was added to the project. The set up of the data service and the technology used in the background is similar to the set up of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . Therefore, the process of enabling data identification was similar to this thesis. CCCA is Open-source and available at GitHub, see the GitHub repository\footnote{https://github.com/ccca-dc}. The concept used for gaining data citation was the RDA recommendations defined in \cite{rauber2016identification}. Figure \ref{fig:ccca} shows an overview of the CCCA implementation. It uses a ckan\footnote{https://ckan.org} \DIFdelbegin \DIFdel{webserver }\DIFdelend \DIFaddbegin \DIFadd{web server }\DIFaddend to handle the request and responses of the user, which are then passed to \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend python application. The python part is responsible for the query store functionality. The core element is the \gls{tds}\footnote{https://www.unidata.ucar.edu/software/thredds/current/tds/}, which is responsible for the actual data archive.   
The python application part of the CCCA overview is \DIFdelbegin \DIFdel{similiar }\DIFdelend \DIFaddbegin \DIFadd{similar }\DIFaddend to the implementation of this thesis. The main difference \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend the objectives of the CCCA service compared to the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . On the CCCA platform, any \DIFdelbegin \DIFdel{climate relevant }\DIFdelend \DIFaddbegin \DIFadd{climate-relevant }\DIFaddend information (e.g. air temperature or frost days) can be uploaded and persisted. On the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , preprocessed global earth observation data is persisted, which is used as input data for processing chains. Therefore, the data on EODC is more homogeneous than the data on the CCCA platform.
Nevertheless, the concept of enabling data identification is similar in both projects. The \DIFdelbegin \DIFdel{basic }\DIFdelend \DIFaddbegin \DIFadd{CCCA implementation inspired the }\DIFaddend query store implementation of this thesis\DIFdelbegin \DIFdel{was inspired by the CCCA implementation}\DIFdelend . The query result differs, because EODC uses \gls{geotiff} instead of NetCDF at their \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . Another difference is that CCCA uses HTTP GET requests as query, whereas the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend uses \gls{xml} based queries, which are \DIFdelbegin \DIFdel{in addition }\DIFdelend \DIFaddbegin \DIFadd{also }\DIFaddend limited by the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend API specification  \cite{ccca}.  

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.25]{ccca}
	\caption{Concept of the CCCA NetCDF Cata Citation}
	\label{fig:ccca} % \label has to be placed AFTER \caption (or \subcaption) to produce %correct cross-references.
\end{figure}

\section{Data Identification}\label{Data Identification}

Data identification and citation is \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend main concern in many \DIFdelbegin \DIFdel{computer relying }\DIFdelend \DIFaddbegin \DIFadd{computers relying on }\DIFaddend sciences. For the aim of this work, the input data is a key element of the capturing. If the input data can not be identified correctly, the capturing of the processing on it does not gain useful information. Therefore the identity of the data has to be guaranteed. The Research Data Alliance (RDA) presents general solutions to achieve data identifications. There are 14 recommendations defined to achieve the identification of an exact version and subset of input data. The recommendations are independent of the type of data and database system. In the following the recommendations are summarized \cite{rauber2016identification}.

\begin{itemize}
	\item \textbf{R1: Data Versioning} \\
	Changes on a data record must result in a new version of the data record and the persistence of the deprecated data records. All data record versions have to be identifiable and accessible. 
	\item \textbf{R2: Timestamping} \\
	All changes to the database have to be comprehensible via timestamps. Every time changes are applied to the data, there has to be a time stamp persisted to describe when it happened. 
	\item \textbf{R3: Query Store Facilities} \\
	There has to be a query store implemented at the data provider to store queries including their \DIFdelbegin \DIFdel{meta-data}\DIFdelend \DIFaddbegin \DIFadd{data}\DIFaddend , to be able to re-execute them in the future. The database has to store, according to \cite{rauber2016identification}, the following things: 
	\begin{itemize}
		\item The original query as applied to the database
		\item A potentially re-written unique query created by the system (R4, R5)
		\item Hash of the (unique) query to detect duplicate queries (R4)
		\item Hash of the result set (R6)
		\item Query execution time stamp (R7)
		\item Persistent identifier of the data source
		\item Persistent identifier for the query (R8)
		\item Other \DIFdelbegin \DIFdel{metadata }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend (e.g. author or creator information) required by the landing page (R11)
	\end{itemize}
	\item \textbf{R4: Query Uniqueness} \\
	Since it is not desirable to have equal queries with the same result stored at the query store, there needs to be a normalized query that can be directly compared to other queries. Hence, there needs to be an algorithm to normalize the queries and to guarantee their uniqueness.
	\item \textbf{R5: Stable Sorting} \\
	The sorting of the resulting data has to be unambiguous if the sequence of data item presentation is essential for the reproduction.
	\item \textbf{R6: Result Set Verification} \\
	To ensure that the resulting data of the query is comparable there have to be a checksum or hash key of it. 
	\item \textbf{R7: Query Timestamping} \\
	There has to be a time stamp assigned to every query in the query store, which can be set to the latest update of the entire database, or the query dependent data of the database, or simply the time of query execution.
	\item \textbf{R8: Query PID}\\
	Every query record in the query store must have a Persistent Identifier (PID). There should not be a query with the same normalized query and the same query result checksum. 
	\item \textbf{R9: Store the Query} \\
	The data described in previous recommendations have to be persisted in the query store.
	\item \textbf{R10: Automated Citation Texts} \\
	To make the citation of the data more convenient for researchers, there shall be a automatic generation for the citation text snipped containing the \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend PID.
	\item \textbf{R11: Landing Page} \\
	The PID shall be resolvable in a human readable landing page, where data mentioned in the previous recommendations is provided to the scientist.
	\item \textbf{R12: Machine Actionability} \\
	Providing an API landing page so that not only humans, but machines can access the \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend by resolving the PID.
	\item \textbf{R13: Technology Migration} \\
	If the database where the query store is implemented needs to be migrated to a new system, the queries need to be transferred too. In addition the queries have to be updated according to the new setup, so that they still work exactly like in the old system.
	\item \textbf{R14: Migration Verification} \\
	There shall be a service to verify a data and query migration (see R13) automatically, to prove that the queries in the query store are still correct. 
\end{itemize}
The implementation of the recommendations for the purpose of this thesis is described in Section \ref{Implementation:Data Identification}. \DIFdelbegin %DIFDELCMD < \todo{Add Evaluation Link to this}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{Section \ref{Evaluation:dataidentification} shows the evaluation of the data identification implementation. 
}

\DIFaddend \section{Tools for Reproducibility}\label{Existing Tools}
The section describes tools that are designed to solve similar problems or subproblems that are addressed by this thesis. There is an explanation \DIFaddbegin \DIFadd{of }\DIFaddend why the specific tool was not used for the prototype of this thesis or how it was used in parts of the solution. 

\subsection{noWorkflow}\label{Noworkflow}
noWorkflow is introduced in \cite{c9e0604becba42af96a9cb0a6f60018b} as a script provenance capturing tool with the aim to not influence the way researchers implement experiments. As proof of concept\DIFaddbegin \DIFadd{, }\DIFaddend the noWorkflow command line tool got implemented for python. The provenance is captured in \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{an }\DIFaddend SQLite database by different trials. A trial represents the environment information of one execution. The main benefit of noWorkflow is that it does not instrument the code\DIFaddbegin \DIFadd{, }\DIFaddend and it automatically captures the definition, deployment\DIFaddbegin \DIFadd{, }\DIFaddend and execution environment in a local SQLite database.  The \DIFdelbegin \DIFdel{stored meta-data can be accessed via the }\DIFdelend command line interface of noWorkflow \DIFaddbegin \DIFadd{is capable of providing access to the stored data}\DIFaddend . In addition to just retrieving the information about the execution environment, analyses features are added \cite{c9e0604becba42af96a9cb0a6f60018b}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{noworkflow}
	\caption{Architecture of noWorkflow \cite{c9e0604becba42af96a9cb0a6f60018b}}
	\label{fig:noworkflow} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

The noWorkflow framework got improved regularly after the first announcement. \DIFdelbegin \DIFdel{In \mbox{%DIFAUXCMD
\cite{Pimentel2016TrackingAA} }\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{Next, }\DIFaddend an additional feature of tracking the evolution of the experiment \DIFdelbegin \DIFdel{execution is added }\DIFdelend \DIFaddbegin \DIFadd{executions is added \mbox{%DIFAUXCMD
\cite{Pimentel2016TrackingAA}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend . It improves the possibility to compare different trials of an experiment and to visualize the history of past executions. In \cite{Pimentel:2016:FPC:3090188.3090214} the fine-grained provenance tracking extension of noWorkflow is introduced. With it\DIFaddbegin \DIFadd{, }\DIFaddend the execution of the python script can be viewed as a set of execution lines. It adds a visualization of all called functions with the exclusion of calls in one line on complex data structures such as dictionaries, lists\DIFaddbegin \DIFadd{, }\DIFaddend and objects. In \cite{69bac1252a684629baa43b48e350068d} the provenance capturing of noWorkflow in combination of yesWorkflow, which gathers information about the provenance using comments and annotations (see \cite{192094}), got combined. The combination enabled \DIFdelbegin \DIFdel{a }\DIFdelend more detailed environment information, querying\DIFaddbegin \DIFadd{, }\DIFaddend and visualizations. 
In this thesis\DIFaddbegin \DIFadd{, }\DIFaddend noWorkflow was used in an early attempt for the implementation \DIFdelbegin \DIFdel{, }\DIFdelend but is not part of the final solution due to the high amount of the captured data by noWorkflow and the additional requirements needed by the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend for using it.

\subsection{ReproZip}\label{ReproZip}
ReproZip is a packaging tool to enable the reproducibility of computational executions of any kind. It automatically tracks the dependencies of an experiment and saves it to a package that can be executed on another machine by ReproZip. It is even capable of letting the re-executor modify the original experiment. It was developed for the SIGMOD Reproducibility Review. In Figure \ref{fig:reprozip}\DIFaddbegin \DIFadd{, }\DIFaddend the architecture of ReproZip is shown in detail. It traces the system calls to create a package defined in the configuration file. Thus, a single file with the extension “.rpz” gets produced. These type of files can then be unpacked on a different machine and re-executed. \DIFdelbegin \DIFdel{The aim of ReproZip is }\DIFdelend \DIFaddbegin \DIFadd{ReproZip aims }\DIFaddend to make reproducible science easy to apply for single experiments \cite{29c5846926a4497d95f276604cb0368c}. The reason why it is not used in the solution of this thesis \DIFdelbegin \DIFdel{, }\DIFdelend is that the capturing is very fine granulated and takes too much performance from the \DIFdelbegin \DIFdel{back ends}\DIFdelend \DIFaddbegin \DIFadd{backends}\DIFaddend , which is a key selling point for \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend providers. Depending on the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , the payment for users may be dependent on the duration time of the processing. Another issue with ReproZip in the context of this thesis is that it is not capable of capturing the big data of the \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend within the package, because it would take too much space and performance.   

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{reprozip}
	\caption{Overview of the ReproZip concept from \cite{29c5846926a4497d95f276604cb0368c} }
	\label{fig:reprozip} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\subsection{Docker / Smartcontainer}\label{Smartcontainer}
Docker containers are \DIFdelbegin \DIFdel{very common }\DIFdelend \DIFaddbegin \DIFadd{ubiquitous }\DIFaddend in geoscience executions. The advantages \DIFdelbegin \DIFdel{on }\DIFdelend \DIFaddbegin \DIFadd{of }\DIFaddend reproducible research and cost savings by using \DIFdelbegin \DIFdel{docker containers is }\DIFdelend \DIFaddbegin \DIFadd{Docker containers are }\DIFaddend discussed in \cite{rs9030290} for the \gls{geobia}. The docker implementation of the image analysis was implemented with a docker image\DIFaddbegin \DIFadd{, }\DIFaddend including a user interface that can be used by non-experts. There are experiments for the more general \gls{obia} with \DIFdelbegin \DIFdel{docker }\DIFdelend \DIFaddbegin \DIFadd{Docker }\DIFaddend containers presented in \cite{proceedings456}. The conclusion of the \DIFdelbegin \DIFdel{previous }\DIFdelend \DIFaddbegin \DIFadd{previously }\DIFaddend mentioned paper is \DIFdelbegin \DIFdel{very positive }\DIFdelend \DIFaddbegin \DIFadd{definite, }\DIFaddend with only little shortcomings in the usability. The two papers mentioned above are using the docker images to make it easy to re-run an experiment on the OBIA system. \DIFdelbegin \DIFdel{Remaining }\DIFdelend \DIFaddbegin \DIFadd{The remaining }\DIFaddend question is how the docker configuration can be preserved in a manner so that it can be reproduced in different environments. The aim of \cite{emsley2017a} is to answer this question by introducing, in addition to the Docker description file, a workflow record saving the environment and entities involved. A SPARQL query got introduced to create the possibility to use the container as a repository of \DIFdelbegin \DIFdel{metadata}\DIFdelend \DIFaddbegin \DIFadd{data}\DIFaddend .\\ 
Another approach of preserving a docker container \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{is a }\DIFaddend smart container introduced in \cite{Huo2015SmartCA}. The aim of \DIFaddbegin \DIFadd{a }\DIFaddend smart container is an ontology and software to preserve docker \DIFdelbegin \DIFdel{metadata}\DIFdelend \DIFaddbegin \DIFadd{data}\DIFaddend . It uses the PROV-O standard to define the provenance. 
Docker containers are used at the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend for running all services\DIFdelbegin \DIFdel{, therefore }\DIFdelend \DIFaddbegin \DIFadd{. Therefore }\DIFaddend they are part of the solution. The description files of the used docker containers are persisted in the GitHub repository of the EODC \DIFdelbegin \DIFdel{back end, hence }\DIFdelend \DIFaddbegin \DIFadd{backend. Hence }\DIFaddend they are identifiable by the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version defined in Section \ref{Design}.   


\subsection{Version Control Systems}\label{Version Control Systems}
\gls{vcs} became \DIFdelbegin \DIFdel{a very important part in }\DIFdelend \DIFaddbegin \DIFadd{an essential part of }\DIFaddend all computational sciences. It enables to persist versions of code and the possibility to head back to a \DIFdelbegin \DIFdel{certain }\DIFdelend \DIFaddbegin \DIFadd{particular }\DIFaddend version of it. Before that\DIFaddbegin \DIFadd{, }\DIFaddend programmers tend to have multiple directories to version the code. The basic idea of VCS is that via a command line interface it is possible to set a version of the current state of the code. These versions can be accessed in \DIFaddbegin \DIFadd{the }\DIFaddend future, without changing other versions of the code and without multiple folders \cite{10.1109/MCSE.2009.194}. 
In this thesis\DIFaddbegin \DIFadd{, }\DIFaddend Gitorious (Git) is used as \DIFaddbegin \DIFadd{the }\DIFaddend version control system. Versions in \DIFdelbegin \DIFdel{git }\DIFdelend \DIFaddbegin \DIFadd{Git }\DIFaddend are defined as commits and are stored locally, but can be published to an external server, where, depending on the user rights, they are accessible for other users. The commits are stored locally and remotely \cite{QuickGit}. \DIFdelbegin \DIFdel{In exampleOpenEO }\DIFdelend \DIFaddbegin \DIFadd{For example, openEO }\DIFaddend uses Git as \DIFaddbegin \DIFadd{a }\DIFaddend code version tool\DIFaddbegin \DIFadd{, }\DIFaddend and GitHub is used as the publicly available server. Since \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend is an open source project, the code of every \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , core API and \DIFaddbegin \DIFadd{the }\DIFaddend client is available at GitHub. 

\subsection{Hash}\label{Hash}
Hash functions are used to validate the data without having to save the whole data. They have three important properties to work \DIFdelbegin \DIFdel{properly. First}\DIFdelend \DIFaddbegin \DIFadd{correctly. First, }\DIFaddend the probability that two different inputs have the same hash outcome has to be low. Second\DIFaddbegin \DIFadd{, }\DIFaddend it needs to be hard to find a message with the same hash value as an already known message. The properties described above makes the hash functionality a \DIFdelbegin \DIFdel{common }\DIFdelend \DIFaddbegin \DIFadd{standard }\DIFaddend tool to identify data without having to save the original one \cite{3b412889270f46f59740fbf1ca8cd7e0}.  
There are different hash functions available. In this thesis\DIFaddbegin \DIFadd{, }\DIFaddend the \gls{sha}-256 is used for the \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend of the context model, mostly to compare differences in data outcomes.

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{sha}
%	\caption{\cite{shapaper}}
%	\label{fig:sha} % \label has to be placed AFTER \caption (or %\subcaption) to produce correct cross-references.
%\end{figure}
%The SHA-256 is chosen in the implementation, because of the best combination of performance and security of the above described properties of the hash. 



\section{\DIFdelbegin \DIFdel{OpenEO}\DIFdelend \DIFaddbegin \DIFadd{openEO}\DIFaddend }\DIFdelbegin %DIFDELCMD < \label{OpenEO}
%DIFDELCMD < %%%
\DIFdel{The OpenEO }\DIFdelend \DIFaddbegin \label{openEO}
\DIFadd{The openEO }\DIFaddend project consists of three modules, the client module written in the programming language of the users, the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend drivers that enables for every \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend to understand the calls from the clients and the core API that specifies how the communication takes place. The core API is a standard that the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend providers accepted to implement on their systems. The \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend drivers are the translation of the client calls to the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend specific API. This architecture decouples the clients from the \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend so that every client can connect to every \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend that complies with the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend core API standard see Figure \ref{fig:api2}. An example of a workflow would be the example defined in Section \ref{example} that a scientist wants to run on the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend and defines the processing with the python client in python code \cite{openeo}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{api2}
	\caption{Overview of the \DIFdelbeginFL \DIFdelFL{OpenEO }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{openEO }\DIFaddendFL architecture}
	\label{fig:api2} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}


The communication is specified as an OpenAPI description, which is a way of defining \gls{rest}ful communication in a standardized way. The definition consists of the \DIFdelbegin \DIFdel{end points at the back end }\DIFdelend \DIFaddbegin \DIFadd{endpoints at the backend }\DIFaddend and the requests and the responses. The whole communication protocol is specified with OpenAPI \cite{openapi}. 
In the following\DIFaddbegin \DIFadd{, }\DIFaddend the relevant RESTful request types in \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend and the policy of choosing between them are introduced.

\begin{itemize}
	\item \textbf{GET Request} \\
	GET requests are used to retrieve data and \DIFdelbegin \DIFdel{meta-data from the back ends}\DIFdelend \DIFaddbegin \DIFadd{data from the backends}\DIFaddend . The functionality is limited to read operations on the data records. \\(e.g. GET /collections returns a list of available collections at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .)
	\item \textbf{POST Request} \\ 
	POST requests are used to create new data and \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend records at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . It is also used to carry information to the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .  \\(e.g. POST /jobs creates a new processing job at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , which is defined in the body of the request.)  
	\item \textbf{PATCH Request} \\
	PATCH requests are used to update an existing record at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . \\(e.g. /PATCH /job/{job\_id} modifies an existing job at the \DIFdelbegin \DIFdel{back-end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend but maintains the identifier.)
	\item \textbf{DELETE Request} \\ 
	DELETE requests are used to remove existing records from the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . \\(e.g. /DELETE /job/{job\_id} removes an existing job from the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .)
\end{itemize}

\subsection{Job Execution}\label{Job Execution}
The job execution workflow in \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend starts at a client application that lets the user define what has to be processed in the \DIFdelbegin \DIFdel{client specific }\DIFdelend \DIFaddbegin \DIFadd{client-specific }\DIFaddend programming language.
The main part of the job execution definition is based on the description of what input data shall be used, which filters have to be applied\DIFaddbegin \DIFadd{, }\DIFaddend and the processes that should be executed on the data. Therefore, \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend introduces the process graph, which is defined as a tree structure describing the processes with their data and the input data identifier. The input data id is \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend specific. The process graph has a \gls{json} format and gets generated by the clients in the background without users noticing it directly. In Figure \ref{fig:process_graph} there is an example of a process graph.  

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{process_graph}
	\caption{Process graph of the running example defined in Section \ref{example}}
	\label{fig:process_graph} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

The \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend interpret the process graph from inside out. Figure \ref{fig:process_graph} displays the process graph of the running example of Section \ref{example}. It \DIFdelbegin \DIFdel{describes }\DIFdelend \DIFaddbegin \DIFadd{described }\DIFaddend the calculation of the minimum NDVI image of the Sentinel 2 satellite over \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend in May 2017. The element in the center of the process graph defines the input data identifier in the "imagery" block, with the "get\_collection" process id. In this case\DIFaddbegin \DIFadd{, }\DIFaddend the "s2a\_prd\_msil1c" is chosen as input data identifier, because it is the identifier for Sentinel 2 at the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .
After reading the input data id, the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend iterates one step up in the hierarchy of the process graph and calls the process "filter\_bbox" with the parameters "left", "right" and so on, which is responsible \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend filtering the image spatial (e.g. the area over \DIFdelbegin \DIFdel{south tyrol}\DIFdelend \DIFaddbegin \DIFadd{South Tyrol}\DIFaddend ). After that\DIFaddbegin \DIFadd{, }\DIFaddend the "filter\_daterange" process is used to only take imageries from May 2017. Every process beginning with "filter\_" is a filter operation that specifies the selection of the input data. The output data of the previous process is the input data of the next process. After the last filtering process, the NDVI gets called by "\DIFdelbegin \DIFdel{/user/custom\_ndvi}\DIFdelend \DIFaddbegin \DIFadd{NDVI}\DIFaddend " with the parameters "red" and "nir", which take the identifier of the bands of \DIFdelbegin \DIFdel{near infrared }\DIFdelend \DIFaddbegin \DIFadd{near-infrared }\DIFaddend and red light, which is needed by the NDVI process (see Section \ref{example}). After that\DIFaddbegin \DIFadd{, }\DIFaddend the minimum value is taken from all images using the "min\_time" process, which then results in a single image. In Figure \ref{fig:process_graph_diagram}\DIFaddbegin \DIFadd{, }\DIFaddend the same process graph is visualized from the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend point of view, visualizing the order of how it gets executed. To transfer the process graph in Figure \ref{fig:process_graph} at a \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , it gets sent in the body of the POST /jobs endpoint request.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{backend_pg}
	\caption{Action chain of the \DIFdelbeginFL \DIFdelFL{back end }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{backend }\DIFaddendFL after receiving the process graph of Figure \ref{fig:process_graph}}
	\label{fig:process_graph_diagram} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

There are two different kinds of process executions depending on the \DIFdelbegin \DIFdel{back ends capabilities }\DIFdelend \DIFaddbegin \DIFadd{capabilities of the backend}\DIFaddend , synchronous and asynchronous calls. Synchronous calls are directly executed after the \DIFdelbegin \DIFdel{back end receives them}\DIFdelend \DIFaddbegin \DIFadd{backend receives them, }\DIFaddend and the user has to wait until the job is finished. For example\DIFaddbegin \DIFadd{, }\DIFaddend on the python client\DIFaddbegin \DIFadd{, }\DIFaddend the program waits after sending the process graph to the \DIFdelbegin \DIFdel{back end until the back end }\DIFdelend \DIFaddbegin \DIFadd{backend until the backend }\DIFaddend returns the result, which is directly returned to the user. An asynchronous call does not get executed until the user starts the execution on the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend through an additional endpoint call. When the processing is finished\DIFaddbegin \DIFadd{, }\DIFaddend the user can download the result at another endpoint of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . For asynchronous calls\DIFaddbegin \DIFadd{, }\DIFaddend there is the possibility to subscribe to a notification system on the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , so that the user gets notified when the job execution finished.     
The processes are defined at the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend core API and therefore independent of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend they get called at, other than the data identifier, which is different for every \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .  
\\
The previous example uses a process graph that only consists of the available processes and data of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . Within the \DIFdelbegin \DIFdel{OpenEO}\DIFdelend \DIFaddbegin \DIFadd{openEO}\DIFaddend \footnote{https://github.com/Open-EO/openeo-openshift-driver/tree/release-0.0.2} project\DIFaddbegin \DIFadd{, }\DIFaddend there is the possibility to define individual processes and execute them on the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . In the project\DIFaddbegin \DIFadd{, }\DIFaddend they are called “user defined functions” and are at the writing of this thesis still not well-defined \DIFdelbegin \DIFdel{, but are basically }\DIFdelend \DIFaddbegin \DIFadd{but are }\DIFaddend code written by the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend user that gets sent to the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend and executed there at a secure environment. The user can define processes and can run them with the data provided at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , using the infrastructure of the \DIFdelbegin \DIFdel{back end. Every back end has to individually }\DIFdelend \DIFaddbegin \DIFadd{backend. Every backend has to }\DIFaddend define what the restrictions on user defined functions are. 

\subsection{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend Overview}\DIFdelbegin %DIFDELCMD < \label{Back end Overview}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{Backend Overview}
\DIFaddend Even though the \DIFdelbegin \DIFdel{back ends implement the OpenEO }\DIFdelend \DIFaddbegin \DIFadd{backends implement the openEO }\DIFaddend core API standard, they are still diverse behind this abstraction layer. Some \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend have already an API, where the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend calls have to be adapted to. There are 7 partners within the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project that are implementing a \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver. The \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend have to manage the translation of the process graph to the actual code that executes the defined process chain. The billing of the users can be completely different on every \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . In Table \ref{Tab:backends} there is an overview of all contributing \DIFdelbegin \DIFdel{OpenEO back ends }\DIFdelend \DIFaddbegin \DIFadd{openEO backends }\DIFaddend and the related GitHub repository.
\begin{table}[]
	\caption{List of all \DIFdelbeginFL \DIFdelFL{back end }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{backend }\DIFaddendFL providers of the \DIFdelbeginFL \DIFdelFL{OpenEO }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{openEO }\DIFaddendFL project}
	\begin{tabular}{l|l}
		\textbf{Organisation} & \textbf{GitHub}  \\ \hline
		EODC & \url{https://github.com/Open-EO/openeo-openshift-driver} \\ \hline 
		VITO & \url{https://github.com/Open-EO/openeo-geopyspark-driver} \\ \hline  
		Google  & \url{https://github.com/Open-EO/openeo-earthengine-driver} \\ \hline  
		Mundialis & \url{https://github.com/Open-EO/openeo-grassgis-driver} \\ \hline 
		JRC & \url{https://github.com/Open-EO/openeo-jeodpp-driver} \\ \hline
		WWU & \url{https://github.com/Open-EO/openeo-r-backend} \\ \hline
		Sinergise & \url{https://github.com/Open-EO/openeo-sentinelhub-driver} \\ \hline
		EURAC & \url{https://github.com/Open-EO/openeo-wcps-driver} \\ 
	\end{tabular}
	\label{Tab:backends}
\end{table}

\subsection{EODC Back End}\label{EODC Back End}

The EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is one of the contributing \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend providers of the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project. It is\DIFdelbegin \DIFdel{in general }\DIFdelend \DIFaddbegin \DIFadd{, in general, }\DIFaddend a python implemented \DIFdelbegin \DIFdel{back end that uses virtualisation technologies for the }\DIFdelend \DIFaddbegin \DIFadd{backend that uses virtualization technologies for }\DIFaddend job execution. The overlaying technology is OpenShift (using Kubernetes) \footnote{https://www.openshift.com/learn/what-is-openshift/}, which is capable of scaling \DIFdelbegin \DIFdel{docker }\DIFdelend \DIFaddbegin \DIFadd{Docker }\DIFaddend containers and handles the execution of them. In the docker containers\DIFaddbegin \DIFadd{, }\DIFaddend the python code for the processing gets executed. The docker description files and the python code is available on GitHub. In this thesis\DIFaddbegin \DIFadd{, }\DIFaddend the latest version of the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provided in GitHub with the version 0.3.1 is used. In this version\DIFaddbegin \DIFadd{, }\DIFaddend every process of the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend process graph is represented by an own \DIFdelbegin \DIFdel{docker }\DIFdelend \DIFaddbegin \DIFadd{Docker }\DIFaddend container running the python code needed. The \DIFaddbegin \DIFadd{python library Flask accomplishes the }\DIFaddend service layer for accessing the \DIFdelbegin \DIFdel{back end is accomplished by the python library Flask}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . EODC provides only data from Sentinel 2 and Sentinel 1 within the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project. The provided data are satellite images from the European Space Agency (ESA), which gets the raw data coming directly from the Sentinel satellites. \\
The data management of EODC is file-based, so every image data is stored in a \DIFdelbegin \DIFdel{unique }\DIFdelend \DIFaddbegin \DIFadd{different }\DIFaddend directory and filename combination. \DIFdelbegin \DIFdel{Meta-data }\DIFdelend \DIFaddbegin \DIFadd{Data }\DIFaddend is provided via PostgreSQL database including the PostGIS\footnote{https://postgis.net} plug-in. The provided query tool for EODC users is the \gls{ogc} standard interface\gls{csw}\footnote{http://cite.opengeospatial.org/pub/cite/files/edu/cat/text/main.html}. In Figure \ref{fig:eodceer}\DIFaddbegin \DIFadd{, }\DIFaddend an overview of the EODC \DIFdelbegin \DIFdel{meta-database }\DIFdelend \DIFaddbegin \DIFadd{database }\DIFaddend is displayed. The structure is retrieved from the \DIFdelbegin \DIFdel{github }\DIFdelend \DIFaddbegin \DIFadd{GitHub }\DIFaddend repository of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend \footnote{https://github.com/Open-EO/openeo-openshift-driver}, where every database entity is defined. Every Process entity can have parameters described by the Parameter entity. The \DIFdelbegin \DIFdel{ProcessNode }\DIFdelend \DIFaddbegin \DIFadd{process node }\DIFaddend is representing one node in a process graph and is therefore related to exactly one ProcessGraph entity. Every Job has a related process graph, there may be jobs that use the same process graph, but in the current set up they are persisted both in the database.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{eodc_eer}
	\caption{Overview of the EODC \DIFdelbeginFL \DIFdelFL{meta-database }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{database }\DIFaddendFL structure.}
	\label{fig:eodceer} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\section{Summary}

For the research areas that are related to the solution of the thesis\DIFaddbegin \DIFadd{, }\DIFaddend a vast amount of literature is available. The problem description of the thesis has many possible solutions described in this chapter. The outcome of this thesis has the aim of making it easy for the scientists to reproduce experiments, other than presented solutions for earth observation science like the VZJ approach or the GPF, which tries to motivate scientists to do it themselves. The implementation of Section \ref{Implementation} builds on existing systems like the implementation of the CCCA query store. The following chapter describes the design of the solution. 

 
\chapter{Design}\label{Design}
This chapter describes the concept of capturing the environment of a geoscientific experiment. \DIFdelbegin \DIFdel{The aim of this chapter is }\DIFdelend \DIFaddbegin \DIFadd{This chapter aims }\DIFaddend to explain the general concept of how to gain reproducibility in an earth observation environment. It is structured in six parts. The first part presents an overview of the concept. The next three sections describe the main components of the overview in detail, first the data identification part, then the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance and the job dependent provenance. The following section defines the resulting context model. The next section of this chapter defines the user services needed to use the data of the context model from an earth observation scientist's perspective. \DIFdelbegin \DIFdel{The last section }\DIFdelend \DIFaddbegin \DIFadd{Section \ref{Design:User Defined Functions} }\DIFaddend gives an overview of the capturing of \gls{udf}. The following Chapter \ref{Implementation}\DIFaddbegin \DIFadd{, }\DIFaddend shows an implementation of the concept of this chapter at the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . 



\section{Overview}\label{Design:Overview}
This section presents an overview of the concept\DIFdelbegin \DIFdel{and in Figure \ref{fig:overview} it is visualized. The }\DIFdelend \DIFaddbegin \DIFadd{, and Figure \ref{fig:overview} visualizes it. The white boxes represent the }\DIFaddend components that every \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver has in place\DIFdelbegin \DIFdel{are represented by the white boxes}\DIFdelend . The green elements in Figure \ref{fig:overview} are the proposed extensions of this design. The \DIFaddbegin \DIFadd{following steps describe the }\DIFaddend typical job execution \DIFdelbegin \DIFdel{work-flow is described in the following steps}\DIFdelend \DIFaddbegin \DIFadd{workflow}\DIFaddend :

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{design_overview}
	\caption{Overview of the Design}
	\label{fig:overview} 
\end{figure}

 \begin{enumerate}
	\item \textbf{\gls{eo} Client} \\
	The user defines the input data, the filter operations\DIFaddbegin \DIFadd{, }\DIFaddend and the processes that need to be executed at the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend via an earth observation client. Then the user orders the job to be executed\DIFdelbegin \DIFdel{, therefore }\DIFdelend \DIFaddbegin \DIFadd{. Therefore }\DIFaddend the client creates a process graph description. The \DIFaddbegin \DIFadd{backend driver interface (e.g. openEO) sends the }\DIFaddend user identification and the process graph \DIFdelbegin \DIFdel{is sent }\DIFdelend via a RESTful endpoint\DIFdelbegin \DIFdel{defined by the back end driver interface (e.    
	g. OpenEO).  
	}\DIFdelend \DIFaddbegin \DIFadd{.    
	}\DIFaddend \item \textbf{Data Query} \\ 
	The \DIFdelbegin \DIFdel{Data Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Data Query}} \DIFaddend component receives the process graph and parses the data identifier and the filter operations out of it to query for the input data records needed for the job. \DIFdelbegin \DIFdel{These are forwarded to the Process Execution }\DIFdelend \DIFaddbegin \DIFadd{The }\textit{\DIFadd{Data Query}} \DIFadd{forwards these to the }\textit{\DIFadd{Process Execution}} \DIFaddend component.
	\item \textbf{Process Execution} \\
	The \DIFdelbegin \DIFdel{Process Execution }\DIFdelend \DIFaddbegin \textit{\DIFadd{Process Execution}} \DIFaddend component receives the process graph and the input data from the \DIFdelbegin \DIFdel{Data Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Data Query}} \DIFaddend component. It parses the processes from the process graph and executes them in the order of appearance. After every process defined in the process graph got executed, the \DIFdelbegin \DIFdel{Process Execution }\DIFdelend \DIFaddbegin \textit{\DIFadd{Process Execution}} \DIFaddend component forwards the resulting data to the \DIFdelbegin \DIFdel{Result Handling }\DIFdelend \DIFaddbegin \textit{\DIFadd{Result Handling}} \DIFaddend component.   
	\item \textbf{Result Handling} \\ 
	The \DIFdelbegin \DIFdel{Result Handling }\DIFdelend \DIFaddbegin \textit{\DIFadd{Result Handling}} \DIFaddend component receives the results from the \DIFdelbegin \DIFdel{Process Execution }\DIFdelend \DIFaddbegin \textit{\DIFadd{Process Execution}} \DIFaddend component and persists all \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend about the job and the result. In the meantime\DIFaddbegin \DIFadd{, }\DIFaddend the resulting data is sent back to the client application \DIFdelbegin \DIFdel{, }\DIFdelend if the job is not \DIFdelbegin \DIFdel{defined as }\DIFdelend a batch job. If it is a batch job\DIFaddbegin \DIFadd{, }\DIFaddend the user has to \DIFdelbegin \DIFdel{actively }\DIFdelend order the results from the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend actively}\DIFaddend .  
\end{enumerate}

\DIFdelbegin \DIFdel{To make the whole workflow reproducible, every }\DIFdelend \DIFaddbegin \DIFadd{Every }\DIFaddend single component has to be identifiable\DIFaddbegin \DIFadd{, to make the whole workflow reproducible}\DIFaddend . Hence, the following elements get introduced as additional components to the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .

 \begin{itemize}
	\item \textbf{Query Handler} \\
	The \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend component is responsible for applying data identification to the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The query has to be persisted and the resulting data generated by the \DIFdelbegin \DIFdel{Data Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Data Query}} \DIFaddend component has to be identifiable by a PID. Section \ref{Design:Data Identification} describes the \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend in more detail.     
	\item \textbf{Job Capturing} \\ 
	The \DIFdelbegin \DIFdel{Job Capturing }\DIFdelend \DIFaddbegin \textit{\DIFadd{Job Capturing}} \DIFaddend component is responsible for enabling code identification of the used software at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . Therefore, a PID of the code used for the processing has to be introduced. In addition, \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend of the job execution environment gets captured, to gain feedback information for the users and capture the environment of the execution. Section \ref{Design:Job Capturing} describes the \DIFdelbegin \DIFdel{Job Capturing }\DIFdelend \DIFaddbegin \textit{\DIFadd{Job Capturing}} \DIFaddend component in more detail.
	\item \textbf{Result Handler} \\
	The \DIFdelbegin \DIFdel{Result Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Result Handler}} \DIFaddend component is responsible for creating a comparable result checksum or hash. The data created by an earth observation \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend might be too big to be persisted, hence there need to be a checksum or hash to be capable of confirming equal results. Section \ref{Design:Result Handler} describes the \DIFdelbegin \DIFdel{Result Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Result Handler}} \DIFaddend component in more detail.   
	\item \textbf{Context Model} \\ 
	The \DIFdelbegin \DIFdel{Context Model }\DIFdelend \DIFaddbegin \DIFadd{context model }\DIFaddend is not a component, but a data set that contains all the \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend mention in the previous components. One context model is related to one job execution. Section \ref{Design:Context Model} provides more detailed information about the \DIFdelbegin \DIFdel{Context Model}\DIFdelend \DIFaddbegin \DIFadd{context model}\DIFaddend . 
\end{itemize}

\section{Query Handler}\label{Design:Data Identification}
The input data of the processing is crucial for the outcome of the job execution. Even though the process graph already contains an identifier of the input data, unique for the specific \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , internal changes to the data set might not result in a new identifier. So jobs called later \DIFdelbegin \DIFdel{in time }\DIFdelend might use another version of the input data than previous jobs. \DIFdelbegin \DIFdel{To prevent this the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend input data has to be persisted according to the 14 steps of data provenance defined by the RDA \cite{rauber2016identification}\DIFdelbegin \DIFdel{. Every back end }\DIFdelend \DIFaddbegin \DIFadd{, to prevent this. Every backend }\DIFaddend is responsible for applying data identification. It is assumed that the \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend preserve the data as described in Section \ref{Data Identification}. The \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend is the module where the data persistence is implemented and depends highly on the structure and architecture of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . Therefore, \DIFdelbegin \DIFdel{no common definitions can be made for all back ends }\DIFdelend \DIFaddbegin \DIFadd{there is no standard definition for all backends }\DIFaddend in this section. In Chapter \ref{Implementation} there is a fully implemented solution at the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The input data is represented in the context model by the following element: 

\begin{enumerate}[(a)]
\item \textbf{Input data persistent identifier} \\
	The output of the \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend is the PID of the input data related to a job execution. Every job execution uses input data, which has a PID assigned to it. Therefore, the PID is added to the job dependent context model. 
\end{enumerate}

\section{Job Capturing}\label{Design:Job Capturing}
\DIFdelbegin \DIFdel{The aim of this section is }\DIFdelend \DIFaddbegin \DIFadd{This section aims }\DIFaddend to describe the job execution capturing and a description of data that shall be captured. The \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend provided by the job capturing can be categorized in static and dynamic data, where static data is defined as not dependent on job properties\DIFaddbegin \DIFadd{, }\DIFaddend and dynamic data is defined as job dependent information. In the following two sections\DIFaddbegin \DIFadd{, }\DIFaddend the static data (in this thesis called \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance) and dynamic data (in this thesis called job dependent provenance) are further described.      

\subsection{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend provenance}\DIFdelbegin %DIFDELCMD < \label{Design:Back end provenance}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{Design:Backend provenance}
\DIFaddend The scope of this part of the context model is to get the static environment \DIFdelbegin \DIFdel{of }\DIFdelend where the job execution at the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend takes place. It contains the provenance data that is independent of \DIFdelbegin \DIFdel{a }\DIFdelend job execution, so it does not change through job executions, regardless of how many jobs were processed. It can only change from inside of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend by their maintainers. The earth observation community has a great variety of different \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend providers with very distinct setups. The challenge of this part of the design is to make it simple and generic. Data captured in this part of the context model is not meant to be shown to the user directly \DIFdelbegin \DIFdel{, }\DIFdelend because of security issues. The following \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend is suggested for the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance:

\begin{enumerate}[(A)]
	\item \textbf{Code identification} \\
	Every earth observation \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend has to provide an identifier for the \DIFdelbegin \DIFdel{used }\DIFdelend code. Therefore, a version control system has to be applied to the running services. If the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is open source, it can provide a GitHub repository \DIFdelbegin \DIFdel{were the used }\DIFdelend \DIFaddbegin \DIFadd{where the }\DIFaddend code is stored and publicly available. \DIFdelbegin \DIFdel{The aim of this strategy is }\DIFdelend \DIFaddbegin \DIFadd{This strategy aims }\DIFaddend to get other \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend with similar settings to reuse the already working setups of the running \DIFdelbegin \DIFdel{back ends}\DIFdelend \DIFaddbegin \DIFadd{backends}\DIFaddend . In this case\DIFaddbegin \DIFadd{, }\DIFaddend the information is added to the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance by saving the git repository URL, the used commit identifier and local changes to the repository. If the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is not open source, a local or secure version control repository has to be implemented to keep track of the different code versions. The code identifier is necessary to identify differences in the \DIFdelbegin \DIFdel{back end code}\DIFdelend \DIFaddbegin \DIFadd{backend code, }\DIFaddend and the version control system enables to jump back to versions of interest.  
	\item \textbf{API version} \\
	The \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend API version is the version of the interface API it currently uses. The API defines the syntax and \DIFdelbegin \DIFdel{semantic of the }\DIFdelend \DIFaddbegin \DIFadd{semantics of }\DIFaddend communication. The same API calls can resolve in different results if the version of it differs from the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution, hence the API version is persisted in the context model.
	\item \textbf{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend version} \\
	The \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version is an identifier that gets updated on every change of the \DIFdelbegin \DIFdel{back end. This }\DIFdelend \DIFaddbegin \DIFadd{backend. It }\DIFaddend includes changes on the hardware and software \DIFdelbegin \DIFdel{, }\DIFdelend so that every change on the code identifier or the API version results in a new \DIFdelbegin \DIFdel{back end version. To provide long term stability of the capturing, it }\DIFdelend \DIFaddbegin \DIFadd{backend version. It }\DIFaddend is essential to implement a tool to automatically capture the environment data and persist it in a separate \DIFdelbegin \DIFdel{back end version store}\DIFdelend \DIFaddbegin \DIFadd{backend version store, to provide long term stability of the capturing, }\DIFaddend . Every change on any of the described \DIFdelbegin \DIFdel{back end meta-data }\DIFdelend \DIFaddbegin \DIFadd{backend data }\DIFaddend has to be detected automatically and have to result in a new \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version.   
	\item \textbf{Publication \DIFdelbegin \DIFdel{time-stamp}\DIFdelend \DIFaddbegin \DIFadd{timestamp}\DIFaddend } \\
	The \DIFdelbegin \DIFdel{time-stamp of }\DIFdelend \DIFaddbegin \DIFadd{publication timestamp describes the time }\DIFaddend when the version was accessible at the \DIFdelbegin \DIFdel{back end. The time-stamp }\DIFdelend \DIFaddbegin \DIFadd{backend. The timestamp }\DIFaddend stands for the beginning of a \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version. If there exists no \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend after that, then it is the \DIFdelbegin \DIFdel{current }\DIFdelend \DIFaddbegin \DIFadd{currently }\DIFaddend used version. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend enables the possibility to find the version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend used by a job executed in the past, by only knowing the time it was executed.    
\end{enumerate}

\subsection{Job dependent environment}\label{Design:Job dependent provenance}
This section describes the job dependent provenance of the context model. The data captured is tied to \DIFdelbegin \DIFdel{a }\DIFdelend specific job execution, so for every job\DIFaddbegin \DIFadd{, }\DIFaddend a new context model gets created. The process graph is already a description of the processes that run at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . So sending the same process graph to the same \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend again shall result in the same outcome. To assure that the process graph can be re-executed in the same way as the original execution, \DIFdelbegin \DIFdel{meta-data about the original }\DIFdelend \DIFaddbegin \DIFadd{data about the first }\DIFaddend execution gets captured. 
The following subsections describe the single elements of the context model in more detail. 

\subsubsection{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend provenance}\DIFdelbegin %DIFDELCMD < \label{Job:Back end provenance}
%DIFDELCMD < %%%
\DIFdel{The back end }\DIFdelend \DIFaddbegin \label{Job:Backend provenance}
\DIFadd{The backend }\DIFaddend provenance described in Section \DIFdelbegin \DIFdel{\ref{Design:Back end provenance} }\DIFdelend \DIFaddbegin \DIFadd{\ref{Design:Backend provenance} }\DIFaddend defines the version of the code that is executed and therefore\DIFaddbegin \DIFadd{, }\DIFaddend has to be added to the job context model. Over time the \DIFdelbegin \DIFdel{back end provenance will get }\DIFdelend \DIFaddbegin \DIFadd{backend provenance gets }\DIFaddend updated so that every job need to persist a reference to the original \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version at the execution time. There are two solutions to achieve this. First\DIFaddbegin \DIFadd{, }\DIFaddend the identifier of a specific version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend gets added to the context model. In this case\DIFaddbegin \DIFadd{, }\DIFaddend the data of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment versions have to be persisted and accessible. The \DIFdelbegin \DIFdel{second }\DIFdelend \DIFaddbegin \DIFadd{other }\DIFaddend solution is to put a \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend of the execution into the job execution context model.

\begin{enumerate}[(b)]
	\item \textbf{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend provenance / code identifier} \\
	The \DIFaddbegin \DIFadd{backend provenance represents the }\DIFaddend version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance during the job execution. Since for every change on the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend, }\DIFaddend a new version \DIFdelbegin \DIFdel{will be }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend applied, the version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is used as a code identifier for the execution. Another solution is to persist an execution \DIFdelbegin \DIFdel{time-stamp, }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend so that the version of the given time can be identified.
\end{enumerate}

\subsubsection{Process Environment}\label{Job:Process Data}
This section describes the capturing of the process itself. As mentioned above\DIFaddbegin \DIFadd{, }\DIFaddend the incoming process graph describes the process, but to be \DIFdelbegin \DIFdel{certain }\DIFdelend \DIFaddbegin \DIFadd{sure }\DIFaddend that the same process id results in the same code, the code version has to be added to the context model. To be able to identify code from different versions\DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend source control management needs to be installed. An example of doing so is persisting the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend source code on a public platform like GitHub. The entry in the context model related to this was mentioned in Section \DIFdelbegin \DIFdel{\ref{Design:Back end provenance}}\DIFdelend \DIFaddbegin \DIFadd{\ref{Design:Backend provenance}}\DIFaddend .  \\
The way of executing a specific process graph is not only related to the code running it, but also by the dependencies of the code. Therefore, the programming language version and the additional used libraries of it are persisted in the context model. To gain meta-information about the processing the start time and end time can be added to the context model via timestamps. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend leads to the following capturing elements in the job context model.

\begin{enumerate}[(a)]
	\setcounter{enumi}{2}
	\item \textbf{Programming language}\\
	The programming language of the code that is used for the processing. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend the version of the programming language has to be included in this information.
	\item \textbf{Dependencies of the programming language}\\
	\DIFdelbegin \DIFdel{To identify the environment of the code execution, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend dependencies of the programming language shall be captured and added to the context model \DIFaddbegin \DIFadd{to identify the environment of the code execution}\DIFaddend . The version of the packages have a high influence on the outcome, hence can be included in the context model. For example\DIFdelbegin \DIFdel{in python }\DIFdelend \DIFaddbegin \DIFadd{, in python, }\DIFaddend the installed modules are added with their versions to the context model. The information stored in this element is related to the job execution, because it depends on the \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend of the execution, how the environment is configured, since the job execution may be done in a dynamically generated container.
	\item \textbf{Start and end time of process execution}\\
	On the beginning of the process execution and at the end of it, a \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend is created. \DIFdelbegin \DIFdel{These timestamps are persisted in the context model }\DIFdelend \DIFaddbegin \DIFadd{The context model persists these timestamps}\DIFaddend .
\end{enumerate}

\section{Result Handler}\label{Design:Result Handler}
The output data of the processing has to be captured as well to be able to compare job execution outcomes. In earth observation tasks\DIFaddbegin \DIFadd{, }\DIFaddend the results can be rather \DIFdelbegin \DIFdel{big}\DIFdelend \DIFaddbegin \DIFadd{significant}\DIFaddend , that is why a \DIFdelbegin \DIFdel{simple }\DIFdelend \DIFaddbegin \DIFadd{single }\DIFaddend generated hash value of the output result is enough. The output data does not have to be identifiable within the scope of this thesis, but just checkable of identity with other executions. Therefore, a generated hash value of the output data is sufficient. The aim of the output data capturing is not to add the possibility to find differences between results \DIFdelbegin \DIFdel{, }\DIFdelend but to be able to see that the results are different. Even though the input data of typical earth observation experiments are \DIFdelbegin \DIFdel{big}\DIFdelend \DIFaddbegin \DIFadd{significant}\DIFaddend , the output of such experiments are images of various sizes.   

\begin{enumerate}[(f)]
	\item \textbf{Result hash} \\
	The output result of the job need to be verifiable and therefore\DIFaddbegin \DIFadd{, }\DIFaddend need to be persisted. An easy way to achieve this is to take the hash value over the output files sorted by the alphabetical ascending filenames.
\end{enumerate}

 
\section{Context Model}\label{Design:Context Model}

The context model is the data record that saves the provenance of \DIFdelbegin \DIFdel{a }\DIFdelend job execution.  The \DIFaddbegin \DIFadd{backend provider infrastructure defines the }\DIFaddend type of storage it is persisted\DIFdelbegin \DIFdel{in, is defined by the infrastructure of the back end provider}\DIFdelend . It can e.g. be stored in a relational database or \DIFdelbegin \DIFdel{in }\DIFdelend a file-based system as a file. It has to be integrated into the \DIFdelbegin \DIFdel{meta-database of the back end. The back end has to persist }\DIFdelend \DIFaddbegin \DIFadd{database of the backend. The backend has to store }\DIFaddend two different types of datasets. Figure \ref{fig:design_contextmodel} shows an overview of the elements of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend used to run a job. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend it shows how the context model elements are used to identify the components of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{design_context_model}
	\caption{Overview of the \DIFdelbeginFL \DIFdelFL{back end }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{backend }\DIFaddendFL execution components and the context model elements that identifies them.}
	\label{fig:design_contextmodel} 
\end{figure}

The \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment dataset that saves job independent provenance and results in a \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version. The \DIFdelbegin \DIFdel{mandatory }\DIFdelend \DIFaddbegin \DIFadd{necessary }\DIFaddend element of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment is the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version that defines the state of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend in a specific time. The \DIFdelbegin \DIFdel{back end version need }\DIFdelend \DIFaddbegin \DIFadd{backend version needs }\DIFaddend to be resolvable by a specific code version that was present in a past execution. \DIFdelbegin \DIFdel{In addition meta-data about the back end }\DIFdelend \DIFaddbegin \DIFadd{Besides, data about the backend }\DIFaddend can be added. The elements ((A) - (D)) defined in Section \DIFdelbegin \DIFdel{\ref{Design:Back end provenance} }\DIFdelend \DIFaddbegin \DIFadd{\ref{Design:Backend provenance} }\DIFaddend are the suggestions of elements in the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment record. The following list recaps all of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment elements: 

\begin{enumerate}[(A)]
	\item \textbf{Code identification} \\
	Enables the identification of the code of a \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version.
	\item \textbf{API version} \\
	Enables the identification of the API version of a \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version.
	\item \textbf{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend version} \\ 
	Identifies the whole \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .
	\item \textbf{Publication \DIFdelbegin \DIFdel{time-stamp}\DIFdelend \DIFaddbegin \DIFadd{timestamp}\DIFaddend } \\ 
	Enables the identification of a \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version at a specific time.
\end{enumerate}

The job execution context model that includes all information that is dedicated to a specific job is persisted in the context model. There are three \DIFdelbegin \DIFdel{mandatory }\DIFdelend \DIFaddbegin \DIFadd{necessary }\DIFaddend elements in the job context model. The input data identifier has to be persisted in the context model \DIFdelbegin \DIFdel{, }\DIFdelend so that it can be identified and used in future job executions. The used version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend related to the execution has to be included in the context model, to be able to re-execute it with the same code in the future. The output hash has to be added to the job dependent context model \DIFdelbegin \DIFdel{, }\DIFdelend so that different results can be detected. The following list recaps the elements of the job context model described in Section \ref{Design:Job dependent provenance}: 

\begin{enumerate}[(a)]
	\item \textbf{Input data persistent identifier} \\
	Identifies the input data of the job.
	\item \textbf{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend provenance / code identifier} \\
	Identifies the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version (C) during the execution of the job. Connects the job context model with the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment dataset.
	\item \textbf{Programming language} \\
	Identifies the programming language and version used by the job.
	\item \textbf{Dependencies of the programming language} \\
	Identifies the dependencies of the programming language used by the job.
	\item \textbf{Start and end time of process execution} \\
	Provides the start and end time of the job execution.
	\item \textbf{Result hash} \\
	Describes the job results for comparing it with other job results.
\end{enumerate}

\section{User Services}\label{Design:User Interface}

\DIFdelbegin \DIFdel{In this section }\DIFdelend \DIFaddbegin \DIFadd{This section describes }\DIFaddend the information for the users and how it is provided\DIFdelbegin \DIFdel{gets described}\DIFdelend . The capturing described in the previous sections consists of information about the \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend that shall not be \DIFdelbegin \DIFdel{completely passed }\DIFdelend \DIFaddbegin \DIFadd{passed entirely }\DIFaddend to the users in detail. For example, it is maybe a security risk to provide information on specific programming language packages. On the other hand\DIFaddbegin \DIFadd{, }\DIFaddend the users need to be able to see the differences \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{in }\DIFaddend job executions and get environment information about the \DIFdelbegin \DIFdel{back ends}\DIFdelend \DIFaddbegin \DIFadd{backends}\DIFaddend . Therefore, there has to be a filter on which data \DIFdelbegin \DIFdel{can not }\DIFdelend \DIFaddbegin \DIFadd{cannot }\DIFaddend be shown to the users. Every captured information is not necessarily \DIFdelbegin \DIFdel{interesting for the }\DIFdelend \DIFaddbegin \DIFadd{useful for }\DIFaddend users. Data that is not secure to the user has to be defined by the different \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend themselves. The earth observation community has a very diverse set of \DIFdelbegin \DIFdel{back ends and every one }\DIFdelend \DIFaddbegin \DIFadd{backends, and everyone }\DIFaddend has a unique company security guideline.\\ 
Provenance information need be provided to the user. Therefore, there have to be additions to the core API specification. Additional endpoints for the users to get information about the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend have to be created for the core API, \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend, }\DIFaddend and client. The following recommendations are for \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend and client provider to make features related to the context model accessible for the users.

\begin{enumerate}[I.]
\item \textbf{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend version} \\
	There has to be an endpoint for users to retrieve \DIFdelbegin \DIFdel{back end specific information}\DIFdelend \DIFaddbegin \DIFadd{backend specific information, }\DIFaddend especially the current version. The aim of it is to present the users with information about the current state of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend and to help users to decide, which \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend they want to use. 

\item \textbf{Detailed Job Information} \\
	The \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend has to provide an endpoint to retrieve information about an already executed job. The provenance of the job has to be presented in there. At least the resolvable persisted identifier of the input data, the \DIFdelbegin \DIFdel{back end version}\DIFdelend \DIFaddbegin \DIFadd{backend version, }\DIFaddend and the result set hash has to be added to the view. In addition to this\DIFdelbegin \DIFdel{the whole meta-data }\DIFdelend \DIFaddbegin \DIFadd{, the whole data }\DIFaddend of the context model can be made accessible. \DIFdelbegin \DIFdel{To make it more transparent for users the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend resolvable identifier URL of the input data and the GitHub information can be provided\DIFaddbegin \DIFadd{, to make it more transparent for users}\DIFaddend .   

\item \textbf{Comparing Two Jobs} \\
	The API has to add an \DIFdelbegin \DIFdel{end point }\DIFdelend \DIFaddbegin \DIFadd{endpoint }\DIFaddend for providing a comparison of two different jobs.  Every item of both of the context models of both jobs \DIFdelbegin \DIFdel{have }\DIFdelend \DIFaddbegin \DIFadd{has }\DIFaddend to be compared if they are equal or not. Therefore, the response of the comparison request consists of the differences in the context model between two jobs.

\item \textbf{Data Identifier Landing Page} \\
	After the job is executed\DIFaddbegin \DIFadd{, }\DIFaddend the input data has a relating input data PID. The PID has to be resolved by a landing page. The landing page provides the user with information about the input dataset\DIFaddbegin \DIFadd{, }\DIFaddend and the query gets re-executed to view the \DIFdelbegin \DIFdel{concrete }\DIFdelend \DIFaddbegin \DIFadd{actual }\DIFaddend input data files upon request and given sufficient permissions.

\item \textbf{Re-use of Input Data} \\
	The \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend has to provide an endpoint to the user to re-use \DIFdelbegin \DIFdel{a }\DIFdelend specific input data in another job. Therefore, the API allows to include the use of an input data PID in the process graph, so that users can include cited data directly in a \DIFdelbegin \DIFdel{new }\DIFdelend \DIFaddbegin \DIFadd{newly }\DIFaddend created job description.   
\end{enumerate}

\section{User Defined Functions}\label{Design:User Defined Functions}
This section describes a possible capturing method of User Defined Functions (UDF). UDFs are customized code written by the user that gets executed in a virtualization environment at the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provider. In theory\DIFaddbegin \DIFadd{, }\DIFaddend the user defines a docker base image and the code that should run in it. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend is a black box for the \DIFdelbegin \DIFdel{back end provider , }\DIFdelend \DIFaddbegin \DIFadd{backend provider }\DIFaddend since they can not know what processes run in the docker container. Therefore, the capturing concept needs to be different \DIFdelbegin \DIFdel{then the common }\DIFdelend \DIFaddbegin \DIFadd{than the typical }\DIFaddend way of executing jobs at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The code and the docker base image has to be persisted by the \DIFdelbegin \DIFdel{back end provider. In addition }\DIFdelend \DIFaddbegin \DIFadd{backend provider. Besides, }\DIFaddend the timestamp of the execution, to be able to identify the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version at the execution. This is a rather new concept in earth observation science and not implemented in the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , hence the implementation of the capturing is not part of this thesis.

\section{Summary}
This chapter presented the concept of the proposed solution system. The data elements needed to be able to reproduce a job execution are presented and described. \DIFdelbegin \DIFdel{To achieve it the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend data identification has to be implemented according to the RDA recommendations\DIFdelbegin \DIFdel{. The back end }\DIFdelend \DIFaddbegin \DIFadd{, to achieve it. The backend }\DIFaddend provenance got defined by the identification of the software and the version of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The job dependent environment describes the environment of the job execution information and the \DIFdelbegin \DIFdel{result }\DIFdelend \DIFaddbegin \DIFadd{resulting }\DIFaddend hash. The \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend of the job execution got summarized \DIFdelbegin \DIFdel{into }\DIFdelend \DIFaddbegin \DIFadd{in }\DIFaddend the context model. It consists of the data needed to answer the research questions of Chapter \ref{Introduction}. In addition to the data description, the needed user services to provide the functionality for the users got described in the last chapter. The next chapter presents the implementation of the design of this chapter at the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .    

\chapter{Implementation}\label{Implementation}
This chapter presents the EODC prototype of the concept described in Chapter \ref{Design}. The implementation consists of all suggested changes that have to be made to the \DIFdelbegin \DIFdel{OpenEO workflow}\DIFdelend \DIFaddbegin \DIFadd{openEO workflow, }\DIFaddend including parts of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , core API\DIFaddbegin \DIFadd{, }\DIFaddend and the client. Thus, all three parts of the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project architecture get modified in the presented solution. In the \DIFdelbegin \DIFdel{OpenEO project}\DIFdelend \DIFaddbegin \DIFadd{openEO project, }\DIFaddend there is a vast amount of \DIFdelbegin \DIFdel{back ends }\DIFdelend \DIFaddbegin \DIFadd{backends }\DIFaddend and clients implemented in parallel, which are all compatible with each other. One of each of the clients and the \DIFdelbegin \DIFdel{back ends is }\DIFdelend \DIFaddbegin \DIFadd{backends are }\DIFaddend used for the proof of concept of this thesis. The python client\footnote{https://github.com/Open-EO/openeo-python-client} is modified for \DIFdelbegin \DIFdel{the purpose of }\DIFdelend this thesis. The EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend \footnote{https://github.com/Open-EO/openeo-openshift-driver} gets used for the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend part. The motivation for choosing this options is that python is the most common programming language at the contributing \DIFdelbegin \DIFdel{back ends of the OpenEO }\DIFdelend \DIFaddbegin \DIFadd{backends of the openEO }\DIFaddend project. Both of the chosen implementations are using python as their \DIFdelbegin \DIFdel{main }\DIFdelend \DIFaddbegin \DIFadd{primary }\DIFaddend programming language. The adaptations described in Chapter \ref{Design} are implemented for the usage of the python client accessing the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver. The resulting prototype is open source and can\DIFdelbegin \DIFdel{therefore }\DIFdelend \DIFaddbegin \DIFadd{, therefore, }\DIFaddend be used by other \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend providers with a similar setup.  

The implementation is structured in four parts. The following four sections describe these parts in detail. The first section describes the data identification implementation, e.g. the implementation of the RDA recommendations at the EODC \DIFdelbegin \DIFdel{back end. Section \ref{Implementation:Back end provenance} }\DIFdelend \DIFaddbegin \DIFadd{backend. Section \ref{Implementation:Backend provenance} }\DIFaddend is about the implementation of the automated \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance tracking tool that captures data defined in Section \ref{Design:Job Capturing}. After that Section \ref{Implementation:Job dependent provenance} is about the implementation of the job execution provenance. The last section is about the implementation of user interface additions defined in Section \ref{Design:User Interface}.     

\section{Data Identification}\label{Implementation:Data Identification}

As defined in Section \ref{Design:Data Identification}, the RDA recommendations have to be applied to every \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provider individually. This section presents the data identification solution for the EODC \DIFdelbegin \DIFdel{back end. Other OpenEO back end provider }\DIFdelend \DIFaddbegin \DIFadd{backend. Other openEO backend providers }\DIFaddend can use the presented approach as well to enable data identification on their \DIFdelbegin \DIFdel{back end setup. Some }\DIFdelend \DIFaddbegin \DIFadd{backend setup. All backend providers can use some }\DIFaddend parts of the implementation\DIFdelbegin \DIFdel{can be used by all back end providers. The work-flow }\DIFdelend \DIFaddbegin \DIFadd{. The workflow }\DIFaddend of users getting and using the data identifier\DIFaddbegin \DIFadd{, }\DIFaddend including the landing page is described in Section \ref{Implementation:User Interface}. \\

\subsection{Query Store}
The centerpiece of the RDA recommendations is the implementation of a Query Store. Queries in the Query Store must be comparable, \DIFdelbegin \DIFdel{identify able }\DIFdelend \DIFaddbegin \DIFadd{identifiable, }\DIFaddend and persisted. At the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend, }\DIFaddend the Query Store is realized with two \DIFdelbegin \DIFdel{additional }\DIFdelend \DIFaddbegin \DIFadd{new }\DIFaddend tables in the existing PostgreSQL \DIFdelbegin \DIFdel{meta-database}\DIFdelend \DIFaddbegin \DIFadd{database}\DIFaddend . In Figure \ref{fig:eer_rda} the EODC database structure from Figure \ref{fig:eodceer} is visualized with the proposed additional tables. The \DIFdelbegin \DIFdel{additional Query }\DIFdelend \DIFaddbegin \DIFadd{new }\textit{\DIFadd{Query}} \DIFaddend table consists of the query dataset specified by the RDA recommendations. The \DIFdelbegin \DIFdel{QueryJob }\DIFdelend \DIFaddbegin \textit{\DIFadd{QueryJob}} \DIFaddend table defines the relation between Job and Query. In the current version of the \DIFdelbegin \DIFdel{OpenEO core API}\DIFdelend \DIFaddbegin \DIFadd{openEO core API, }\DIFaddend it is only possible to have one input data query used by a job. In the future\DIFaddbegin \DIFadd{, }\DIFaddend there may be the possibility to have more than one input data query related to one job\DIFdelbegin \DIFdel{, hence the QueryJob }\DIFdelend \DIFaddbegin \DIFadd{. Hence the }\textit{\DIFadd{QueryJob}} \DIFaddend table gets introduced. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{eodc_eer_rda}
	\caption{Overview of the \DIFdelbeginFL \DIFdelFL{meta-database }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{database }\DIFaddendFL of EODC with the proposed additional tables (green).}
	\label{fig:eer_rda} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\subsection{Query Handler \DIFdelbegin \DIFdel{Work-flow}\DIFdelend \DIFaddbegin \DIFadd{Workflow}\DIFaddend }
Figure \ref{fig:impldataid} shows an overview of the data identification solution of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The green parts of the diagram are representing the additions proposed in the thesis. The \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend itself is an additional python module in the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend that gets called after the job execution. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{implementation_dataidentification}
	\caption{Overview of the proposed data identification workflow at the EODC \DIFdelbeginFL \DIFdelFL{back end}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{backend}\DIFaddendFL }
	\label{fig:impldataid} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

The \DIFdelbegin \DIFdel{Query Generation and Query Execution }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Generation}} \DIFadd{and }\textit{\DIFadd{Query Execution}} \DIFaddend components of the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provide the input data of the \DIFdelbegin \DIFdel{Query Handler}\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}}\DIFaddend . The process graph is the raw input process graph received by the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver from the client application. The list of result files is the result of the query execution process and therefore\DIFaddbegin \DIFadd{, }\DIFaddend the input data of the \DIFdelbegin \DIFdel{work-flow. The Query Execution }\DIFdelend \DIFaddbegin \DIFadd{workflow. The }\textit{\DIFadd{Query Execution}} \DIFaddend component provides the execution \DIFdelbegin \DIFdel{time stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend additionally. The \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend consists of the following components:

 \begin{itemize}
	\item \textbf{Query Processor} \\
	The Query Processor takes the query as input and generates the necessary query data out of it. Since the process graph at this point also includes the processes and not only the filter operations, this step is necessary. The output consists of the original query, the unique/normalized query, the hash of the normalized query\DIFaddbegin \DIFadd{, }\DIFaddend and the persisted data identifier of the query. This information is forwarded to the Query Record Handler. The mentioned resulting items are described more in detail in the section below.   
	\item \textbf{Data Handler} \\ 
	The Data Handler is responsible for creating the result data dependent query information. The output is the hash over the file list result, the execution \DIFdelbegin \DIFdel{time stamp and additional meta }\DIFdelend \DIFaddbegin \DIFadd{timestamp and additional }\DIFaddend data. In this case\DIFaddbegin \DIFadd{, }\DIFaddend the Data Handler only calculates the number of files as \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend output. Since EODC uses the OGC standard CSW\footnote{http://cite.opengeospatial.org/pub/cite/files/edu/cat/text/main.html} to query the data, the sorting of the resulting files is predefined. The order of the files has no impact on the processing\DIFaddbegin \DIFadd{, }\DIFaddend and openEO users are not able to choose a specific sorting\DIFdelbegin \DIFdel{, therefore }\DIFdelend \DIFaddbegin \DIFadd{. Therefore }\DIFaddend the predefined CSW sorting is used for the hash production. The results of the component \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend forwarded to the Query Record Handler.    
	\item \textbf{Query Record Handler} \\
	The Query Record Handler communicates with the meta database from EODC to see if an identical Query already exists. The \DIFdelbegin \DIFdel{Work-flow }\DIFdelend \DIFaddbegin \DIFadd{Workflow }\DIFaddend of the Query Record Handler can be viewed in Figure \ref{fig:queryhandler_activity}. If the Query Record already exists the Query Record Handler forwards the existing Query PID to the context model, otherwise a new Query PID gets created and persisted in the Query Table. The combination of the result-file hash and the normalized query hash must not have duplicates in the \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table. On saving the Query PID in the job context model, a \DIFdelbegin \DIFdel{QueryJob }\DIFdelend \DIFaddbegin \textit{\DIFadd{QueryJob}} \DIFaddend table entry gets created to store the relation between the job id and the Query PID. 

\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{queryhandler_activity}
	\caption{Activity diagram of the Query Record Handler component.}
	\label{fig:queryhandler_activity} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\subsection{Query Table Structure}

The Query Table stores the data related to the query instance, additional result\DIFdelbegin \DIFdel{and meta-data }\DIFdelend \DIFaddbegin \DIFadd{, and data }\DIFaddend information. Table \ref{Tab:querytable} visualizes the Query Table structure. \DIFdelbegin \DIFdel{To illustrate the Query data record, example }\DIFdelend \DIFaddbegin \DIFadd{Example }\DIFaddend input values are used to explain how the single data entries are generated\DIFaddbegin \DIFadd{, to illustrate the Query data record}\DIFaddend . Therefore, in Figure \ref{fig:processgraph_example} there is an example input process graph.   

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{original_query}
	\caption{Example original query and unique query.}
	\label{fig:processgraph_example} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure} 

\begin{table}[]
	\caption{Structure of the Query Table in the EODC meta database.}
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
	\hline	\textbf{Query} & \textbf{Dataset} & \textbf{Original} & \textbf{Unique} & \textbf{Hash} & \textbf{Hash} &
		\textbf{Exec.} & \textbf{Add.}  \\ 
		\textbf{PID} & \textbf{PID} & \textbf{Query} & \textbf{Query} & \textbf{Query} & \textbf{Result} &
		\textbf{Timest.} & \textbf{Metad.}  \\ \hline
		\textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} &
		\textbf{7} & \textbf{8} \\ \hline
		\textbf{...} & \textbf{...} & \textbf{...} & \textbf{...} & \textbf{...} & \textbf{...} & \textbf{...} & \textbf{...} \\ \hline
	\end{tabular}
	\label{Tab:querytable}
\end{table}

The following list describes how the elements of the query data set record are created in more detail:

\begin{enumerate}
	\item \textbf{Query PID} \\
	The Query PID gets generated by the python library uuid\footnote{https://docs.python.org/3/library/uuid.html}. The library is used to generate unique identifiers and used in the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend for generating the job ids. In the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend ids have in the beginning \DIFdelbegin \DIFdel{a }\DIFdelend code for the specific entity (e.g. "jb-UUID" for Job entities). That is why the id for a Query Table entry is structured like "qu-UUID". \\
	\textit{Example: "qu-16fd2706-8baf-433b-82eb-8c7fada847da"}
	\item \textbf{Dataset PID} \\
	The dataset PID is the identifier of the satellite in the process graph (e.g. the "product\_id" element). \\
	\textit{Example: "s1a\_csar\_grdh\_iw"}	    	
	\item\textbf{Original Query} \\
	The original query is the input query of from the query execution component, which got executed by it.  \\ 
	\textit{Example: See the Original Query in Figure \ref{fig:processgraph_example}}	 
	\item \textbf{Unique Query} \\
	The unique query is the restructured query that is comparable to other unique queries. Since the order of the filters makes no difference in the outcome of the query execution, the elements of the original query are alphabetically sorted by the JSON keys to get the unique query. \\
	\textit{Example: See the Unique Query in Figure \ref{fig:processgraph_example}}	  	 	
	\item \textbf{Unique Query Hash} \\ 
	Newline and space characters are removed from the unique query string. The unique query hash is the output of the SHA-256 hash function (using the python module "hashlib") with the unique query as input.  \\
	\textit{Example: "AE7EF888CDEDF8A9A371\dots"} 
	\item \textbf{Result Hash} \\
	The result hash is the output of the SHA-256 hash function (using the python module "hashlib") with the result file list as input. Before the list is applied to the hash function it is transformed to a string and the newline and white space characters are removed. \\
	\textit{Example: "565D229FCE4772869343\dots"} 
	\item \textbf{Execution \DIFdelbegin \DIFdel{Time-stamp}\DIFdelend \DIFaddbegin \DIFadd{Timestamp}\DIFaddend } \\
	The execution time stamp is the input parameter of the \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend and is transformed to the data type needed by the data base. The \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend is taken by the query execution handler and is part of the query execution result. To forward it to the \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend it has to be read from the result object. \\
	\textit{Example: "2018-10-17 18:03:20,609"}  
	\item \textbf{Additional \DIFdelbegin \DIFdel{Meta-data}\DIFdelend \DIFaddbegin \DIFadd{Data}\DIFaddend } \\
	The additional \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend column of the \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table can be used by the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend to store additional information about the query execution. In the implementation of this thesis, only the number of output files are persisted in the database. The column is defined as a JSON object and can be extended with additional \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend easily. \\
	\textit{Example: "\{ "number\_of\_files": 10\}"}    	 
\end{enumerate}


\section{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend provenance}\DIFdelbegin %DIFDELCMD < \label{Implementation:Back end provenance}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{Implementation:Backend provenance}
\DIFaddend 

The \DIFdelbegin \DIFdel{aim of the back end provenance is }\DIFdelend \DIFaddbegin \DIFadd{backend provenance aims }\DIFaddend to persist the versions of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend and therefore the version of the software used to execute the jobs. The following subsections explain how the, in Section \DIFdelbegin \DIFdel{\ref{Design:Back end provenance} }\DIFdelend \DIFaddbegin \DIFadd{\ref{Design:Backend provenance} }\DIFaddend defined, provenance data elements are read from the GitHub repository.   
\begin{enumerate}[(A)]
\item \textbf{GitHub Repository} \\
	The GitHub repository is directly used to create the services of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . Therefore, the currently used GitHub repository information can be read via the \gls{cli} of Git. The used commit and branch are accessed directly via the Git CLI. Listing \ref{lst:impl_backendprov} shows the Git CLI calls used to get the GitHub repository information.       

\begin{code}
	\begin{minted}{bash}
	# Receiving the Git Repository URL
	git config --get remote.origin.url 
	# Receiving Branch
	git branch
	# Receiving the commit messages with the timestamps
	git log 
	\end{minted}
	\caption{Git commands used to get access the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend identification.}
	\label{lst:impl_backendprov}
\end{code}

\item \textbf{Core API Version} \\
	\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend developers manually update the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend core API version of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . It is in the GitHub repository of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . 

\item \textbf{Back End Version} \\
	Since EODC takes the code for the services directly from GitHub, the Git commit identifier is used as the version of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .

\item \textbf{Publication \DIFdelbegin \DIFdel{Time-stamp}\DIFdelend \DIFaddbegin \DIFadd{Timestamp}\DIFaddend } \\
	The publication \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend of the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is defined by the \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend when the Git commit happened. It is persisted by GitHub and can be retrieved via the Git CLI. 
\end{enumerate}

\section{Job dependent provenance}\label{Implementation:Job dependent provenance}
The EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend of the release version "0.3.1" transforms the process graph into separate docker containers. For every process in the process graph\DIFaddbegin \DIFadd{, }\DIFaddend there is a \DIFdelbegin \DIFdel{docker }\DIFdelend \DIFaddbegin \DIFadd{Docker }\DIFaddend container running the related python code. The input of the current process is the output of the previous process. The first process docker container has the input data defined in the process graph as \DIFaddbegin \DIFadd{an }\DIFaddend input value, which is the result of the query execution. Every process saves the results in a temporary folder dedicated to the specific process execution. Every process has its \DIFdelbegin \DIFdel{own }\DIFdelend temporary output directory until the whole process chain is finished. After that\DIFdelbegin \DIFdel{the back end }\DIFdelend \DIFaddbegin \DIFadd{, the backend }\DIFaddend deletes the temporary folders\DIFaddbegin \DIFadd{, }\DIFaddend and only the \DIFdelbegin \DIFdel{end }\DIFdelend result is persisted in the job directory.
\DIFdelbegin \DIFdel{To achieve the job environment data capturing described in Chapter \ref{Design:Job dependent provenance}, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend processes must provide \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend about the input data, the output data\DIFaddbegin \DIFadd{, }\DIFaddend and the processing itself\DIFaddbegin \DIFadd{, to achieve the job environment data capturing described in Chapter \ref{Design:Job dependent provenance}}\DIFaddend . \\ 
The implementation adds \DIFdelbegin \DIFdel{the }\DIFdelend specific information of interest to the logging of the processing and reads the logging files to generate the context model. This solution modifies and extends the existing python code and is used for the evaluation of this thesis. In Figure \ref{fig:impljobcapture}\DIFaddbegin \DIFadd{, }\DIFaddend an overview of the job capturing \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend is presented. Section \ref{Implementation:Python Implementation} describes the single parts of the overview in more detail.


\subsection{Context Model Repository}\label{Implementation:Provenance Repository}
Each executed job creates a context model. If the job gets re-executed, the context model gets replaced by the new context model. Job re-execution is internally handled as a new job with the same process graph. The functionality of letting the same job be re-executed without creating a new job id is dropped from the agenda of the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project in version 0.3.1 (see the GitHub repository\footnote{https://open-eo.github.io/openeo-api/v/0.3.1/apireference/}). 
The context model is stored as a JSON object in the job execution \DIFdelbegin \DIFdel{meta-database}\DIFdelend \DIFaddbegin \DIFadd{database}\DIFaddend . After the job is carried out in the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend the results are saved in a folder named after the unique job identifier and the \DIFdelbegin \DIFdel{meta }\DIFdelend data information is stored in a PostgreSQL database (see Section \ref{EODC Back End}). Jobs are saved in the \DIFdelbegin \DIFdel{Job }\DIFdelend \DIFaddbegin \textit{\DIFadd{Job}} \DIFaddend table of the database\DIFaddbegin \DIFadd{, }\DIFaddend and in this solution\DIFaddbegin \DIFadd{, }\DIFaddend the context model is an additional column of it. The context model JSON object gets created by the implementations described below. \\
Table \ref{Tab:contextmodel} provides the mapping between the context model elements from Section \ref{Design} and the keys of the JSON context model object of the prototype. The elements have a one to one mapping of the context model and the JSON key except for the timestamps of the execution. The execution \DIFdelbegin \DIFdel{time stamps }\DIFdelend \DIFaddbegin \DIFadd{timestamps }\DIFaddend are part of the \DIFdelbegin \DIFdel{Job }\DIFdelend \DIFaddbegin \textit{\DIFadd{Job}} \DIFaddend table in the EODC \DIFdelbegin \DIFdel{meta-database}\DIFdelend \DIFaddbegin \DIFadd{database}\DIFaddend . \\
Figure \ref{fig:job_context_model} shows an example context model. It consists of all elements described for the context model in the Design section. Information on the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment during the execution of the job is persisted in the context model. In the context model\DIFdelbegin \DIFdel{the back end }\DIFdelend \DIFaddbegin \DIFadd{, the backend }\DIFaddend version and the execution \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend is persisted and can be used to identify the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance of the execution time. The code environment is a list of all python dependencies with their versions installed during the execution of the jobs. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend the python interpreter version is added to the JSON object. How the data is captured in detail is described in the sections below.    



 
%DIF < \todo{Make Glossary !}
 \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{table}[]
	\caption{Relation of context model elements and EODC JSON context model.}
	\begin{tabular}{l|l}
		\textbf{Context Model Definition} & \textbf{JSON Key} \\ \hline
		\textbf{(a): Input data persistent identifier} & input\_data \\ \hline
		\textbf{(b): \DIFdelbeginFL \DIFdelFL{Back end }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Backend }\DIFaddendFL provenance / code identifier} & backend\_env \\ \hline
		\textbf{(c): Programming language} & interpreter \\ \hline
		\textbf{(d): Dependencies of the programming language} & code\_env \\ \hline
		\textbf{(e): Start and end time of the process execution} & start\_time, end\_time \\ \hline
		\textbf{(f): Result hash} & output\_data \\ %\hline
	%DIF < 	\textbf{J7: Back end provenance} & backend\_env \\ 
	%DIF > 	\textbf{J7: Backend provenance} & backend\_env \\ 
	\end{tabular}
\label{Tab:contextmodel}
\end{table}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{job_context_model}
	\caption{Example context model of a job execution at the EODC \DIFdelbeginFL \DIFdelFL{back end}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{backend}\DIFaddendFL .}
	\label{fig:job_context_model} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\subsection{Python Implementation}\label{Implementation:Python Implementation}
The EODC implementation proposed by this thesis is an example for other \DIFdelbegin \DIFdel{back ends}\DIFdelend \DIFaddbegin \DIFadd{backends}\DIFaddend , hence needs to be easy to implement and maintain. Therefore, the implementation is done in the python version of the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend without any additional requirements. The python solution uses logging messages to transfer the needed data from the process execution program to the capturing tool. The cleanup service of the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver triggers the execution of it. It is an additional python module and parses the log files after the job is finished. This solution generates, except for the additional logging calls, little impact on the existing \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend implementation. The EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver has already a logging system installed\DIFdelbegin \DIFdel{, hence }\DIFdelend \DIFaddbegin \DIFadd{. Hence }\DIFaddend the modifications are added in the existing logging policy. \\
Figure \ref{fig:impljobcapture} visualizes the additional python module \textit{Job Capturing} and \textit{Result Handler} in the context of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment. The following list describes the \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend of the solution at the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{job_capturing_overview}
	\caption{Overview of the Job capturing architecture at the EODC \DIFdelbeginFL \DIFdelFL{back end}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{backend}\DIFaddendFL }
	\label{fig:impljobcapture} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\begin{enumerate}
	\item \textbf{Process Execution} \\
	The \textit{Process Execution} module at the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is responsible for the actual execution of the job. It creates the additional logging information ((a), (c), (d) and (e) of the context model) and persists it in the logging file of the job execution. 
	\item \textbf{Processing Cleanup} \\
	After the job execution is finished, the temporary folders are deleted from the file system and the result is copied to the persisted job folder. Then the additional \textit{Job Capturing} module starts with the path to the logging file.  
	\item \textbf{Result Provider} \\
	The \textit{Result Provider} is responsible of making the result available for the user and providing the user with the feedback of the finished job. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Additionally }\DIFaddend it invokes the \DIFdelbegin \DIFdel{added }\DIFdelend \textit{Result Handler} module with the path of the resulting file.
	\item \textbf{Result Handler} \\
	The \textit{Result Handler} reads the result file and calculates an SHA-256 hash over it. After completion\DIFaddbegin \DIFadd{, }\DIFaddend it is provided to the \textit{Context Model Creator} module.  
	\item \textbf{Job Capturing} \\
	The \textit{Job Capturing} module parses the logging file to extract the information needed for the context model ((a), (c), (d) and (e)) and passes the formatted information to the \textit{Context Model Creator}. The \textit{Context Model Creator} needs the \textit{Result Handler} (f) and \textit{Back End Provenance Handler} (b) for the remaining parts of the context model. After receiving all necessary information the JSON context model is created and stored to the \DIFdelbegin \DIFdel{Job }\DIFdelend \DIFaddbegin \textit{\DIFadd{Job}} \DIFaddend table in the EODC \DIFdelbegin \DIFdel{meta-database}\DIFdelend \DIFaddbegin \DIFadd{database}\DIFaddend .    
\end{enumerate}

The following sections describe the capturing of each data element of the job dependent context model in more detail.
\begin{enumerate}[(a)]
\item \textbf{Source Input Data Identifier} \\
	The source input data identifier is the \DIFdelbegin \DIFdel{pid }\DIFdelend \DIFaddbegin \DIFadd{PID }\DIFaddend of the input data and query provided by the EODC query store described in Section \ref{Implementation:Data Identification}. It is forwarded to the \textit{Job Capturing} module by the \textit{Processing Cleanup} module. 

\item \textbf{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend provenance / Code Identifier} \\
	The \textit{Job Capturing} module reads the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance from the \textit{BackendMonit} output JSON file described in Section \DIFdelbegin \DIFdel{\ref{Implementation:Back end provenance}}\DIFdelend \DIFaddbegin \DIFadd{\ref{Implementation:Backend provenance}}\DIFaddend . The latest \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version during the execution is copied to the context model.

\item[(c)(d)] \textbf{Programming language and  installed packages of programming language} \\
	The \textit{Process Execution} module uses the installed python module \textit{pip} to list all installed packages including their versions. The module is at the moment used to manage the python packages of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The GitHub repository of the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver includes a python environment file to \DIFdelbegin \DIFdel{automatically }\DIFdelend install all needed dependencies of python via \textit{pip} \DIFaddbegin \DIFadd{automatically}\DIFaddend . A feature of that tool is the \textit{pip freeze} call, which returns all installed python packages including their \DIFdelbegin \DIFdel{concrete versions. This }\DIFdelend \DIFaddbegin \DIFadd{particular versions. It }\DIFaddend is then transformed \DIFdelbegin \DIFdel{to }\DIFdelend \DIFaddbegin \DIFadd{into }\DIFaddend a JSON list object and saved to the context model. In addition to this the \textit{Process Execution} captures the python version by using the \textit{sys.version} call. All of this executions are done in the \textit{Process Execution} module, in the actual processing environment and stored in the output log file of the job execution.    

\item[(e)] \textbf{Start and end time of process execution} \\
	The start and end time of the process execution is already done at the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend in the  \textit{Process Execution} module. The resulting time stamps are persisted in the \DIFdelbegin \DIFdel{Job }\DIFdelend \DIFaddbegin \textit{\DIFadd{Job}} \DIFaddend table of the \DIFdelbegin \DIFdel{meta-database}\DIFdelend \DIFaddbegin \DIFadd{database}\DIFaddend . The \textit{Process Execution} module adds the values to the output logging file. \DIFdelbegin \DIFdel{These logging entries are not added by this solution, since they are already implemented by the EODC back end provider }\DIFdelend \DIFaddbegin \DIFadd{This solution does not add these logging entries since the EODC backend provider already implements them}\DIFaddend .

\item[(f)] \textbf{Result Identifier} \\
	The result identifier consists of the resulting data of the whole process graph execution. It is \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{an }\DIFaddend SHA-256 hash (using the \textit{hashlib} python library) of the resulting alphabetical sorted output files, which are placed in the resulting folder of the job execution directory. In the current version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend, }\DIFaddend there is only one result file created, due to the limitations of the available processes.   
\end{enumerate}
\section{User Services}\label{Implementation:User Interface}
The previous sections describe only the technical insight of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . This section describes the implementation of the interfaces for the users. Therefore, endpoints are added to the current existing \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend core API version 0.3.1. \DIFdelbegin \DIFdel{In addition}\DIFdelend \DIFaddbegin \DIFadd{Besides}\DIFaddend , the proposed endpoints are applied to the python client and the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The implementation of the recommended additions of Section \ref{Design:User Interface} are described in the following:
\begin{enumerate}[I.]
\item \textbf{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend version} \\
	Retrieving additional information about the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend was added into a new \DIFdelbegin \DIFdel{end point }\DIFdelend \DIFaddbegin \DIFadd{endpoint }\DIFaddend of the API. The new endpoint is a GET request called “/version” and no authentication is needed to access it. \DIFdelbegin \DIFdel{Response }\DIFdelend \DIFaddbegin \DIFadd{The response }\DIFaddend of the version endpoint is the whole job independent provenance information of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend ((A)-(D), see Section \DIFdelbegin \DIFdel{\ref{Implementation:Back end provenance}}\DIFdelend \DIFaddbegin \DIFadd{\ref{Implementation:Backend provenance}}\DIFaddend ). In a production version, the data may be filtered for information marked as a security risk. This endpoint is added to the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend to return the latest \DIFdelbegin \DIFdel{back end version including meta-data}\DIFdelend \DIFaddbegin \DIFadd{backend version including data}\DIFaddend . The endpoint takes a \DIFdelbegin \DIFdel{time-stamp as }\DIFdelend \DIFaddbegin \DIFadd{timestamp as a }\DIFaddend parameter to get the \DIFdelbegin \DIFdel{back end version meta-data }\DIFdelend \DIFaddbegin \DIFadd{backend version data }\DIFaddend of a specific time. Further additions are created to the python client \DIFdelbegin \DIFdel{, }\DIFdelend so that the user can call a method in the python client called “version()” to retrieve the information of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend directly in the python client. \DIFdelbegin \DIFdel{The }\DIFdelend \DIFaddbegin \DIFadd{A }\DIFaddend result is a JSON object consisting of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance data. Listing \ref{lst:implementation_back_end_version} shows a result example.

\begin{listing}[ht]
	\begin{minted}{json}
{'branch': 'master',
'commit': '1a0cefd25c2a0fbb64a78cd9445c3c9314eaeb5b',
'url': 'https://github.com/bgoesswein/implementation_backend.git'}
	\end{minted}
	\caption{\DIFdelbegin \DIFdel{Back end }\DIFdelend \DIFaddbegin \DIFadd{Backend }\DIFaddend version example.}
	\label{lst:implementation_back_end_version}
\end{listing}

\item \textbf{Detailed Job Information} \\
	In the \DIFdelbegin \DIFdel{OpenEO coreAPI}\DIFdelend \DIFaddbegin \DIFadd{openEO coreAPI, }\DIFaddend there is an endpoint to get detailed information about the job status. The endpoint path is “GET /jobs/<job\_id>” , which by the current release version of 0.3.1 only contains the execution state of the job and the job id. In addition to this\DIFaddbegin \DIFadd{, }\DIFaddend the available information of the whole job dependent provenance gets added to this endpoint in the implementation of this thesis (e.g. see Figure \ref{fig:job_context_model}). Since the python client \DIFdelbegin \DIFdel{just }\DIFdelend returns the resulting JSON response from the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend as a python dictionary, there is no modification needed.

\item \textbf{Comparing two Jobs} \\
	The core API does not define the comparison of two jobs\DIFdelbegin \DIFdel{, hence }\DIFdelend \DIFaddbegin \DIFadd{. Hence }\DIFaddend there does not exist any user interface. The modified core API defines a new endpoint in the manner of existing endpoint definitions. For \DIFdelbegin \DIFdel{the purpose of this thesis}\DIFdelend \DIFaddbegin \DIFadd{this thesis, }\DIFaddend the endpoint  “POST /jobs/<job\_id>/diff” gets introduced to the new core API. In the \DIFdelbegin \DIFdel{url }\DIFdelend \DIFaddbegin \DIFadd{URL }\DIFaddend of the request\DIFaddbegin \DIFadd{, }\DIFaddend the user defines the base job id, which context model \DIFdelbegin \DIFdel{will be }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend compared with other jobs. In the body of the request\DIFaddbegin \DIFadd{, }\DIFaddend the target job ids are defined in a JSON list. After getting the request from the user, the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend compares the context models of the base job with every target job occurring in the request body list. The result from the \DIFdelbegin \DIFdel{back end consists of , }\DIFdelend \DIFaddbegin \DIFadd{backend consists of a term }\DIFaddend for every item in the base job context model\DIFdelbegin \DIFdel{, the }\DIFdelend \DIFaddbegin \DIFadd{. The }\DIFaddend term “EQUAL”, if the item is the same in both context models, the difference if the items are not the same in both context models, or “MISSING” if the item is in the base job context model, but missing in the target job context model or the other way around. If an element is different, the elements that differ are visible. The latest mentioned outcome can occur if the context model definition is modified in future job executions and there are e.g. additional fields of it. The response contains the context model of all jobs with one of the previously described three states inside of \DIFdelbegin \DIFdel{every items value }\DIFdelend \DIFaddbegin \DIFadd{the value of every item}\DIFaddend . In the python client\DIFaddbegin \DIFadd{, }\DIFaddend this feature is added with an additional function of the Job class called "diff(target\_job)". Listing \ref{lst:job_comparison} provides an example of the dictionary output of this function.

\begin{listing}[ht]
	\begin{minted}{json}
{
"process_graph":"EQUAL",
"input_data":"EQUAL",
"code_env":"EQUAL",
"output_data":"EQUAL",
"openeo_api":"EQUAL",
"different":{
   "interpreter":"Python 3.5",
   "job_id":"jb-47e062e4-d39c-4f7f-bc5e-aa877f039a84",
   "start_time":"2019-04-05 12:16:38.286217",
   "back_end_timestamp":"20190405121638.286217",
   "end_time":"2019-04-05 12:18:08.286217"}
}
	\end{minted}
	\caption{Example of resulting job comparison.}
	\label{lst:job_comparison}
\end{listing}	


\item \textbf{Data Identifier Landing Page} \\
	Depending on the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , the input data may be restricted to the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend interface. Therefore, the resolver of the input data PID is set within the core-API specification. In the coreAPI release version 0.3.1, there exists an endpoint to retrieve detailed information about a data set. The "GET /data/<data-pid>" endpoint is introduced. If the user calls the endpoint with a data PID, the result \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend the details of the underlying dataset and \DIFdelbegin \DIFdel{in addition }\DIFdelend \DIFaddbegin \DIFadd{besides, }\DIFaddend the result of the query execution and the original query parameters. The landing page contains a link to another page with the file list after a query re-execution ("GET /data/<data-pid>/result" endpoint). If the result file list differs from the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution, there \DIFdelbegin \DIFdel{will be }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend a list of the files that differ\DIFdelbegin \DIFdel{, otherwise it will state }\DIFdelend \DIFaddbegin \DIFadd{. Otherwise, it states }\DIFaddend that the file list is equal to the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution. 

\item \textbf{Re-use of Input Data} \\
	To re-use the input data in \DIFdelbegin \DIFdel{a }\DIFdelend different job execution, the data PID can be used in the process graph directly as source data, instead of just the unfiltered dataset identifier. If a process graph uses the input data PID, the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend automatically applies the queries in \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend way of the \DIFdelbegin \DIFdel{original execution. To pass the data PID to the process graph, it }\DIFdelend \DIFaddbegin \DIFadd{first execution. It }\DIFaddend is inserted in the process "get\_collection" as an additional filter argument named "data\_pid"\DIFaddbegin \DIFadd{, to pass the data PID to the process graph}\DIFaddend . Section \ref{Implementation:Use Case1} provides an example of such a process graph. If the query result data changed from the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution of the PID, the job shall be processed \DIFdelbegin \DIFdel{anyway}\DIFdelend \DIFaddbegin \DIFadd{nonetheless}\DIFaddend , but there has to be a warning message to notify the user.  
\end{enumerate}
%DIF < \todo{Maybe add summary of changes to the EODC back end with a "killer" diagram}
\DIFaddbegin 

\DIFaddend \section{Use Cases}
This section shows how the use cases of Section \ref{Use Cases} can be addressed using the implementation described in the sections before. In the first section\DIFaddbegin \DIFadd{, }\DIFaddend there is an overview \DIFaddbegin \DIFadd{of }\DIFaddend how the data identification recommendations by \acrshort{rda} are implemented. 

\subsection{Use Case 1 – Re-Use of Input Data}\label{Implementation:Use Case1}
The first scenario describes the process of a researcher using \DIFdelbegin \DIFdel{OpenEO as }\DIFdelend \DIFaddbegin \DIFadd{openEO as a }\DIFaddend processing environment. In this use case\DIFaddbegin \DIFadd{, }\DIFaddend the focus is on the data identification and data citation part of the solution. Researchers that use \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend may want to cite the data that is used in the applied process chain. Other \DIFdelbegin \DIFdel{scientist }\DIFdelend \DIFaddbegin \DIFadd{scientists }\DIFaddend then may want to use this data in their related research experiment. The step-by-step description of the scenario can be viewed in \DIFdelbegin \DIFdel{the }\DIFdelend Section \ref{UseCase1}. In this section\DIFaddbegin \DIFadd{, }\DIFaddend the steps of the use case are executed \DIFdelbegin \DIFdel{at }\DIFdelend \DIFaddbegin \DIFadd{in }\DIFaddend the evaluation environment. The implementation of the use cases is available at GitHub\footnote{https://github.com/bgoesswein/dataid\_openeo/tree/master/openeo-python-client/examples}.

\begin{enumerate}
	\item \textbf{Researcher A runs an experiment (job A) at the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .} \\
	This step is basic \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend functionality and is not influenced by the solution from the user point of view. The researcher chooses Sentinel-2 data by loading the Sentinel-2 collection of EODC with the data-set identifier "s2a\_prd\_msil1c". In the next step\DIFaddbegin \DIFadd{, }\DIFaddend the data is filtered temporally (May of 2017) and spatially (bounding box of the \DIFdelbegin \DIFdel{south tyrol }\DIFdelend \DIFaddbegin \DIFadd{South Tyrol }\DIFaddend area). In the next step the researcher applies the NDVI\footnote{https://earthobservatory.nasa.gov/features/MeasuringVegetation/measuring\_vegetation\_2.php} process on the filtered data as well as the minimum value of each pixel in the time range with the "min\_time" process. The NDVI calculation needs the measurements of the satellite in near-infrared (parameter "nir") and the measurements of red light (parameter "red"). for Sentinel 2 data of the EODC the \DIFdelbegin \DIFdel{two measurements are represented by the band identifiers }\DIFdelend \DIFaddbegin \DIFadd{band identifiers represent the two measurements }\DIFaddend "B08" for near-infrared and "B04" for \DIFaddbegin \DIFadd{the }\DIFaddend visible red light. The last two lines create a job at the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend with the specified process graph and start the execution of it.
	At the \DIFdelbegin \DIFdel{back end the Query Handler }\DIFdelend \DIFaddbegin \DIFadd{backend the }\textit{\DIFadd{Query Handler}} \DIFaddend is generating a new data PID or returns an already existing one if the same query got executed in the past. The following python client code represents the experiment of Researcher A.

\begin{code}
	\begin{minted}{python}
import openeo
EODC_DRIVER_URL = "http://openeo.local.127.0.0.1.nip.io"

con = openeo.connect(EODC_DRIVER_URL)

# Choose dataset
processes = con.get_processes()
pgA = processes.get_collection(name="s2a_prd_msil1c")
pgA = processes.filter_daterange(pgA, extent=["2017-05-01", 
"2017-05-31"])
pgA = processes.filter_bbox(pgA, west=10.288696, 
south=45.935871, east=12.189331, 
north=46.905246, crs="EPSG:4326")

# Choose processes
pgA = processes.ndvi(pgA, nir="B08", red="B04")
pgA = processes.min_time(pgA)

# Create job A out of the process graph A (pgA)

jobA = con.create_job(pgA.graph)
jobA.start_job()
	\end{minted}
	\caption{Researcher A runs job A with the python client.}
	\label{lst:impl_usecase1_1}
\end{code}	

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{openeo_example_output}
	\caption{Resulting image of the first step of Use Case 1.}
	\label{fig:impl_usecase1_min} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

	\item \textbf{Researcher A retrieves the used input data of job A.} \\
	If Researcher A wants to receive the input data identifier, the job description endpoint has to be called. The following code \DIFdelbegin \DIFdel{snipped }\DIFdelend \DIFaddbegin \DIFadd{snippet }\DIFaddend shows the call using the python client.

\begin{code}
	\begin{minted}{python}
pidA_url = jobA.get_data_pid_url()
print(pidA_url)
# Output: EODC_DRIVER_URL/data
#	/qu-d1701f4e-e7c5-4a83-92e0-9facbd401a06
pidA = jobA.get_data_pid()

# retrieve information about the pidA 
# e.g. executed query and description about the dataset.
desc = con.describe_collection(pidA)

query = desc["query"]
# re-execute query and get the resulting 
# file list from the \DIFdelbegin \DIFdel{back end
}\DIFdelend \DIFaddbegin \DIFadd{backend
}\DIFaddend file_list = con.get_filelist(pidA)
	\end{minted}
	\caption{Researcher A retrieves the used input data \DIFdelbegin \DIFdel{pid}\DIFdelend \DIFaddbegin \DIFadd{PID}\DIFaddend .}
	\label{lst:impl_usecase1_2}
\end{code}

	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{usecase1_2}
		\caption{Screenshot of the job A landing page.}
		\label{fig:usecase1-pid} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
	\end{figure}

	The job\_details variable is an unfiltered python dictionary object that contains the response of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . It has a data element at the key of "input\_data" that consists of the input data PID as a resolvable web address. Figure \ref{fig:usecase1-pid} shows the response data of the \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend PID information. After calling the page, the user is provided by the executed filter parameter, the dataset identifier, a description \DIFdelbegin \DIFdel{about }\DIFdelend \DIFaddbegin \DIFadd{of }\DIFaddend the dataset. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend the link to get the re-execution results of the query and the resolvable \DIFdelbegin \DIFdel{query pid }\DIFdelend \DIFaddbegin \DIFadd{data PID }\DIFaddend is added to the page (see Figure \ref{fig:usecase1-pid}). \\
	\DIFdelbegin \DIFdel{To }\DIFdelend \DIFaddbegin \DIFadd{The last three calls of the code block above are used to }\DIFaddend gather the information about the input data directly in the python client code\DIFdelbegin \DIFdel{, the last three calls of the code block above can be used}\DIFdelend .  

	\item \textbf{Researcher A cites the input data in a publication.} \\
	This step is independent from the implementation and therefore not explained in detail. For the further steps it is assumed that Researcher A used the resolvable PID from step 2 (\textit{"EODC\_DRIVER\_URL/data/qu-d1701f4e-e7c5-4a83-92e0-9facbd401a06"}) for the citation.   

	\item \textbf{Researcher B uses the same input data of job A for job B.} \\
	\DIFdelbegin \DIFdel{In order to }\DIFdelend \DIFaddbegin \DIFadd{To }\DIFaddend use the same input data as Researcher A, Researcher B uses the data PID from the publication and puts it into the input data element of the process chain of job B. In the block below\DIFaddbegin \DIFadd{, }\DIFaddend an example of \DIFaddbegin \DIFadd{the }\DIFaddend code needed to use the same input with a different process chain (max\_time instead of min\_time process) is printed.  

\begin{code}
	\begin{minted}{python}
import openeo

# Take input data of job A by using the input data \DIFdelbegin \DIFdel{pid }\DIFdelend \DIFaddbegin \DIFadd{PID }\DIFaddend A
# pidA = qu-d1701f4e-e7c5-4a83-92e0-9facbd401a06
pgB = processes.get_data_by_pid(
data_pid="qu-d1701f4e-e7c5-4a83-92e0-9facbd401a06")
# Alternative: data_pid="EODC_DRIVER_URL/data/pidA" 

# Choose processes
pgB = processes.ndvi(pgB, nir="B08", red="B04")
pgB = processes.max_time(pgB)

# Create job B out of the process graph B (pgB)

jobB = con.create_job(pgB.graph)
jobB.start_job()
	\end{minted}
	\caption{Researcher B uses \DIFdelbegin \DIFdel{pid }\DIFdelend \DIFaddbegin \DIFadd{PID }\DIFaddend A for different job.}
	\label{lst:impl_usecase1_3}
\end{code}

\end{enumerate}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{usecase1_max}
	\caption{Resulting image of the last step of Use Case 1.}
	\label{fig:impl_usecase1_max} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\subsection{Use Case 2 – Capturing job dependent environments}\label{Implementation:Use Case2}
This scenario focuses on the execution environment. It handles the need \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend researchers to describe the execution process. Section \ref{UseCase2} describes the second use case in more detail. The implementation at the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend gives the users the option to gain additional \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend about the execution. In the following\DIFaddbegin \DIFadd{, }\DIFaddend the steps of the scenario are run with the implemented instance of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . 

\begin{enumerate}
	\item \textbf{Researcher runs an experiment (job A) at a \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .}\\
	The researcher A runs a job at the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend with the following python client code. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend is the default workflow of executing a job using the python client. 

\begin{code}
	\begin{minted}{python}
import openeo
EODC_DRIVER_URL = "http://openeo.local.127.0.0.1.nip.io"

con = openeo.connect(EODC_DRIVER_URL)

# Choose dataset
processes = con.get_processes()
pgA = processes.get_collection(name="s2a_prd_msil1c")
pgA = processes.filter_daterange(pgA, extent=["2017-05-01", 
"2017-05-31"])
pgA = processes.filter_bbox(pgA, west=10.288696, 
south=45.935871, east=12.189331, 
north=46.905246, crs="EPSG:4326")

# Choose processes
pgA = processes.ndvi(pgA, nir="B08", red="B04")
pgA = processes.min_time(pgA)

# Create job A out of the process graph A (pgA)
jobA = con.create_job(pgA.graph)
jobA.start_job()
	\end{minted}
	\caption{Researcher A runs job A with the python client.}
	\label{lst:impl_usecase2_1}
\end{code}
	\item \textbf{Researcher wants to describe the experiment environment.}\\
	The researcher wants to publish the result of the experiment and therefore\DIFaddbegin \DIFadd{, }\DIFaddend needs to describe the environment in detail. The following code-lines provide the user with detailed information about the job execution:

\begin{code}
	\begin{minted}{python}
# Get context model of job A
context_model = jobA.describe_job["context_model"]

# Retrieve the information that Researcher A needs.

interpreter = context_model["interpreter"]
code_env = context_model["code_env"]
input_data = jobA.get_data_pid_url()
backend_version = jobA.get_backend_version()
logging.info("Interpreter: {}".format(interpreter))
logging.info("Code Environment: {}".format(code_env))
logging.info("Input Data PID URL: {}".format(input_data))
logging.info("Back End Version (commit): {}"
             .format(backend_version["commit"]))
	\end{minted}
	\caption{Describe jobA execution environment.}
	\label{lst:impl_usecase2_2}
\end{code}
	After the execution of the lines above, the researcher \DIFdelbegin \DIFdel{is able to }\DIFdelend \DIFaddbegin \DIFadd{can }\DIFaddend get the used programming language\DIFaddbegin \DIFadd{, }\DIFaddend including the version and installed packages. \DIFdelbegin \DIFdel{In addition the back end }\DIFdelend \DIFaddbegin \DIFadd{Besides, the backend }\DIFaddend version describes the used processing code. For more transparency\DIFaddbegin \DIFadd{, }\DIFaddend the researcher can link to the used GitHub repository and the commit to \DIFdelbegin \DIFdel{identify the used }\DIFdelend \DIFaddbegin \DIFadd{identifying the }\DIFaddend code. Listing \ref{lst:use_case2_logfile} shows the output of the logging calls.  

\end{enumerate}

\begin{code}
	\begin{minted}[fontsize=\footnotesize]{text}
INFO:root:Interpreter: Python 3.7.1
INFO:root:Code Environment: ['alembic==0.9.9', 'amqp==1.4.9',
         ..., 'GitPython==2.1.11', 'numpy==1.16.2', 'GDAL==2.4.0']
INFO:root:Input Data PID URL:
http://openeo.local.127.0.0.1.nip.io/collections
              /qu-8cdac780-7f47-4fac-9c4b-7a9ffff17d1d
INFO:root:Back End Version (commit): 
              16c3b32b5cb2d92d1c32d8c1f929065ee6bf2831
	\end{minted}
	\caption{Logging output of the second Use Case.}
	\label{lst:use_case2_logfile}
\end{code}
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{usecase2_2}
%	\caption{Screenshot of logging output of the second Use Case.}
%	\label{fig:usecase2_2} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
%\end{figure}

\subsection{Use Case 3 – Getting differences of job executions}\label{Implementation:Use Case3}
The last use case is focused on the users of \DIFdelbegin \DIFdel{OpenEO}\DIFdelend \DIFaddbegin \DIFadd{openEO}\DIFaddend . It describes the need \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend transparency of job executions for the users. If results differ with the same job later in time, the user can access \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend to find reasons why it happened. Other users might compare different job execution environments. Section \ref{UseCase3} describes the third use case in more detail.  
\begin{enumerate}
	\item \textbf{Researcher runs an experiment (job A) at a \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend .}\\
	The researcher A runs a job at the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend with the following python client code. 

\begin{code}
	\begin{minted}{python}
import openeo
EODC_DRIVER_URL = "http://openeo.local.127.0.0.1.nip.io"

con = openeo.connect(EODC_DRIVER_URL)

# Choose dataset
processes = con.get_processes()
pgA = processes.get_collection(name="s2a_prd_msil1c")
pgA = processes.filter_daterange(pgA, extent=["2017-05-01", 
"2017-05-31"])
pgA = processes.filter_bbox(pgA, west=10.288696, 
south=45.935871, east=12.189331, 
north=46.905246, crs="EPSG:4326")

# Choose processes
pgA = processes.ndvi(pgA, nir="B08", red="B04")
pgA = processes.min_time(pgA)

# Create job A out of the process graph A (pgA)

jobA = con.create_job(pgA.graph)
jobA.start_job()
	\end{minted}
	\caption{Researcher A runs job A with the python client.}
	\label{lst:impl_usecase3_1}
\end{code}
	\item \textbf{Researcher re-runs the same experiment (job B).}\\
	Since re-execution of the same job id is handled internally with the re-execution of the same job configuration with a new job id, \DIFdelbegin \DIFdel{the the }\DIFdelend \DIFaddbegin \DIFadd{then the }\DIFaddend following code is used to create the same job again (job B). It uses the same process graph\DIFaddbegin \DIFadd{, }\DIFaddend and the definition of pgA is the same as in Listing \ref{lst:impl_usecase3_1}:
\begin{code}
	\begin{minted}{python}
jobA = con.create_job(pgA.graph)
jobA.start_job()
	\end{minted}
	\caption{Researcher \DIFdelbegin \DIFdel{re-reuns }\DIFdelend \DIFaddbegin \DIFadd{re-reruns }\DIFaddend job A resulting in job B.}
	\label{lst:impl_usecase3_2}
\end{code}

	\item \textbf{Researcher runs a different experiment (job C).}\\
	The third job (job C) is created with a different process configuration and input data query.

\begin{code}
	%		\inputminted{octave}{BitXorMatrix.m}
	\begin{minted}{python}
# Choose dataset
processes = con.get_processes()
pgC = processes.get_collection(name="s2a_prd_msil1c")
pgC = processes.filter_daterange(pgC, extent=["2017-05-01", 
                                              "2017-05-31"])
pgC = processes.filter_bbox(pgC, west=10.288696, south=45.935871, 
                                 east=12.189331, north=46.905246, 
                                 crs="EPSG:4326")

# Choose processes
pgC = processes.ndvi(pgC, nir="B08", red="B04")
pgC = processes.max_time(pgC) # differs from job A

# Create job C out of the process graph C (pgC)

jobC = con.create_job(pgC.graph)
jobC.start_job()
	\end{minted}
	\caption{Researcher runs experiment different from job A.}
	\label{lst:impl_usecase3_3}

\end{code}

	\item \textbf{Researcher wants to compare the jobs by their environment and outcome.}\\
	Now the researcher wants to compare job B and job C with the first job A. Therefore the following code has to be executed:

	\begin{code}
%		\inputminted{octave}{BitXorMatrix.m}
		\begin{minted}{python}
diffAB = jobA.diff(jobB)
diffAC = jobA.diff(jobC)
logging.info(diffAB)
logging.info(diffAC)
		\end{minted}
		\caption{Researcher compares the different jobs.}
		\label{lst:impl_usecase3_4}

	\end{code}

	With the lines above\DIFaddbegin \DIFadd{, }\DIFaddend the researcher gets two dictionaries of the comparisons between job A with job B (diffAB) and job A with job C (diffAC). The content of the dictionary is a comparison of every key of the jobs context model with each other. Listing \ref{lst:use_case3_logfile} shows the logging output of Listing \ref{lst:impl_usecase3_4}.
\end{enumerate}

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\textwidth]{usecase3_4}
%	\caption{Logging output of the job comparisons diffAB and diffAC.}
%	\label{fig:eva_diff} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
%\end{figure}

\begin{code}
	\begin{minted}[fontsize=\footnotesize]{text}
INFO:root:{'input_data': 'EQUAL', 'output_data': 'EQUAL', 
 'process_graph': 'EQUAL', 'openeo_api': 'EQUAL', 
 'interpreter': 'EQUAL', 'code_env': 'EQUAL',
 'different': 
 {'back_end_timestamp': '20190417194702.496810', 
 'job_id': 'jb-b92c688c-7fdc-4126-bcdf-85bc07030237', 
 'start_time': '2019-04-17 19:47:02.496810', 
 'end_time': '2019-04-17 19:47:03.258261'}} 
INFO:root:{'input_data': 'EQUAL', 'output_data': 'EQUAL', 
 'openeo_api': 'EQUAL', 'interpreter': 'EQUAL', 'code_env': 'EQUAL',
 'different': {'process_graph': {'imagery': {'imagery': {'extent': 
 {'crs': 'EPSG:4326', 'east': 12.189331, 'north': 46.905246, 
 'south': 45.935871, 'west': 10.288696}, 'imagery': 
 {'extent': ['2017-05-01', '2017-05-31'], 
 'imagery': {'name': 's2a_prd_msil1c', 'process_id': 'get_collection'}, 
 'process_id': 'filter_daterange'}, 'process_id': 'filter_bbox'}, 
 'nir': 'B08', 'process_id': 'NDVI', 'red': 'B04'}, 'process_id': 
 'max_time'}, 'start_time': '2019-04-17 19:47:09.089075', 
 'end_time': '2019-04-17 19:47:11.786845', 
 'back_end_timestamp': '20190417194709.089075', 
 'job_id': 'jb-ecdd5768-3c22-4c73-85b8-ac6f4bdc138f'}}
	\end{minted}
	\caption{Logging output of the job comparisons diffAB and diffAC.}
	\label{lst:use_case3_logfile}
\end{code}

\section{Summary}
This chapter presented an implementation of the design of Chapter \ref{Design} at the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . It consists of an implementation of the RDA recommendations for the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The additional modules needed to achieve this are presented in Section \ref{Implementation:Data Identification} and are using the CSW\footnote{http://cite.opengeospatial.org/pub/cite/files/edu/cat/text/main.html} \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \DIFadd{query }\DIFaddend system. The implementation of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend provenance uses the advantages of GitHub on versioning the used software and persisting the software versions of the past. The job dependent environment is captured by retrieving the python environment using pip and capturing the version of the software at the execution time. The logging system of the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is used to communicate the \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend needed by the context model without changing the processing code. The data of the query and the context model are persisted in additions to the existing PostgreSQL database. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend the endpoints needed to access the provenance information are inserted into the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend API. In the last section of the chapter, the implementation of the defined Use Cases of Section \ref{Use Cases} are presented. The next chapter evaluates the implementation by the impact on the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend and by testing the implementation against \DIFdelbegin \DIFdel{special cases.   
%DIF < \todo{optional: Maybe adding a sequence diagram somewhere in the implementation showing how things work would complement it?}
}\DIFdelend \DIFaddbegin \DIFadd{exceptional cases.   
}

\DIFaddend \chapter{Evaluation}\label{Evaluation}
This chapter evaluates the \DIFaddbegin \DIFadd{concept of the }\DIFaddend prototype implementation described in \DIFdelbegin \DIFdel{the previous section. The work-flow of the evaluation is applying test cases to the solution of Section \ref{Implementation:Python Implementation} and evaluate if the outcome reaches the expectations. Additionally the performance and storace }\DIFdelend \DIFaddbegin \DIFadd{Chapter \ref{Implementation}. The solution is evaluated by test cases that simulate updates on data as well as on the backend environment. The performance and storage }\DIFaddend impact of the solution \DIFdelbegin \DIFdel{compared to the EODC back end implementation is evaluated . First }\DIFdelend \DIFaddbegin \DIFadd{is evaluated by applying 18 test cases derived from 9 publications that used EODC data in the past. The chapter is structured as follows: }\\
\DIFadd{First, }\DIFaddend the set up of the evaluation environment is described in Section \ref{Evaluation:Setup}. Section \ref{Evaluation:special_dataid} evaluates how the solution behaves on \DIFdelbegin \DIFdel{the data identification aspects of the solution. }\DIFdelend \DIFaddbegin \DIFadd{data updates at the backend. The following }\DIFaddend Section \ref{Evaluation:special_jobcap}  evaluates \DIFaddbegin \DIFadd{if the recreation of an older back end version is possible using }\DIFaddend the \DIFdelbegin \DIFdel{job capturing part of the }\DIFdelend solution. \DIFdelbegin \DIFdel{The last Section \ref{Evaluation:impact} compares the needed storage and performance }\DIFdelend \DIFaddbegin \DIFadd{Section \ref{Evaluation:impact} provides measurements on the performance and storage impact }\DIFaddend of the solution \DIFdelbegin \DIFdel{to the EODC back end using the same environment}\DIFdelend \DIFaddbegin \DIFadd{system}\DIFaddend . 

\section{Evaluation Setup}\label{Evaluation:Setup}
\DIFdelbegin \DIFdel{The solution is an extended duplicate of the productive service running at the EODC back end. EODC }\DIFdelend \DIFaddbegin 

\DIFadd{EODC }\DIFaddend uses an OpenShift\footnote{https://www.openshift.com/} service to provide the \DIFdelbegin \DIFdel{back ends functionality. The version provided at GitHub from EODC to build the local instance of the EODC back end is used to host the original EODC back end. The implementation of this thesis is a GitHub fork of the EODC back end to deploy the adapted back end of Section \ref{Implementation}. }\DIFdelend \DIFaddbegin \DIFadd{backends functionality. }\DIFaddend For this evaluation\DIFdelbegin \DIFdel{the back end }\DIFdelend \DIFaddbegin \DIFadd{, OpenShift }\DIFaddend is installed locally \DIFdelbegin \DIFdel{and does not run on the EODC infrastructure}\DIFdelend \DIFaddbegin \DIFadd{to run the solution backend}\DIFaddend . Nevertheless, the data querying of EODC is publicly available and is used\DIFdelbegin \DIFdel{, therefore the data identification part }\DIFdelend \DIFaddbegin \DIFadd{. The }\textit{\DIFadd{Query Handler}} \DIFadd{component }\DIFaddend of the solution is operating with the actual data that EODC provides for \DIFdelbegin \DIFdel{OpenEO users. Changes on the data (like removed files or updated files) are simulated by changing the output of the query directly after execution. }\DIFdelend \DIFaddbegin \DIFadd{openEO users. }\\
\DIFaddend The processing service of EODC \DIFdelbegin \DIFdel{can not }\DIFdelend \DIFaddbegin \DIFadd{cannot }\DIFaddend be executed locally \DIFdelbegin \DIFdel{, }\DIFdelend because the data files \DIFdelbegin \DIFdel{can only be accessed }\DIFdelend \DIFaddbegin \DIFadd{are only }\DIFaddend inside of the EODC \DIFdelbegin \DIFdel{environment. Therefore}\DIFdelend \DIFaddbegin \DIFadd{infrastructure available. Therefore, }\DIFaddend the processing mechanism is mocked up. \DIFdelbegin \DIFdel{It }\DIFdelend \DIFaddbegin \DIFadd{The mockup }\DIFaddend creates an array with \DIFdelbegin \DIFdel{mock-up }\DIFdelend \DIFaddbegin \DIFadd{mockup }\DIFaddend values in the size of the query results and applies the defined processes of the process graph on it. To \DIFdelbegin \DIFdel{make the solution of this thesis comparable to the original back end version, this processing mock-up is also applied to the local version of the original EODC back end (reference back end). To }\DIFdelend be able to run \DIFdelbegin \DIFdel{the }\DIFdelend test cases independently\DIFaddbegin \DIFadd{, }\DIFaddend a "reset" endpoint is added to the \DIFdelbegin \DIFdel{back end. If itis called}\DIFdelend \DIFaddbegin \DIFadd{backend. After calling it}\DIFaddend , all stored job and query records are deleted from the \DIFdelbegin \DIFdel{meta-database. }\DIFdelend \DIFaddbegin \DIFadd{database. Figure \ref{fig:evaluation_setup} gives an overview of the evaluation setup. }\DIFaddend \\
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Table \ref{Tab:eva_hardware} specifies the \DIFdelbegin \DIFdel{system }\DIFdelend \DIFaddbegin \DIFadd{local machine }\DIFaddend used for the evaluation. \DIFdelbegin \DIFdel{In order to minimum the }\DIFdelend \DIFaddbegin \DIFadd{To get a minimum }\DIFaddend performance bias, not necessary background programs are disabled for the evaluation execution.\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend 
 The evaluation is done from a user point of view, hence \DIFdelbegin \DIFdel{an OpenEO client needs to be used for the evaluation. The python client is chosen for this purpose including the additional functionality described in Section \ref{Implementation:User Interface} }\DIFdelend \DIFaddbegin \DIFadd{the openEO python client with the additions of Section \ref{Implementation:User Interface} is used for the evaluation}\DIFaddend . The python client, the solution \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend, }\DIFaddend and the code for the evaluation is available and further described on GitHub\footnote{https://github.com/bgoesswein/dataid\_openeo}. 
\DIFdelbegin %DIFDELCMD < \\ 
%DIFDELCMD < %%%
\DIFdelend 

\DIFaddbegin \begin{figure}[h]
	\centering
	\includegraphics[scale=0.46]{evaluation_setup}
	\caption{\DIFaddFL{Overview of the evaluation setup.}}
	\label{fig:evaluation_setup} %DIF >  \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\DIFaddend \begin{table}[]
	\caption{Evaluation system specifications.}
	\centering
	\begin{tabular}{l|l}
		\multicolumn{2}{c}{\textbf{Hardware}} \\ \hline
		\textbf{\acrshort{cpu}} & Intel(R) Core(TM) i7-3770T CPU @ 2.50GHz \\ 
		\textbf{\acrshort{gpu}} & Radeon HD 7750/8740 / R7 250E  \\ 
		\textbf{\acrshort{ram}} & 16 GB  \\ 
		\multicolumn{2}{c}{\textbf{Software}} \\ \hline
		\textbf{\acrshort{os}} & Kubuntu 18.04.1 LTS \\ 
		\textbf{OpenShift} & 3.9.0  \\ 
		\textbf{Python} & 3.7.1  \\ 
	\end{tabular}
	\label{Tab:eva_hardware}
\end{table}

%\section{Special Test Cases}\label{Evaluation:special}
%DIF < This section evaluates special test cases to get an overview on how the solution behaves, if special circumstances on the back end occur. The GitHub repository of the solution has unit tests in the "test" folder of the python client testing the basic functionality. The test cases in this section are created to look into the error proneness of the solution. The code as well as the results of the evaluation in this section are available on GitHub\footnote{https://github.com/bgoesswein/dataid\_openeo/tree/master/openeo-python-client/examples/}.
%DIF > This section evaluates special test cases to get an overview on how the solution behaves, if special circumstances on the backend occur. The GitHub repository of the solution has unit tests in the "test" folder of the python client testing the basic functionality. The test cases in this section are created to look into the error proneness of the solution. The code as well as the results of the evaluation in this section are available on GitHub\footnote{https://github.com/bgoesswein/dataid\_openeo/tree/master/openeo-python-client/examples/}.

\section{Data Identification}\label{Evaluation:special_dataid}
This section \DIFdelbegin \DIFdel{describes the evaluation of the }\DIFdelend \DIFaddbegin \DIFadd{evaluates the }\DIFaddend data identification mechanism \DIFdelbegin \DIFdel{of the solution back end. Section \ref{Evaluation:dataidentification} summarizes }\DIFdelend \DIFaddbegin \DIFadd{that is a proposed extension to a backend. We first show }\DIFaddend how the solution \DIFdelbegin \DIFdel{tackles the recommendations defined by the \acrshort{rda}. The next sections are evaluating data update situations of }\DIFdelend \DIFaddbegin \DIFadd{fulfills the \acrshort{rda} recommendations and later how the solution behaves if data updates or deletions occur at }\DIFaddend the back end\DIFdelbegin \DIFdel{, to see how the solution system behaves.}\DIFdelend \DIFaddbegin \DIFadd{.}\\  
\DIFaddend The policy of the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend regarding updates on data sets is dependent on the type of data:

\begin{enumerate}
	\item \textbf{Sentinel Data} \\
	The Sentinel data, which is used by EODC for the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project, has never been updated before. The data comes directly from ESA\DIFaddbegin \DIFadd{, }\DIFaddend and updated datasets from ESA were not applied to the EODC data yet. \DIFdelbegin \DIFdel{But }\DIFdelend \DIFaddbegin \DIFadd{However, }\DIFaddend the occurrence of updates \DIFdelbegin \DIFdel{are in general}\DIFdelend \DIFaddbegin \DIFadd{is, in general, }\DIFaddend not forbidden in future of the EODC \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . If a dataset \DIFdelbegin \DIFdel{would be }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend updated, it would follow the \DIFdelbegin \DIFdel{update }\DIFdelend \DIFaddbegin \DIFadd{updated }\DIFaddend policy of ESA, therefore having a new file name for the updated file. Hence, updates on the data can be simulated by renaming the input files and the creation \DIFdelbegin \DIFdel{time-stamp}\DIFdelend \DIFaddbegin \DIFadd{timestamp}\DIFaddend . Other than updating the existing datasets, there are regular updates on the extent of the Sentinel data sets.   
	\item \textbf{Processed Data} \\
	Data that is processed by partners of EODC is maintained and therefore also updated by the partners. Old versions of this data are deleted or archived by EODC, depending on the decision of the partner. This type of data is not available using \DIFdelbegin \DIFdel{OpenEO, }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend since it is \DIFaddbegin \DIFadd{the }\DIFaddend property of the partners and not EODC. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend to be able to use the same processes on different \DIFdelbegin \DIFdel{OpenEO back ends, the common }\DIFdelend \DIFaddbegin \DIFadd{openEO backends, the standard }\DIFaddend data layer needs to be the same. That is why only unprocessed data is used by all \DIFdelbegin \DIFdel{back ends contributing to OpenEO}\DIFdelend \DIFaddbegin \DIFadd{backends contributing to openEO}\DIFaddend . Since the data sets are not available for \DIFdelbegin \DIFdel{OpenEO}\DIFdelend \DIFaddbegin \DIFadd{openEO}\DIFaddend , the data identification of them is not in \DIFaddbegin \DIFadd{the }\DIFaddend scope of the thesis. By now\DIFdelbegin \DIFdel{it is planned that EODC uses the OpenEO }\DIFdelend \DIFaddbegin \DIFadd{, EODC plans to use the openEO }\DIFaddend framework for their whole infrastructure, which \DIFdelbegin \DIFdel{will bring }\DIFdelend \DIFaddbegin \DIFadd{brings }\DIFaddend the partners to use \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend for their processing\DIFdelbegin \DIFdel{, therefore }\DIFdelend \DIFaddbegin \DIFadd{. Therefore }\DIFaddend they can use the data identification implementation for their tasks.
\end{enumerate}

\subsection{RDA Recommendations}\label{Evaluation:dataidentification}
The Design (Section \ref{Design:Data Identification}) defines that the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend needs to implement the \acrshort{rda} data identification recommendations. Therefore the following enumeration shows how the recommendations are achieved in this implementation. 

\begin{itemize}
	\item \textbf{R1: Data Versioning} \\
	The \DIFaddbegin \DIFadd{EODC backend has }\DIFaddend versioning of data \DIFdelbegin \DIFdel{is already implementedat the EODC back end}\DIFdelend \DIFaddbegin \DIFadd{already implemented}\DIFaddend , by following the naming policies of ESA. The path to the \DIFdelbegin \DIFdel{files are }\DIFdelend \DIFaddbegin \DIFadd{file is }\DIFaddend the version identifier \DIFdelbegin \DIFdel{, }\DIFdelend since modified files must have a different file name.
	\item \textbf{R2: Timestamping} \\
	The creation \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend of the files are stored by the file system of EODC \DIFdelbegin \DIFdel{, }\DIFdelend since the files have to change the name after an update, the creation \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend of the updated file is the update \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend of the original file. 
	\item \textbf{R3: Query Store Facilities} \\
	\DIFdelbegin \DIFdel{The query store is implemented by an additional table }\DIFdelend \DIFaddbegin \DIFadd{A new table implements the query store }\DIFaddend in the EODC \DIFdelbegin \DIFdel{meta-database }\DIFdelend \DIFaddbegin \DIFadd{database }\DIFaddend (PostgreSQL) used for \DIFdelbegin \DIFdel{OpenEO. The Query }\DIFdelend \DIFaddbegin \DIFadd{openEO. The query }\DIFaddend Table stores the following data about each query:
	\begin{itemize}
		\item The original executed \acrshort{csw} query in XML format
		\item The unique query is generated out of the JSON object created by the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend after parsing the filter arguments of the process graph. To assure uniqueness the JSON is sorted alphabetically.
		\item Hash of the unique query after removing all characters with no semantic meaning from it.
		\item Hash of the alphabetically sorted resulting file list after removing all characters with no semantic meaning from it. 
		\item \DIFdelbegin \DIFdel{Time-stamp }\DIFdelend \DIFaddbegin \DIFadd{Timestamp }\DIFaddend of the original query execution.
		\item Dataset identifier is set to the used collection identifier of the query.
		\item Generated persistent identifier of the query using the uuid library of python.
		\item JSON object with the number of result files.
	\end{itemize}
	\item \textbf{R4: Query Uniqueness} \\
	The alphabetically sorted filter JSON object provided by the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend (See Figure \ref{fig:processgraph_example}).
	\item \textbf{R5: Stable Sorting} \\
	\DIFdelbegin \DIFdel{Stable sorting is assured by the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend original \acrshort{csw} query \DIFdelbegin \DIFdel{, where }\DIFdelend \DIFaddbegin \DIFadd{assures stable sorting. It sorts }\DIFaddend the resulting file list \DIFdelbegin \DIFdel{is sorted }\DIFdelend in ascending order (alphabetically).
	\item \textbf{R6: Result Set Verification} \\
	The resulting file list is \DIFdelbegin \DIFdel{given as }\DIFdelend a list sorted alphabetically in ascending order. \DIFdelbegin \DIFdel{This is transferred }\DIFdelend \DIFaddbegin \DIFadd{It transfers }\DIFaddend to a string object that gets cleaned up by characters that are not relevant, before being hashed by the SHA-256 hash function. 
	\item \textbf{R7: Query Timestamping} \\
	The \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{implementation stores the timestamp }\DIFaddend of the original query execution\DIFdelbegin \DIFdel{is stored}\DIFdelend . 
	\item \textbf{R8: Query PID}\\
	The \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \DIFadd{query }\DIFaddend PID is getting created (using Python UUID\footnote{https://docs.python.org/3/library/uuid.html}) \DIFdelbegin \DIFdel{, }\DIFdelend if the same unique query and \DIFaddbegin \DIFadd{the }\DIFaddend resulting hash combination is not in the database yet.
	\item \textbf{R9: Store the Query} \\
	The Query Store is implemented as an additional table of an already existent relational \DIFdelbegin \DIFdel{meta-data base }\DIFdelend \DIFaddbegin \DIFadd{database }\DIFaddend of EODC for \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend (PostgreSQL). 
	\item \textbf{R10: Automated Citation Texts} \\
	The citation text for the data-set is already available at EODC, the generated data PID is added to it. 
	\item \textbf{R11 \& R12: Landing Page \& Machine Actionability} \\
	The \DIFdelbegin \DIFdel{landing page is defined at the EODC back end as an OpenEO }\DIFdelend \DIFaddbegin \DIFadd{EODC backend defines the landing page as an openEO }\DIFaddend endpoint. It is publicly accessible and in a JSON format, which makes it machine actionable. The landing page consists of an additional link to re-execute the query and \DIFdelbegin \DIFdel{list }\DIFdelend \DIFaddbegin \DIFadd{lists }\DIFaddend the result files. Since it is part of the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend API, it can also be accessed from the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend clients and be used in future jobs as \DIFdelbegin \DIFdel{used }\DIFdelend input data.
	\item \textbf{R13 \& R14: Technology Migration \& Migration Verification} \\
	These recommendations are not implemented in this thesis \DIFdelbegin \DIFdel{, }\DIFdelend since there are no migrations planned at EODC.
\end{itemize}

The test cases of the following sections \DIFdelbegin \DIFdel{are focused }\DIFdelend \DIFaddbegin \DIFadd{focus }\DIFaddend on the Sentinel Data of EODC, and how \DIFdelbegin \DIFdel{possible updated are handled by the solution }\DIFdelend \DIFaddbegin \DIFadd{the solution handles data updates}\DIFaddend . The running job (referenced as jobA) defined in Section \ref{example} is used for this evaluation. \DIFdelbegin \DIFdel{In }\DIFdelend \DIFaddbegin \DIFadd{At }\DIFaddend the beginning of each test case\DIFaddbegin \DIFadd{, }\DIFaddend the first step is to run a job on the empty database to create the first query Table entry. 

\DIFdelbegin \subsection*{\DIFdel{Test Case Preparation}}
%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Test Case Preparation}}
\DIFaddend This section describes the first step of each test case in the following sections. The purpose is to add \DIFdelbegin \DIFdel{an initial }\DIFdelend \DIFaddbegin \DIFadd{a first }\DIFaddend job execution with an input data \DIFdelbegin \DIFdel{pid}\DIFdelend \DIFaddbegin \DIFadd{PID}\DIFaddend , which is needed for each test scenario. Before this step is executed, the database of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is cleaned up.\\ The original data of EODC \DIFdelbegin \DIFdel{can not }\DIFdelend \DIFaddbegin \DIFadd{cannot }\DIFaddend be changed for this evaluation \DIFdelbegin \DIFdel{, }\DIFdelend since the evaluation \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is using the actual data endpoint (https://csw.eodc.eu). To simulate changes on the data, the query result received by the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend driver gets modified. Figure \ref{fig:eva_data_simulation} gives an overview \DIFdelbegin \DIFdel{on }\DIFdelend \DIFaddbegin \DIFadd{of }\DIFaddend where the data update simulator is located in the data identification implementation overview of Figure \ref{fig:impldataid}. \DIFdelbegin \DIFdel{What data will be modified is described by the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend test cases below \DIFaddbegin \DIFadd{describe what data is modified}\DIFaddend . \\

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{evaluation_data_simulation}
	\caption{Overview of the data update simulation. It shows the extension on the data identification implementation of Figure \ref{fig:impldataid}.}
	\label{fig:eva_data_simulation} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\begin{enumerate}
	\item \textbf{Run jobA, which creates query pidA. Get result files of pidA} \\
	Listing \ref{lst:eva_datachange_1} shows the code to run jobA using the python client. The researcher defines the spatial and temporal filter arguments and applies the NDVI and the minimum time process on it, described in Section \ref{example}. The "con.create\_job()" line sends the process graph to the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend and retrieves a job object containing the \DIFdelbegin \DIFdel{new }\DIFdelend \DIFaddbegin \DIFadd{newly }\DIFaddend generated job id. After executing jobA (calling start\_job()), a job entity and a query entity are created and persisted in the database. The query entity with a generated \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend PID is created by the \DIFdelbegin \DIFdel{Query Handler}\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}}\DIFaddend , since the \DIFdelbegin \DIFdel{data base }\DIFdelend \DIFaddbegin \DIFadd{database }\DIFaddend has no query entry. Table \ref{Tab:eva_datachanges1} shows the state of the \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table after the execution of the following python code.
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
		con = openeo.connect("http://openeo.local.127.0.0.1.nip.io")
# Choose dataset
processes = con.get_processes()
pgA = processes.get_collection(name="s2a_prd_msil1c")
pgA = processes.filter_daterange(pgA, extent=["2017-05-01", "2017-05-31"])
pgA = processes.filter_bbox(pgA, west=10.288696, south=45.935871, 
east=12.189331, north=46.905246, crs="EPSG:4326")
# Choose processes
pgA = processes.ndvi(pgA, nir="B08", red="B04")
pgA = processes.min_time(pgA)
# Create and start job A out of the process graph A (pgA)
jobA = con.create_job(pgA.graph)
jobA.start_job()
# Get \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend PID of jobA
pidA = jobA.get_data_pid()
# Re-execute the query to print the used files.
file_listA = con.get_filelist(pidA)
# Get state of the resultfiles, so if they changed since 
# the original execution 
file_listA["input_files"]["state"] # Returns "EQUAL"
		\end{minted}
		\caption{Researcher runs jobA and retrieves the result files status.}
		\label{lst:eva_datachange_1}
	\end{code}
	Figure \ref{fig:eva_data_changes_1_query} shows the original query that got executed at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . It is \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{an }\DIFaddend XML schema consisting \DIFaddbegin \DIFadd{of }\DIFaddend the filter values defined in the process graph (pgA). The \DIFdelbegin \DIFdel{time stamp of the original query execution is highlighted in the }\DIFdelend figure of the query \DIFaddbegin \DIFadd{highlights the timestamp of the first query execution}\DIFaddend . The content of the database tables as well as query re-execution results after each execution step are available in the result folder\footnote{https://github.com/bgoesswein/dataid\_openeo/tree/master/openeo-python-client/examples/results} of the GitHub repository. 

	\begin{figure}[h]
		\centering
		\includegraphics[width=\textwidth]{eva_data_changes_1_query}
		\caption{Original query of jobA at the first evaluation step.}
		\label{fig:eva_data_changes_1_query} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
	\end{figure}

	Listing \ref{lst:eva_datachange_nq1} shows the normalized query created by the \DIFdelbegin \DIFdel{Query Handler}\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}}\DIFaddend . It contains all possible filter arguments and their values in a JSON object alphabetically sorted.

	\begin{code}
		\begin{minted}[fontsize=\footnotesize]{text}
{'bands': None, 
'data_id': None, 
'derived_from': None, 
'extent': {'extent': {'crs': 'EPSG:4326', 'east': 12.189331, 
'north': 46.905246, 'south': 45.935871, 'west': 10.288696}}, 
'license': None, 
'name': 's2a_prd_msil1c', 
'time': {'extent': ['2017-05-01', '2017-05-31']}}
		\end{minted}
		\caption{Normalized query of the first query entry.}
		\label{lst:eva_datachange_nq1}
	\end{code}

	Listing \ref{lst:eva_datachange_rf1} shows the first four file paths and timestamps of the resulting file list, after executing the query in Figure \ref{fig:eva_data_changes_1_query}. The resulting file list (file\_listA["input\_files"]) consists of 51 files and the re-execution of the query results in the same 51 files. The path of each file \DIFdelbegin \DIFdel{is containing meta-data }\DIFdelend \DIFaddbegin \DIFadd{contains data }\DIFaddend about the data record. The term "s2a\_prd\_msil1c" defines the dataset identifier of Sentinel 2 at EODC. The \DIFdelbegin \DIFdel{next }\DIFdelend \DIFaddbegin \DIFadd{following }\DIFaddend folders define the date of the data record. The filename follows the ESA naming convention\footnote{https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/naming-convention}. The Nxxyy value defines the \DIFdelbegin \DIFdel{basline }\DIFdelend \DIFaddbegin \DIFadd{baseline }\DIFaddend number, Rxxx defines the orbit number of the satellite and Txxxxx defines the tile number. The first date is the sensing time and the second at the end of the file name is the product disclaimer, therefore identifies products that came from the data of the same sensing time. The \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend entry shows when the data was first available at EODC.

	\begin{code}
		\begin{minted}[fontsize=\footnotesize]{text}
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPR_20170504T101349.zip'}, 
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQS_20170504T101349.zip', 
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQR_20170504T101349.zip'}, 
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPT_20170504T101349.zip'},
...
		\end{minted}
		\caption{First four resulting files of the file list.}
		\label{lst:eva_datachange_rf1}
	\end{code}

	Table \ref{Tab:eva_datachanges1} shows the content of the \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table after the execution of this step. There is one entry with a new generated \DIFdelbegin \DIFdel{query pid}\DIFdelend \DIFaddbegin \DIFadd{data PID}\DIFaddend , by the \DIFdelbegin \DIFdel{Query Handler}\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}}\DIFaddend , since there was no query entry in the database that matches the normalized hash and the \DIFdelbegin \DIFdel{result }\DIFdelend \DIFaddbegin \DIFadd{resulting }\DIFaddend hash.

	%\begin{table}[]
	%DIF < 	\caption{Query table after the execution of Listing \ref{lst:eva_datachange_1}}
%DIF > 	\caption{\textit{Query} table after the execution of Listing \ref{lst:eva_datachange_1}}
	%	\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
	%		\hline	\textbf{query\_pid} & \textbf{dataset\_pid} & \textbf{original} & \textbf{normalized} & \textbf{norm\_hash} & \textbf{result\_hash} &
	%		\textbf{updated\_at} & \textbf{meta\_data} &   
	%		\textbf{created\_at} \\ \hline
	%		qu-a3bbe4a0-a875-4687-bb78-9457f33134a9 & s2a\_prd\_msil1c & see Figure \ref{fig:eva_data_changes_1_query} & see Listing \ref{lst:eva_datachange_nq1} & 0917c7a21cec960b8a6617b22ad26578c2c67f0b0501ba1a359b078c6c51d77d & abf43f519007050cbaeb59a067a2226d64b041c6d6ec323b2401109176e66455 & 2019-03-31 17:36:44.613893 & {'result\_files': 51} & 2019-03-31 17:36:43.064445 \\ \hline
	%	\end{tabular}
	%	\label{Tab:eva_datachanges1}
	%\end{table}

	\begin{table}[]
		\caption{\DIFdelbeginFL \DIFdelFL{Query }\DIFdelendFL \DIFaddbeginFL \textit{\DIFaddFL{Query}} \DIFaddendFL table after the execution of Listing \ref{lst:eva_datachange_1}}
		\centering
		\begin{tabular}{|r|l|}
			\hline \multicolumn{2}{|c|}{\textbf{Query pidA}} \\
			\hline \multicolumn{1}{|c|}{\textbf{Column}}  &  \multicolumn{1}{c|}{\textbf{Value}} \\ \hline
			query\_pid & qu-a3bbe4a0-a875-4687-bb78-9457f33134a9  \\ 
			dataset\_pid & s2a\_prd\_msil1c  \\ 
			original & see Figure \ref{fig:eva_data_changes_1_query}   \\
			normalized & see Listing \ref{lst:eva_datachange_nq1}  \\
			norm\_hash & 0917c7a21cec960b8a6617b22ad26578c2c67f0b0501ba1a359b078c6c51d77d  \\
			result\_hash & abf43f519007050cbaeb59a067a2226d64b041c6d6ec323b2401109176e66455   \\
			updated\_at & 2019-03-31 17:36:44.613893   \\
			meta\_data & {'result\_files': 51}  \\
			created\_at & 2019-03-31 17:36:43.064445   \\ \hline
		\end{tabular}
		\label{Tab:eva_datachanges1}
	\end{table}
\end{enumerate}

\DIFdelbegin \subsection*{\DIFdel{Test Case 1: Is it possible to re-execute a query after a file is updated?}}	%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Test Case 1: Is it possible to re-execute a query after a file is updated?}}	\DIFaddend \label{Tab:eva_datachanges_tc1}
%{ \large \textbf{} } \\

\begin{enumerate}
	\setcounter{enumi}{+1}
	\item \textbf{Update one of the resulting files of the pidA query} \\
	The "update\_file()" method is added to the python client and EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend to let the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend activate the "Data Update Simulator" shown in Figure \ref{fig:eva_data_simulation}. If it is activated\DIFaddbegin \DIFadd{, }\DIFaddend it simulates the update of the first file in the query result. The update sets the creation \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend of the file to the execution time of the "update\_file()" method and renames the updated output file with an additional "\_new". The update does not replace the old file \DIFdelbegin \DIFdel{, }\DIFdelend but adds a new file to the result.
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
# Update the first file of the pidA query resulting files.
con.update_file()
		\end{minted}
		\caption{Update one of the pidA resulting files, but keep the original file.}
		\label{lst:eva_datachange_2}
	\end{code}

	Listing \ref{lst:eva_datachange_fl2} shows the resulting file list with the activated "Data Update Simulator". There is now an additional file with an "\_new" \DIFdelbegin \DIFdel{in }\DIFdelend \DIFaddbegin \DIFadd{at }\DIFaddend the end of the path, but with the same \DIFdelbegin \DIFdel{meta-data }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend in the file path and name as the first file. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend the creation timestamp is set to the time of the execution of Listing \ref{lst:eva_datachange_2}.

	\begin{code}
		\begin{minted}[fontsize=\small]{text}
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPR_20170504T101349.zip'}, 
{'timestamp': '2019-03-31 17:44:43', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPR_20170504T101349_new.zip'}
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQS_20170504T101349.zip', 
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQR_20170504T101349.zip'}, 
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPT_20170504T101349.zip'},
...
		\end{minted}
		\caption{Modified file list output of the "Data Update Simulator" component.}
		\label{lst:eva_datachange_fl2}
	\end{code}

	\item \textbf{Re-execution of pidA query} \\
	In Listing \ref{lst:eva_datachange_3} the query with the identifier pidA gets re-executed, after the activation of the "Data Update Simulator" component in the previous step. The second query re-execution results in the same file list as the first (see Listing \ref{lst:eva_datachange_rf3}), since the old file is still available and the updated file is added after the execution \DIFdelbegin \DIFdel{time-stamp of the original }\DIFdelend \DIFaddbegin \DIFadd{timestamp of the first }\DIFaddend execution (see \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend element in the query at Figure \ref{fig:eva_data_changes_1_query}). The query contains the execution \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend of the original query and filters every file out of the result that has a creation \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend after that.
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
# Get state of the resultfiles, so if they changed since 
# the original execution. 
file_listA = con.get_filelist(pidA)
file_listA["input_files"]["state"] # Returns "EQUAL"
		\end{minted}
		\caption{Re-execute pidA query after one file got updated.}
		\label{lst:eva_datachange_3}
	\end{code}

	\begin{code}
		\begin{minted}[fontsize=\footnotesize]{text}
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPR_20170504T101349.zip'}, 
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQS_20170504T101349.zip', 
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQR_20170504T101349.zip'}, 
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPT_20170504T101349.zip'},
...
		\end{minted}
		\caption{First four resulting files of the file list.}
		\label{lst:eva_datachange_rf3}
	\end{code}

	\item \textbf{Run duplicate of jobA named jobB} \\
	Listing \ref{lst:eva_datachange_4} shows the execution of a second job using the same process graph as jobA. In the first line of Listing \ref{lst:eva_datachange_4} the process graph of jobA (defined in Listing \ref{lst:eva_datachange_1}) is used to create a new job instance named jobB. Therefore, jobB uses the same process graph as jobA \DIFdelbegin \DIFdel{, }\DIFdelend but gets executed after the update of the file. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend leads to a different file list, where the original and the new file is in the query result file list (see Listing \ref{lst:eva_datachange_fl3}).
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
# Reuse the defined process Graph (pgA) from jobA at Step 1 to create jobB
jobB = con.create_job(pgA.graph)
jobB.start_job()
# re-execute query and get the resulting file list from the \DIFdelbegin \DIFdel{back end
}\DIFdelend \DIFaddbegin \DIFadd{backend
}\DIFaddend pidB = jobB.get_data_pid()
file_listB = con.get_filelist(pidB)
# comparing the resultfiles of jobA with the resultfiles of jobB
(file_listA == file_listB) # Returns False
		\end{minted}
		\caption{Step 4: Create jobB, which uses the same process graph as jobA.}
		\label{lst:eva_datachange_4}
	\end{code}

	\begin{code}
		\begin{minted}[fontsize=\small]{text}
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPR_20170504T101349.zip'}, 
{'timestamp': '2019-03-31 17:44:43', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPR_20170504T101349_new.zip'}
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQS_20170504T101349.zip', 
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQR_20170504T101349.zip'}, 
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPT_20170504T101349.zip'},
...
		\end{minted}
		\caption{Resulting file list of jobB.}
		\label{lst:eva_datachange_fl3}
	\end{code}

	Table \ref{Tab:eva_datachanges3} shows the \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table of the \DIFdelbegin \DIFdel{meta-database }\DIFdelend \DIFaddbegin \DIFadd{database }\DIFaddend after the execution of Listing \ref{lst:eva_datachange_4}. There is now an additional query entry. In the table\DIFaddbegin \DIFadd{, }\DIFaddend the important differences are marked red. There is a different result hash \DIFdelbegin \DIFdel{, }\DIFdelend since the number of result files has increased (see Listing \ref{lst:eva_datachange_fl3}). The normalized query is still the same, but since the result of the query changed\DIFdelbegin \DIFdel{a new query pid }\DIFdelend \DIFaddbegin \DIFadd{, a new data PID }\DIFaddend is generated. 

	\begin{table}[]
		\caption{\DIFdelbeginFL \DIFdelFL{Query }\DIFdelendFL \DIFaddbeginFL \textit{\DIFaddFL{Query}} \DIFaddendFL table after the execution of Listing \ref{lst:eva_datachange_4}. Important elements are highlighted blue if they are the same and red if they are different.}
		\centering
		\begin{tabular}{|r|l|}
			\hline \multicolumn{2}{|c|}{\textbf{Query pidA (full entry in Table \ref{Tab:eva_datachanges1})}} \\
			\hline \multicolumn{1}{|c|}{\textbf{Column}}  &  \multicolumn{1}{c|}{\textbf{Value}} \\ \hline
			query\_pid & {\color{red}qu-a3bbe4a0-a875-4687-bb78-9457f33134a9}  \\ 
			norm\_hash & {\color{blue}0917c7a21cec960b8a6617b22ad26578c2c67f0b0501ba1a359b078c6c51d77d}  \\
			result\_hash & {\color{red}abf43f519007050cbaeb59a067a2226d64b041c6d6ec323b2401109176e66455}   \\
			meta\_data & {'result\_files': 51}  \\
			\hline \multicolumn{2}{|c|}{\textbf{Query pidB}} \\
			\hline \multicolumn{1}{|c|}{\textbf{Column}}  &  \multicolumn{1}{c|}{\textbf{Value}} \\ \hline
			query\_pid & { \color{red} qu-23f5a313-e804-4faa-aa33-60ed1ac69e2d}  \\ 
			dataset\_pid & s2a\_prd\_msil1c  \\ 
			original & see Figure \ref{fig:appendix_pidB} in the appendix \\
			normalized & see Listing \ref{lst:eva_datachange_nq1}  \\
			norm\_hash & {\color{blue}0917c7a21cec960b8a6617b22ad26578c2c67f0b0501ba1a359b078c6c51d77d}  \\
			result\_hash & {\color{red}28088d113de19ce037e9651a64bae3f7957822665f17d8f4e2c7e6b2cf4250b3 }  \\
			updated\_at & 2019-03-31 18:01:49.214956   \\
			meta\_data & {\color{red}'result\_files': 52}  \\
			created\_at & 2019-03-31 18:01:47.695042   \\ \hline
		\end{tabular}
		\label{Tab:eva_datachanges3}
	\end{table}

	\item \textbf{Run duplicate of jobA, by using the data PID of jobA named jobC}\\
	\DIFdelbegin \DIFdel{To create jobC, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend persistent input data identifier of job A (pidA) is used as the input data \DIFaddbegin \DIFadd{to create jobC}\DIFaddend . The execution \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend is part of the query in pidA\DIFaddbegin \DIFadd{, }\DIFaddend and the original query gets executed for jobC. Since jobC uses the same data PID as jobA, the updated file is filtered out of the resulting files\DIFaddbegin \DIFadd{, }\DIFaddend and there is no new \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend PID generated. Therefore, the \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table is still in the state of Table \ref{Tab:eva_datachanges3}. 
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
# Take input data of job A by using the input data \DIFdelbegin \DIFdel{pid }\DIFdelend \DIFaddbegin \DIFadd{PID }\DIFaddend A of job A
pgC = processes.get_data_by_pid(data_pid=pidA)
# Choose processes
pgC = processes.ndvi(pgC, nir="B08", red="B04")
pgC = processes.min_time(pgC)
# Create and start Job C
jobC = con.create_job(pgC.graph)
jobC.start_job()
# re-execute query and get the resulting file list from the \DIFdelbegin \DIFdel{back end
}\DIFdelend \DIFaddbegin \DIFadd{backend
}\DIFaddend pidC = jobC.get_data_pid()
file_listC = con.get_filelist(pidC)
# Compare resulting files with the original execution of jobA
(file_listA == file_listC) # Returns True
		\end{minted}
		\caption{Create job C, which uses the input data identified by pidA.}
		\label{lst:eva_datachange_5}
	\end{code}
\end{enumerate}

Table \ref{Tab:eva_datachanges4} presents the mapping between the executed jobs and the input data \DIFdelbegin \DIFdel{query pid }\DIFdelend \DIFaddbegin \DIFadd{PID }\DIFaddend of Test Case 1. The results are consistent if files are updated at the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . Jobs that used the original \DIFdelbegin \DIFdel{query pid }\DIFdelend \DIFaddbegin \DIFadd{data PID }\DIFaddend as input data, also have used the data defined by the \DIFdelbegin \DIFdel{pid}\DIFdelend \DIFaddbegin \DIFadd{PID}\DIFaddend , even after the update, because the original file was still available. The current way of reproducing a job in \DIFdelbegin \DIFdel{OpenEO}\DIFdelend \DIFaddbegin \DIFadd{openEO}\DIFaddend , by applying the same process graph (jobB), fails since the resulting files differ from the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution (jobA). The \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend information for the filtering is missing. Hence,  the input data of jobB result in a different \DIFdelbegin \DIFdel{query pid}\DIFdelend \DIFaddbegin \DIFadd{data PID}\DIFaddend .     

\begin{table}[]
	\caption{Resulting mapping of the jobs and the used \DIFdelbeginFL \DIFdelFL{query pid }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{data PID }\DIFaddendFL of Test Case 1.}
	\centering
	\begin{tabular}{|r|l|}
		\hline \multicolumn{1}{|c|}{\textbf{Job}}  &  \multicolumn{1}{c|}{\textbf{Query PID}} \\ \hline
		jobA & qu-a3bbe4a0-a875-4687-bb78-9457f33134a9  \\ 
		jobB & qu-23f5a313-e804-4faa-aa33-60ed1ac69e2d \\
		jobC & qu-a3bbe4a0-a875-4687-bb78-9457f33134a9  \\ \hline
	\end{tabular}
	\label{Tab:eva_datachanges4}
\end{table}

\DIFdelbegin \subsection*{\DIFdel{Test Case 2: Is it possible to re-execute a query after a file is updated with the original one deleted?}}
%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Test Case 2: Is it possible to re-execute a query after a file is updated with the original one deleted?}}
\DIFaddend 

\begin{enumerate}
	\setcounter{enumi}{+1}
	\item \textbf{Update one of the resulting files of the pidA query and remove the original one.}\\ 
	The method in Listing \ref{lst:eva_datachange_6} updates the first file of the query result as described in Section \ref{Tab:eva_datachanges_tc1} and removes the original file. Listing \ref{lst:eva_datachange_fl4} shows the updated file list, where the first entry replaced the original first file (see Listing \ref{lst:eva_datachange_fl2}).
	\begin{code}
		\begin{minted}{python}
		con.update_file(deleted=True)
		\end{minted}
		\caption{Update one of the pidA resulting files and delete the original file.}
		\label{lst:eva_datachange_6}
	\end{code}

	\begin{code}
		\begin{minted}[fontsize=\small]{text} 
{'timestamp': '2019-03-31 17:44:43', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPR_20170504T101349_new.zip'}
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQS_20170504T101349.zip', 
{'timestamp': '2017-05-08 00:00:00', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TQR_20170504T101349.zip'}, 
{'timestamp': '2017-05-08 00:00:00',
'path':'/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/04/
S2A_MSIL1C_20170504T101031_N0205_R022_T32TPT_20170504T101349.zip'},
...
		\end{minted}
		\caption{Modified file list output of the "Data Update Simulator" component, by removing the original file from the list.}
		\label{lst:eva_datachange_fl4}
	\end{code}	

	\item \textbf{Get File-list of pidA}\\ 
	The re-execution of the query pidA results in a file list without the deleted file. Since files are filtered out by the query due to the execution \DIFdelbegin \DIFdel{time-stamp}\DIFdelend \DIFaddbegin \DIFadd{timestamp}\DIFaddend , the new file does not appear in the result file list. If the re-execution results not in the same file list, the "state" attribute consists of the list of files that replaced files that are no longer available. Listing \ref{lst:eva_datachange_state} shows the content of the "state" attribute of Listing \ref{lst:eva_datachange_7}. The \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend returns the most recent file version, even if there are versions between the original and the most recent file available. In the result of Listing \ref{lst:eva_datachange_7} the "state" contains only the one file that replaces the original file. So users can see the alternatives for the original file, but not the original file itself. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend is because the full file list is not persisted in the query store, but the number of result files. The decision for not storing the complete file list comes from EODC, because of the additional needed storage size it would cause and the rare occasion of \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend data update. If the number of resulting files is different at a re-execution, the query gets executed without the \DIFdelbegin \DIFdel{original time-stamp filter}\DIFdelend \DIFaddbegin \DIFadd{first timestamp filter, }\DIFaddend and the result is compared to the re-execution with the original execution \DIFdelbegin \DIFdel{time-stamp}\DIFdelend \DIFaddbegin \DIFadd{timestamp}\DIFaddend . All files that are missing in the re-execution with the original execution \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend are added to the "state" attribute. The researcher can use the identifying file path to order \DIFaddbegin \DIFadd{an }\DIFaddend older version from EODC or ESA.   
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
# re-execute query and get the resulting file list from the \DIFdelbegin \DIFdel{back end
}\DIFdelend \DIFaddbegin \DIFadd{backend
}\DIFaddend file_listA = con.get_filelist(pidA)
# Stdout: Warning: The resulting file list changed from the original query
# execution! Look into the "state" attribute to see the list of files that
# have changed. 
file_listA["input_files"]["state"] # Returns one file entry
		\end{minted}
		\caption{Re-execute pidA query after one file got updated and the old version got erased.}
		\label{lst:eva_datachange_7}
	\end{code}	

	%\begin{figure}[h]
	%	\centering
	%	\includegraphics[width=\textwidth]{eva_data_changes_7_state}
	%	\caption{Step 1: State of the re-execution of pidA at the 7th evaluation step.}
	%	\label{fig:eva_data_changes_7_state} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
	%\end{figure}

	\begin{code}
		\begin{minted}[fontsize=\small]{text} 
[{'timestamp': '2019-03-31 17:44:43', 
'path': '/eodc/products/copernicus.eu/s2a_prd_msil1c/2017/05/24/
S2A_MSIL1C_20170524T101031_N0205_R022_T32TQR_20170524T101353_new.zip'}]
		\end{minted}
		\caption{List of files that replaced original files of the query result.}
		\label{lst:eva_datachange_state}
	\end{code}

	\item \textbf{Run duplicate of jobA, by using the data PID of jobA named jobD}\\
	Listing \ref{lst:eva_datachange_8} shows the code of running a new jobD with the \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend PID pidA. The code is executed after one file is deleted from the resulting file list of pidA. After the creation of the job, the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend notices that the number of result files is different than the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution of the input \DIFdelbegin \DIFdel{query pid}\DIFdelend \DIFaddbegin \DIFadd{data PID}\DIFaddend . Therefore, a warning message is displayed into the console of the researcher. If the \DIFdelbegin \DIFdel{job gets then started }\DIFdelend \DIFaddbegin \DIFadd{scientist starts the job }\DIFaddend nevertheless, the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend looks for updated files like described in the previous step and adds the most recent version of the missing file to the query result. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend leads to a new \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend PID for jobD, since the query result has changed. Table \ref{Tab:eva_datachanges5} show the \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table status after the execution. The second query entry has a different result hash, since one file changed and therefore, gets a new \DIFdelbegin \DIFdel{query pid}\DIFdelend \DIFaddbegin \DIFadd{data PID}\DIFaddend . 
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
# Take input data of job A by using the input data \DIFdelbegin \DIFdel{pid }\DIFdelend \DIFaddbegin \DIFadd{PID }\DIFaddend A of job A
pgD = processes.get_data_by_pid(data_pid=pidA)
# Choose processes
pgD = processes.ndvi(pgC, nir="B08", red="B04")
pgD = processes.min_time(pgD)
# Create and start Job D
jobD = con.create_job(pgD.graph)
# Stdout: Warning: The resulting file list changed from the original query
# execution! Look into the "state" attribute to see the list of files that
# have changed. 
jobD.start_job()
pidD = jobD.get_data_pid() # pidD != pidA
		\end{minted}
		\caption{Run duplicate of jobA, by using the data PID of jobA named jobD.}
		\label{lst:eva_datachange_8}
	\end{code}

	\begin{table}[]
		\caption{\DIFdelbeginFL \DIFdelFL{Query }\DIFdelendFL \DIFaddbeginFL \textit{\DIFaddFL{Query}} \DIFaddendFL table after the execution of Listing \ref{lst:eva_datachange_4}. Important elements are highlighted blue if they are the same and red if they are different.}
		\centering
		\begin{tabular}{|r|l|}
			\hline \multicolumn{2}{|c|}{\textbf{Query pidA (full entry in Table \ref{Tab:eva_datachanges1})}} \\
			\hline \multicolumn{1}{|c|}{\textbf{Column}}  &  \multicolumn{1}{c|}{\textbf{Value}} \\ \hline
			query\_pid & {\color{red}qu-a3bbe4a0-a875-4687-bb78-9457f33134a9}  \\ 
			norm\_hash & {\color{blue}0917c7a21cec960b8a6617b22ad26578c2c67f0b0501ba1a359b078c6c51d77d}  \\
			result\_hash & {\color{red}abf43f519007050cbaeb59a067a2226d64b041c6d6ec323b2401109176e66455}   \\
			meta\_data & {'result\_files': 51}  \\
			\hline \multicolumn{2}{|c|}{\textbf{Query pidD}} \\
			\hline \multicolumn{1}{|c|}{\textbf{Column}}  &  \multicolumn{1}{c|}{\textbf{Value}} \\ \hline
			query\_pid & { \color{red} qu-3544aeae-cd24-4b6d-ad34-0d674c2a400f}  \\ 
			dataset\_pid & s2a\_prd\_msil1c  \\ 
			original & see Figure \ref{fig:appendix_pidD} in the appendix \\
			normalized & see Listing \ref{lst:eva_datachange_nq1}  \\
			norm\_hash & {\color{blue}0917c7a21cec960b8a6617b22ad26578c2c67f0b0501ba1a359b078c6c51d77d}  \\
			result\_hash & {\color{red}28088d113de19ce037e9651a64bae3f7957822665f17d8f4e2c7e6b2cf4250b3}  \\
			updated\_at & 2019-03-31 18:10:26.931578   \\
			meta\_data & {'result\_files': 51}  \\
			created\_at & 2019-03-31 18:10:25.46402   \\ \hline
		\end{tabular}
		\label{Tab:eva_datachanges5}
	\end{table}
\end{enumerate}

The result of the second Test Case shows how the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend behaves on data updated that replace the original data. If the file is deleted, the researcher gets a warning message and a list of files that are replacing the original files. Since the path of the file identifies the date and tile that was used, the researcher \DIFdelbegin \DIFdel{ask }\DIFdelend \DIFaddbegin \DIFadd{asks }\DIFaddend EODC about the concrete changed file. On creation of the new jobD that uses the input data identifier pidA, the user gets notified that the result files changed, before the execution happens. Then the user can decide to start the job \DIFdelbegin \DIFdel{anyway }\DIFdelend \DIFaddbegin \DIFadd{nevertheless }\DIFaddend or contact EODC about the modified files.

\DIFdelbegin \subsection*{\DIFdel{Test Case 3: Is it possible to re-execute a query after a data file is deleted}}
%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Test Case 3: Is it possible to re-execute a query after a data file is deleted?}}
\DIFaddend The deletion of a file without a new file replacing it \DIFdelbegin \DIFdel{, }\DIFdelend is not within the policies of EODC \DIFdelbegin \DIFdel{, }\DIFdelend since they would restrict their range on available data. If this happens nevertheless, there is in the current solution\DIFaddbegin \DIFadd{, }\DIFaddend no possibility to get the exact \DIFdelbegin \DIFdel{files that were removed }\DIFdelend \DIFaddbegin \DIFadd{removed files}\DIFaddend . The number of result files in the meta\_data column \DIFdelbegin \DIFdel{show }\DIFdelend \DIFaddbegin \DIFadd{shows }\DIFaddend how many files changed. \DIFdelbegin \DIFdel{To achieve the knowledge of missing files, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend whole result files need to be added to the \DIFdelbegin \DIFdel{query table , so that it can be compared with the result of the re-execution}\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFadd{table to achieve the knowledge of missing files}\DIFaddend . In the current solution\DIFaddbegin \DIFadd{, }\DIFaddend EODC needs to be notified about the missing files. EODC has to look up what files were deleted using the filter arguments of the query to identify the original files and copy them from ESA again. 

%The researcher can query the data at ESA directly with the same filter arguments and can compare the resulting file list with the ones from the re-execution of EODC, but the solution is not doing it automatically.  

%DIF < The remaining question is how the system behaves if the data is updated during the query processing. Since there is no permission to perform a test on it, it can only be assumed theoretically. Since the query system is file based and an update results in a new file replacing an existing file. There are two possibilities of the outcome of a query be executed at the same time. Either the query results in the old file, which then will result in an error at the execution if the file is replaced already at the time of the job execution, or the query results in the new file. In both cases the query PID will be generated consistent with the job execution, since it takes the original executed query and the actual file list used for the execution.     
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend %DIF > The remaining question is how the system behaves if the data is updated during the query processing. Since there is no permission to perform a test on it, it can only be assumed theoretically. Since the query system is file based and an update results in a new file replacing an existing file. There are two possibilities of the outcome of a query be executed at the same time. Either the query results in the old file, which then will result in an error at the execution if the file is replaced already at the time of the job execution, or the query results in the new file. In both cases the data PID will be generated consistent with the job execution, since it takes the original executed query and the actual file list used for the execution.     
\section{Job Capturing}\label{Evaluation:special_jobcap}

This section reviews the outcome of \DIFdelbegin \DIFdel{the }\DIFdelend job capturing. The focus is on the created context model of the job executions. The following \DIFdelbegin \DIFdel{two question are }\DIFdelend \DIFaddbegin \DIFadd{question is }\DIFaddend used to discuss the impact of the captured data regarding \DIFdelbegin \DIFdel{the }\DIFdelend job execution.
  \\

\textbf{Is it possible to recreate an older version of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend ?} \\
The \DIFdelbegin \DIFdel{aim of the implementation is }\DIFdelend \DIFaddbegin \DIFadd{implementation aims }\DIFaddend to capture enough data to make it possible to re-run the same job. The EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is created directly by its GitHub repository. \DIFdelbegin \DIFdel{To recreate an old version of the back end, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend GitHub repository URL and the commit identifier of the original set up \DIFdelbegin \DIFdel{is needed . The time-stamp }\DIFdelend \DIFaddbegin \DIFadd{are needed to recreate an old version of the backend. The timestamp }\DIFaddend of the job execution is persisted in the context model of the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend job execution. The \DIFaddbegin \DIFadd{backend provenance can resolve the }\DIFaddend original GitHub repository and commit\DIFdelbegin \DIFdel{can be resolved by the back end provenance}\DIFdelend . Therefore, the information \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{on }\DIFaddend re-creating the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend state from an older version is captured. \DIFdelbegin \DIFdel{To re-create the original job on the back end, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend process graph of it is persisted and can be re-executed \DIFaddbegin \DIFadd{to re-create the first job on the backend}\DIFaddend . The execution of the job is the same, assuming the input data has not been deleted in the meantime, which can be checked by re-executing the data query of the job. The \DIFdelbegin \DIFdel{result }\DIFdelend \DIFaddbegin \DIFadd{resulting }\DIFaddend hash makes it possible to validate \DIFdelbegin \DIFdel{, }\DIFdelend if the job re-execution was the same. The following code presents how this \DIFdelbegin \DIFdel{work-flow }\DIFdelend \DIFaddbegin \DIFadd{workflow }\DIFaddend can be achieved in the solution.

\begin{enumerate}
	\item \textbf{Run Job A, which creates query pidA and job\_idA.} \\
	The first step shows the definition, creation\DIFaddbegin \DIFadd{, }\DIFaddend and execution of jobA. The last two lines show how the version can be retrieved from the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . The "con.version()" method returns the current version of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , but can also take a \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend as a parameter to get the version of that time. The "jobA.get\_backend\_version()" method returns the version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend during the execution of jobA. Both \DIFdelbegin \DIFdel{version }\DIFdelend \DIFaddbegin \DIFadd{versions }\DIFaddend are at the end of Step 1 identical. 
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
import openeo
# Connect with GEE backend
con = openeo.connect("http://openeo.local.127.0.0.1.nip.io")
# Choose dataset
processes = con.get_processes()
pgA = processes.get_collection(name="s2a_prd_msil1c")
pgA = processes.filter_daterange(pgA, extent=["2017-05-01", "2017-05-31"])
pgA = processes.filter_bbox(pgA, west=10.288696, south=45.935871, 
east=12.189331, north=46.905246, crs="EPSG:4326")
# Choose processes
pgA = processes.ndvi(pgA, nir="B08", red="B04")
pgA = processes.min_time(pgA)
# Create and start job A
jobA = con.create_job(pgA.graph)
jobA.start_job()
# Get current \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version
version_old = con.version()
# Get \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version of jobA 
versionA = jobA.get_backend_version()
		\end{minted}
		\caption{Step 1: Researcher runs Job A and gets the used \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version.}
		\label{lst:eva_jobcapture_1}
	\end{code}

	\begin{code}
		\begin{minted}[fontsize=\small]{json}
{'branch': 'master',
'commit': '1a0cefd25c2a0fbb64a78cd9445c3c9314eaeb5b',
'url': 'https://github.com/bgoesswein/implementation_backend.git'}
		\end{minted}
		\caption{Step 1: Version of the jobA execution version\_old.}
		\label{lst:eva_jobcapture_1_1}
	\end{code} 

	\item \textbf{Publish job\_idA and pidA.} \\
	The researcher can get the persistent input data identifier pidA and the job identifier jobA\_id to cite the provenance of the execution. The \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend of the execution in the context model can be used to identify the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version used for the calculation.
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
# Get input data \DIFdelbegin \DIFdel{pid }\DIFdelend \DIFaddbegin \DIFadd{PID }\DIFaddend of jobA 
pidA = jobA.get_data_pid()
jobA_id = jobA.job_id
		\end{minted}
		\caption{Researcher gets the input data PID of jobA and the job\_id of jobA.}
		\label{lst:eva_jobcapture_2}
	\end{code}

	\item \textbf{Update \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version} \\
	In this step\DIFdelbegin \DIFdel{the back end }\DIFdelend \DIFaddbegin \DIFadd{, the backend }\DIFaddend gets modified slightly \DIFdelbegin \DIFdel{, }\DIFdelend by updating a python package to the requirements file of the job execution. The file gets edited by replacing the line "urllib3==1.23" with "urllib3==1.24.1". After editing the file the "git commit" command gets called. 
	\item \textbf{Get original context of jobA} \\
	Listing \ref{lst:eva_jobcapture_4} shows the code the researcher has to run to get the original version of the jobA execution. The variable "versionA" is containing the same version that is displayed in Listing \ref{lst:eva_jobcapture_1_1}. 
	\item \textbf{Re-run jobA with the original version.} \\
	To get the original version of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend , EODC has to create a second instance of the \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend . Then EODC has to check out the commit of the job execution version, by running "git checkout commit\_id" in the console, where "commit\_id" is the value of "versionA["commit"]".
	\begin{code}
		\begin{minted}[fontsize=\small]{python}
import openeo
# Connect with GEE backend
con = openeo.connect("http://openeo.local.127.0.0.1.nip.io")
# Get jobA using the jobA_id.
jobA = con.get_job(jobA_id)
# Get the version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend that was active during the job A execution
versionA = jobA.get_backend_version()
		\end{minted}
		\caption{Researcher gets the original \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend version of the jobA execution.}
		\label{lst:eva_jobcapture_4}
	\end{code}	

\end{enumerate}

\section{Performance and Storage Impact}\label{Evaluation:impact}

This section evaluates the \DIFaddbegin \DIFadd{performance and storage }\DIFaddend impact of the implementation \DIFdelbegin \DIFdel{on the EODC back end. To achieve this, }\DIFdelend \DIFaddbegin \DIFadd{of the EODC backend. }\DIFaddend 18 input process graphs from 9 publications that used data provided by EODC from the last two years are defined \DIFdelbegin \DIFdel{. The process graphs are not representing the same process chain, since the processes implemented for OpenEO at EODC are limited at the time of writing this thesis. The spatial and temporal extent are taken from the }\DIFdelend \DIFaddbegin \DIFadd{to achieve that. The }\DIFaddend data used in the papers \DIFaddbegin \DIFadd{provide spatial and temporal extents}\DIFaddend . These 18 input process graphs represent the 18 test cases, Table \ref{Tab:appendix} shows the \DIFdelbegin \DIFdel{used }\DIFdelend papers used for the test cases. The \DIFdelbegin \DIFdel{values of the spatial and temporal extent are stored in the }\DIFdelend evaluation code (evaluation\_impact.py) \DIFdelbegin \DIFdel{. The impact is measured by the impact on the performance and the impact on the storage. The fact that the evaluation setup is using a local OpenEO back end makes the performance measurements }\DIFdelend \DIFaddbegin \DIFadd{contains the values of the spatial and temporal extent. The performance }\DIFaddend of the solution \DIFdelbegin \DIFdel{not comparable to the productive system of the EODC back end. Therefore, the performance of the solution }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend is compared to \DIFdelbegin \DIFdel{an EODC back end}\DIFdelend \DIFaddbegin \DIFadd{a local EODC backend}\DIFaddend , without the extensions of this thesis (\DIFdelbegin \DIFdel{hereinafter }\DIFdelend \DIFaddbegin \DIFadd{from now on }\DIFaddend referred to as "reference \DIFdelbegin \DIFdel{back end}\DIFdelend \DIFaddbegin \DIFadd{backend}\DIFaddend "). Note that in the reference \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend the processing is mocked up in the same way as the solution system\DIFdelbegin \DIFdel{as well as the added timestamps in the logging environment for measuring the duration time. The performance and storage measurements are captured in the same code execution.  
Every input process graph is executed 50 times at each back end and after each iteration the duration and the used storage size is captured. After that the back end gets cleaned up, so that each iteration happens in the same back end conditions. The clean up process removes all database entries. The results are then stored in one JSON file for the storage and one for the performance. For better readability the results are stored additionally in CSV format.      
}\DIFdelend \DIFaddbegin \DIFadd{. Figure \ref{fig:experiment_overview} gives an overview of the evaluation setup.  
}\DIFaddend 

\DIFaddbegin \begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{experiment_overview}
	\caption{\DIFaddFL{Overview of the evaluation setup.}}
	\label{fig:experiment_overview} %DIF >  \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\DIFaddend \begin{table}[]
	\caption{List of geoscientific papers used for the input data of the impact evaluation in Section \ref{Evaluation:Setup}}
	\centering
	\begin{tabular}{c|c|c}
		\textbf{Test Cases} & \textbf{DOI} & \textbf{Citation}  \\ \hline
		1 & 10.1080/01431160902887339 & \cite{evaluation1} \\ 
		2-4 & 10.3390/rs8050402 & \cite{evaluation2} \\ 
		5,6 & 10.1016/j.jag.2014.12.001  & \cite{evaluation3} \\
		7 & 10.1016/j.jag.2016.12.003  & \cite{evaluation4} \\
		8 & 10.3390/rs8110938  & \cite{evaluation5} \\
		9 & 10.1080/2150704X.2016.1225172  & \cite{evaluation6} \\
		10-13 & 10.1109/TGRS.2018.2858004  & \cite{evaluation7} \\
		14 & 10.1080/01431161.2018.1479788  & \cite{evaluation8} \\
		15-18 & 10.3390/rs10071030  & \cite{evaluation9} \\
	\end{tabular}
	\label{Tab:appendix}
\end{table}

\subsection{Performance}\label{Evaluation:impact_perf}

In this section\DIFaddbegin \DIFadd{, }\DIFaddend we evaluate the performance impact on the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend by measuring the difference of \DIFdelbegin \DIFdel{the job execution duration }\DIFdelend \DIFaddbegin \DIFadd{job execution durations }\DIFaddend between the reference \DIFdelbegin \DIFdel{EODC back end and the EODC back end with the solution of this thesis}\DIFdelend \DIFaddbegin \DIFadd{backend and the solution backend}\DIFaddend . The execution time of the implementation is measured by writing timestamps into a log file and calculating the duration \DIFdelbegin \DIFdel{afterwards. The duration of the Query Handler }\DIFdelend \DIFaddbegin \DIFadd{afterward. Duration of the }\textit{\DIFadd{Query Handler}} \DIFaddend (see Section \ref{Implementation:Data Identification}) execution is measured, as well as the duration of the context model creation process (described in Section \ref{Implementation:Job dependent provenance}). \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < \begin{table}[]
%DIFDELCMD < 	%%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Mean duration time over 50 runs of the solution and the reference back end by executing the test cases}}
	%DIFAUXCMD
%DIFDELCMD < \centering
%DIFDELCMD < 	\begin{tabular}{r|r|r|r|r|r}
%DIFDELCMD < 		

%DIFDELCMD < 		%%%
\textbf{} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Ref. Back end}} %DIFAUXCMD
%DIFDELCMD < & \multicolumn{3}{c|}{\textbf{Solution Back end}} &  \\ \hline %%%
\textbf{\DIFdelFL{Test}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Comparison}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Query}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Context}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Solution}} %DIFAUXCMD
%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\textbf{\DIFdelFL{Case}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Total}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Total}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Handler}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Model}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{Addition}} %DIFAUXCMD
%DIFDELCMD < \\ \hline
%DIFDELCMD < 		%%%
\DIFdelFL{1 }%DIFDELCMD < & %%%
\DIFdelFL{322.946 ms }%DIFDELCMD < & %%%
\DIFdelFL{345.127 ms }%DIFDELCMD < & %%%
\DIFdelFL{14.187 ms }%DIFDELCMD < & %%%
\DIFdelFL{7.994 ms }%DIFDELCMD < & %%%
\DIFdelFL{22.181 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{2 }%DIFDELCMD < & %%%
\DIFdelFL{369.066 ms }%DIFDELCMD < & %%%
\DIFdelFL{393.505 ms }%DIFDELCMD < & %%%
\DIFdelFL{16.342 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.097 ms }%DIFDELCMD < & %%%
\DIFdelFL{24.439 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{3 }%DIFDELCMD < & %%%
\DIFdelFL{281.657 ms }%DIFDELCMD < & %%%
\DIFdelFL{305.407 ms }%DIFDELCMD < & %%%
\DIFdelFL{15.669 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.081 ms }%DIFDELCMD < & %%%
\DIFdelFL{23.750 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{4 }%DIFDELCMD < & %%%
\DIFdelFL{276.324 ms }%DIFDELCMD < & %%%
\DIFdelFL{298.954 ms }%DIFDELCMD < & %%%
\DIFdelFL{14.015 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.615 ms }%DIFDELCMD < & %%%
\DIFdelFL{22.630 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{5 }%DIFDELCMD < & %%%
\DIFdelFL{312.150 ms }%DIFDELCMD < & %%%
\DIFdelFL{334.802 ms }%DIFDELCMD < & %%%
\DIFdelFL{13.925 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.727 ms }%DIFDELCMD < & %%%
\DIFdelFL{22.652 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{6 }%DIFDELCMD < & %%%
\DIFdelFL{314.571 ms }%DIFDELCMD < & %%%
\DIFdelFL{337.290 ms }%DIFDELCMD < & %%%
\DIFdelFL{13.985 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.734 ms }%DIFDELCMD < & %%%
\DIFdelFL{22.719 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{7 }%DIFDELCMD < & %%%
\DIFdelFL{320.081 ms }%DIFDELCMD < & %%%
\DIFdelFL{343.552 ms }%DIFDELCMD < & %%%
\DIFdelFL{14.555 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.916 ms }%DIFDELCMD < & %%%
\DIFdelFL{23.471 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{8 }%DIFDELCMD < & %%%
\DIFdelFL{304.998 ms }%DIFDELCMD < & %%%
\DIFdelFL{328.633 ms }%DIFDELCMD < & %%%
\DIFdelFL{14.742 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.893 ms }%DIFDELCMD < & %%%
\DIFdelFL{23.635 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{9 }%DIFDELCMD < & %%%
\DIFdelFL{565.289 ms }%DIFDELCMD < & %%%
\DIFdelFL{740.751 ms }%DIFDELCMD < & %%%
\DIFdelFL{48.766 ms }%DIFDELCMD < & %%%
\DIFdelFL{126.696 ms }%DIFDELCMD < & %%%
\DIFdelFL{175.462 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{10 }%DIFDELCMD < & %%%
\DIFdelFL{401.922 ms }%DIFDELCMD < & %%%
\DIFdelFL{425.026 ms }%DIFDELCMD < & %%%
\DIFdelFL{14.874 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.230 ms }%DIFDELCMD < & %%%
\DIFdelFL{23.104 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{11 }%DIFDELCMD < & %%%
\DIFdelFL{521.022 ms }%DIFDELCMD < & %%%
\DIFdelFL{605.185 ms }%DIFDELCMD < & %%%
\DIFdelFL{34.660 ms }%DIFDELCMD < & %%%
\DIFdelFL{49.503 ms }%DIFDELCMD < & %%%
\DIFdelFL{84.163 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{12 }%DIFDELCMD < & %%%
\DIFdelFL{387.536 ms }%DIFDELCMD < & %%%
\DIFdelFL{412.079 ms }%DIFDELCMD < & %%%
\DIFdelFL{15.711 ms }%DIFDELCMD < & %%%
\DIFdelFL{8.832 ms }%DIFDELCMD < & %%%
\DIFdelFL{24.543 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{13 }%DIFDELCMD < & %%%
\DIFdelFL{510.517 ms }%DIFDELCMD < & %%%
\DIFdelFL{538.784 ms }%DIFDELCMD < & %%%
\DIFdelFL{17.070 ms }%DIFDELCMD < & %%%
\DIFdelFL{11.197 ms }%DIFDELCMD < & %%%
\DIFdelFL{28.267 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{14 }%DIFDELCMD < & %%%
\DIFdelFL{657.989 ms }%DIFDELCMD < & %%%
\DIFdelFL{706.329 ms }%DIFDELCMD < & %%%
\DIFdelFL{19.010 ms }%DIFDELCMD < & %%%
\DIFdelFL{29.330 ms }%DIFDELCMD < & %%%
\DIFdelFL{48.340 ms  }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{15 }%DIFDELCMD < & %%%
\DIFdelFL{345.806 ms }%DIFDELCMD < & %%%
\DIFdelFL{371.984 ms }%DIFDELCMD < & %%%
\DIFdelFL{17.027 ms }%DIFDELCMD < & %%%
\DIFdelFL{9.151 ms }%DIFDELCMD < & %%%
\DIFdelFL{26.178 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{16 }%DIFDELCMD < & %%%
\DIFdelFL{585.730 ms }%DIFDELCMD < & %%%
\DIFdelFL{658.493 ms }%DIFDELCMD < & %%%
\DIFdelFL{23.956 ms }%DIFDELCMD < & %%%
\DIFdelFL{48.807 ms }%DIFDELCMD < & %%%
\DIFdelFL{72.763 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{17 }%DIFDELCMD < & %%%
\DIFdelFL{563.755 ms }%DIFDELCMD < & %%%
\DIFdelFL{589.776 ms }%DIFDELCMD < & %%%
\DIFdelFL{16.778 ms }%DIFDELCMD < & %%%
\DIFdelFL{9.243 ms }%DIFDELCMD < & %%%
\DIFdelFL{26.021 ms }%DIFDELCMD < \\ 
%DIFDELCMD < 		%%%
\DIFdelFL{18 }%DIFDELCMD < & %%%
\DIFdelFL{836.377 ms }%DIFDELCMD < & %%%
\DIFdelFL{862.271 ms }%DIFDELCMD < & %%%
\DIFdelFL{16.801 ms }%DIFDELCMD < & %%%
\DIFdelFL{9.093 ms }%DIFDELCMD < & %%%
\DIFdelFL{25.894 ms }%DIFDELCMD < \\ \hline
%DIFDELCMD < 		%%%
\textbf{\DIFdelFL{Avg.}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{437.652 ms}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{477.664 ms}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{19.004 ms}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{21.008 ms}} %DIFAUXCMD
%DIFDELCMD < & %%%
\textbf{\DIFdelFL{40.012 ms}} %DIFAUXCMD
%DIFDELCMD < \\ 
%DIFDELCMD < 	\end{tabular}
%DIFDELCMD < 	\label{Tab:eva_performance}
%DIFDELCMD < \end{table}
%DIFDELCMD < %%%
\DIFdel{Table \ref{Tab:eva_performance} shows the mean duration time of }\DIFdelend \DIFaddbegin \DIFadd{Every test case is executed }\DIFaddend 50 \DIFdelbegin \DIFdel{runs of the test case execution. The second column presents the duration of the reference back end, the other columns are measurements of the solution system. Including the total execution duration of the solution, the duration of the Query Handler and the duration of the Context Model creation. The last column consists of the additional time that the solution systems needs in comparison to the reference system. The last row of the table shows the mean duration over all test cases.
The execution time of the reference back end is directly depending on the spatial and temporal input of the test case. It represents the complexity of the test case processing, since the back end is only processing the data. The increase in execution duration at the solution back end is due to the additional functionality of the Query Handler and the Context Model creation, explained in more detail in the following sections. 
}%DIFDELCMD < \\ 
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{times at each backend. Before each test case execution, the backend gets cleaned up, so that each iteration happens in the same backend conditions. 
}\DIFaddend 

\subsubsection{Performance of Query Handler}\label{Evaluation:impact_perf_query}
This section provides for each element of the \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table a description regarding the performance \DIFdelbegin \DIFdel{constrains.
}\DIFdelend \DIFaddbegin \DIFadd{constraints.
}

\DIFaddend \begin{itemize}
	\item \textbf{dataset\_pid} \\
	The dataset\_pid is taken directly from the parsed filter arguments of EODC and has\DIFdelbegin \DIFdel{therefore }\DIFdelend \DIFaddbegin \DIFadd{, therefore, }\DIFaddend a complexity of $O(1)$. Figure \ref{fig:evaluation_perf_datapid} shows the performance \DIFdelbegin \DIFdel{at the performance evaluation, which is }\DIFdelend \DIFaddbegin \DIFadd{results, which are }\DIFaddend between 4 µs and 26 µs with a median of 8 µs. \DIFaddbegin \DIFadd{The boxplot shows that the duration time of retrieving the dataset PID is, except for a view aberrations, similar between all test case executions and independent of the job configuration.
	}\DIFaddend \begin{figure}[!h]
		\centering
		\caption{\DIFdelbeginFL \DIFdelFL{Box-plot }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Boxplot }\DIFaddendFL of the duration \DIFdelbeginFL \DIFdelFL{time of }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{needed in }\DIFaddendFL the test cases to handle the data \DIFdelbeginFL \DIFdelFL{pid }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{PID }\DIFaddendFL entry. }
		\label{fig:evaluation_perf_datapid}
		\begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [µs]},
		y=2cm,
		ytick={1},
		yticklabels={dataset\_pid},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=4,
			lower quartile=6,
			median=8,
			upper quartile=9,
			upper whisker=26,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	\item \textbf{original} \\
	The \DIFdelbegin \DIFdel{original query is passed by the Query Execution }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Execution}} \DIFaddend component of EODC \DIFdelbegin \DIFdel{. The Query Handler }\DIFdelend \DIFaddbegin \DIFadd{passes the original query. The }\textit{\DIFadd{Query Handler}} \DIFaddend transforms the query into a string. The \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \DIFadd{query }\DIFaddend string has the same size for each query, \DIFaddbegin \DIFadd{and }\DIFaddend just the argument values are exchanged\DIFdelbegin \DIFdel{, therefore }\DIFdelend \DIFaddbegin \DIFadd{. Therefore }\DIFaddend it has a complexity of $O(1)$. The execution time of the test cases is between 24 µs and 98 µs\DIFaddbegin \DIFadd{, }\DIFaddend with a median of 37 µs during the test cases. Figure \ref{fig:evaluation_perf_original} shows the box-plot of all test case executions \DIFdelbegin \DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{and therefore the distribution of duration. It shows that most of the original query retrieval time is in a small range. The duration of the first query execution is constant in time and independent on the job configuration. 
	}\DIFaddend \begin{figure}[!h]
		\centering
		\caption{\DIFdelbeginFL \DIFdelFL{Box-plot }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Boxplot }\DIFaddendFL of the duration time of the test cases to handle the original query entry.}
		\label{fig:evaluation_perf_original}
		\begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [µs]},
		y=2cm,
		ytick={1},
		yticklabels={original},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=24,
			lower quartile=29,
			median=37,
			upper quartile=45,
			upper whisker=98,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	\item \textbf{normalized} \\
	The normalized query is the result of an alphabetical sorting of the parsed filter arguments of EODC. The sorting takes nearly constant duration time, since there are only 4 filter arguments allowed in the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend API version 0.3.1, but they may not all be used. The sorting algorithm has a complexity of $O(n\log{}n)$, where n is the \DIFdelbegin \DIFdel{amount of key }\DIFdelend \DIFaddbegin \DIFadd{number of crucial }\DIFaddend elements in the dictionary \DIFaddbegin \DIFadd{(amount of filter operations)}\DIFaddend . In this evaluation\DIFaddbegin \DIFadd{, }\DIFaddend four filter arguments were used for each test case\DIFdelbegin \DIFdel{, therefore }\DIFdelend \DIFaddbegin \DIFadd{. Therefore }\DIFaddend it has a constant complexity for each test case. In the test cases\DIFaddbegin \DIFadd{, }\DIFaddend it takes between 38 µs and 132 µs with a median of 59 µs of duration time. Figure \ref{fig:evaluation_perf_normalized} shows the box-plot of all test case executions \DIFaddbegin \DIFadd{to visualize that the execution time is not spread widely}\DIFaddend .  
	\begin{figure}[!h]
		\centering
		\caption{\DIFdelbeginFL \DIFdelFL{Box-plot }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Boxplot }\DIFaddendFL of the duration time of the test cases to handle the normalized query entry.}
		\label{fig:evaluation_perf_normalized}
		\begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [µs]},
		y=2cm,
		ytick={1},
		yticklabels={normalized},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=38,
			lower quartile=47,
			median=59,
			upper quartile=75,
			upper whisker=132,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	\item \textbf{norm\_hash} \\
	The performance of the normalized query hash is dependent on the size of the \DIFdelbegin \DIFdel{the }\DIFdelend normalized query ($O(n)$, where n is the size of the normalized query string). In this evaluation\DIFaddbegin \DIFadd{, }\DIFaddend it is constant \DIFdelbegin \DIFdel{, }\DIFdelend since in every test case, there are four filter arguments used, which makes the normalized query have a constant length. In the test cases\DIFaddbegin \DIFadd{, }\DIFaddend the duration is between 15 µs and 54 µs with a median of 20 µs. Figure \ref{fig:evaluation_perf_norm_hash} shows the box-plot of all test case executions. \DIFaddbegin \DIFadd{Except for a view exceptions the duration has a small range of distribution. It shows the distribution of the duration, which is independent on the job configuration. 
	}\DIFaddend \begin{figure}[!h]
		\centering
		\caption{\DIFdelbeginFL \DIFdelFL{Box-plot }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Boxplot }\DIFaddendFL of the duration time of the test cases to handle the normalized query hash entry.}
		\label{fig:evaluation_perf_norm_hash}
		\begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [µs]},
		y=2cm,
		ytick={1},
		yticklabels={norm\_hash},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=15,
			lower quartile=17,
			median=20,
			upper quartile=25,
			upper whisker=54,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	\item \textbf{result\_hash} \\
	\DIFdelbegin \DIFdel{The duration }\DIFdelend \DIFaddbegin \DIFadd{Duration }\DIFaddend of the result hash creation is dependent on the length of the resulting file list. The \DIFdelbegin \DIFdel{sha-256 }\DIFdelend \DIFaddbegin \DIFadd{SHA-256 }\DIFaddend operation has a complexity of $O(n)$, where n is the length of \DIFdelbegin \DIFdel{the }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend resulting file list string. In the test cases\DIFaddbegin \DIFadd{, }\DIFaddend the duration time of the result hash calculation is between \DIFdelbegin \DIFdel{83 }\DIFdelend \DIFaddbegin \DIFadd{28 }\DIFaddend µs and \DIFdelbegin \DIFdel{161 }\DIFdelend \DIFaddbegin \DIFadd{9167 }\DIFaddend µs with a median of \DIFdelbegin \DIFdel{136 }\DIFdelend \DIFaddbegin \DIFadd{51 }\DIFaddend µs. Figure \DIFdelbegin \DIFdel{\ref{fig:evaluation_perf_result_hash} shows the box-plot of the evaluation execution of all test cases. Figure \ref{fig:evaluation_impact_data_resulthash} shows the linear relation between the length of the file list and the duration time of the hash calculation. Table \ref{Tab:data_result_hash}}\DIFdelend \DIFaddbegin \DIFadd{\ref{fig:evaluation_data_resulthash} shows the duration of the result hash creation of the test cases, sorted by the size of the result file set in an ascending way. The data of the chart is in Table \ref{Tab:data_result_hash}, which }\DIFaddend shows the relationship between \DIFaddbegin \DIFadd{the }\DIFaddend number of files and average duration time of the test cases.  

	\begin{table}[]
		\caption{Result hash performance of the test cases depending on the number of result files.}
		\DIFaddbeginFL 

		\centering
		\DIFaddendFL \begin{tabular}{c|c|c}
			\textbf{Test Case} & \textbf{Number of files} & \textbf{Result hash duration [µs]}  \\ \hline
			1 & 14  & 47.244 \\ \hline 
			2 & 24 & 78.047 \\ \hline
			3 & 11 & 37.209 \\ \hline
			4 & 9 & 32.419 \\ \hline
			5 & 10 & 35.000 \\ \hline
			6 & 10 & 35.884 \\ \hline
			7 & 10 & 33.818 \\ \hline
			8 & 12 & 36.886 \\ \hline
			9 & 2255 & 8698.7 \\ \hline
			10 & 17 & 59.359 \\ \hline
			11 & 1551 & 5343.588 \\ \hline
			12 & 12 & 41.548 \\ \hline
			13 & 28 & 95.182 \\ \hline
			14 & 420 & 1400.707 \\ \hline
			15 & 15 & 50.146 \\ \hline
			16 & 1356 & 4985.047 \\ \hline
			17 & 15 & 51.565 \\ \hline
			18 & 54 & 187.186 \\ 
		\end{tabular}
		\label{Tab:data_result_hash}
	\end{table}
	\DIFdelbegin %DIFDELCMD < \begin{figure}[!h]
%DIFDELCMD < 	\centering
%DIFDELCMD < 	%%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Box-plot of the duration time of the test cases to handle the result file list hash entry.}}
	%DIFAUXCMD
%DIFDELCMD < \label{fig:evaluation_perf_result_hash}
%DIFDELCMD < \begin{tikzpicture}
%DIFDELCMD < \begin{axis}
%DIFDELCMD < [
%DIFDELCMD < xlabel={duration [µs]},
%DIFDELCMD < y=2cm,
%DIFDELCMD < ytick={1},
%DIFDELCMD < yticklabels={result\_hash},
%DIFDELCMD < ]
%DIFDELCMD < \addplot+[
%DIFDELCMD < boxplot prepared={
%DIFDELCMD < 	lower whisker=83,
%DIFDELCMD < 	lower quartile=112,
%DIFDELCMD < 	median=136,
%DIFDELCMD < 	upper quartile=161,
%DIFDELCMD < 	upper whisker=1646,
%DIFDELCMD < 	box extend=1,
%DIFDELCMD < 	whisker extend=0.5,
%DIFDELCMD < },
%DIFDELCMD < ] coordinates {};
%DIFDELCMD < \end{axis}
%DIFDELCMD < \end{tikzpicture}
%DIFDELCMD < \end{figure}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin 

	\DIFaddend \begin{figure}[h]
		\centering
		\DIFdelbeginFL %DIFDELCMD < \includegraphics[scale=0.35]{evaluation_impact_data_resulthash}
%DIFDELCMD < 	%%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[scale=0.45]{eva_data_resulthash}
		\DIFaddendFL \caption{Execution duration of the \DIFdelbeginFL \DIFdelFL{sha-256 over the }\DIFdelendFL test cases\DIFaddbeginFL \DIFaddFL{, ascending sorted by result size}\DIFaddendFL .}
		\DIFdelbeginFL %DIFDELCMD < \label{fig:evaluation_impact_data_resulthash} %%%
\DIFdelendFL \DIFaddbeginFL \label{fig:evaluation_data_resulthash} \DIFaddendFL % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
	\end{figure}
	\DIFaddbegin 

	%DIF > 	\begin{figure}[h]
	%DIF > 		\centering
	%DIF > 		\includegraphics[scale=0.35]{evaluation_impact_data_resulthash}
	%DIF > 		\caption{Execution duration of the sha-256 over the test cases.}
	%DIF > 		\label{fig:evaluation_impact_data_resulthash} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
	%DIF > 	\end{figure}
	\DIFaddend \item \textbf{meta\_data} \\
	The \DIFdelbegin \DIFdel{meta }\DIFdelend data in the implementation is the number of result files. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend is measured by the built-in len() operator in python, which has a complexity of $O(1)$ according to the official python description\footnote{https://wiki.python.org/moin/TimeComplexity}. In the test cases\DIFdelbegin \DIFdel{the meta }\DIFdelend \DIFaddbegin \DIFadd{, the }\DIFaddend data calculation takes a duration between 6 µs and 359 µs with a median of 12 µs. Figure \ref{fig:evaluation_perf_meta_data} shows the box-plot of the test case execution.
	\begin{figure}[!h]
		\centering
		\caption{\DIFdelbeginFL \DIFdelFL{Box-plot }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Boxplot }\DIFaddendFL of the duration time of the test cases to handle the meta\_data entry.}
		\label{fig:evaluation_perf_meta_data}	
		\begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [µs]},
		y=2cm,
		ytick={1},
		yticklabels={meta\_data},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=6,
			lower quartile=10,
			median=12,
			upper quartile=17,
			upper whisker=359,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	\item \textbf{Database operations} \\
	To ensure that there are no duplicate query entries in the database, the \DIFdelbegin \DIFdel{Query Handler }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFaddend executes a SQL SELECT statement to check if there is already an entry with the same normalized query hash and the same result hash. In the evaluation\DIFdelbegin \DIFdel{the query }\DIFdelend \DIFaddbegin \DIFadd{, the }\textit{\DIFadd{Query}} \DIFaddend table is empty before the SELECT statement\DIFdelbegin \DIFdel{, hence }\DIFdelend \DIFaddbegin \DIFadd{. Hence }\DIFaddend the complexity is $O(1)$. The INSERT statement to store the query has a complexity of $O(1)$. The database operations in the test cases take between 10.501 ms and 49.283 ms with a median of 13.377 ms duration time. Figure \ref{fig:evaluation_perf_data_database} shows the box-plot of the test case executions.  
	\begin{figure}[!h]
		\centering
		\caption{\DIFdelbeginFL \DIFdelFL{Box-plot }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Boxplot }\DIFaddendFL of the duration time of the test cases to make the needed database operations.}
		\label{fig:evaluation_perf_data_database}	
		\begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [ms]},
		y=2cm,
		ytick={1},
		yticklabels={database operations},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=10.501,
			lower quartile=11.837,
			median=13.377,
			upper quartile=16.680,
			upper whisker=49.283,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture}
	\end{figure}
\end{itemize}

\DIFaddbegin \DIFadd{Figure \ref{fig:eva_data_static} show the duration of the constant query elements of the different test cases. The test cases are sorted by the result size in ascending order.
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{eva_data_static}
	\caption{\DIFaddFL{Constant elements of the }\textit{\DIFaddFL{Query Handler}} \DIFaddFL{calculation in relation to the test cases sorted by result size.}}
	\label{fig:eva_data_static} %DIF >  \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}


\DIFaddend \subsubsection{Performance of Context Model Creation}\label{Evaluation:impact_perf_context}

\begin{enumerate}
	\item \textbf{\DIFdelbegin \DIFdel{Static }\DIFdelend \DIFaddbegin \DIFadd{Constant }\DIFaddend context model elements} \\
	The following elements of the context model are \DIFdelbegin \DIFdel{static}\DIFdelend \DIFaddbegin \DIFadd{constant in duration time}\DIFaddend : the programming language, the input data identifier, the \DIFdelbegin \DIFdel{back end version}\DIFdelend \DIFaddbegin \DIFadd{backend version, }\DIFaddend and the start and end timestamp. These are independent of the job configuration and are read operations with a complexity of $O(1)$. The \DIFdelbegin \DIFdel{static }\DIFdelend \DIFaddbegin \DIFadd{constant }\DIFaddend context model elements in the test cases take a duration time between 26 µs and 122 µs with \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend median of 41.5 µs. Figure \ref{fig:evaluation_perf_static_cm} shows the box-plot of the test case execution.
	\begin{figure}[!h]
		\centering
		\caption{Box-plot of the duration time of the test cases to handle the \DIFdelbeginFL \DIFdelFL{static }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{constant }\DIFaddendFL context model elements.}
		\label{fig:evaluation_perf_static_cm}		
		\DIFdelbeginFL %DIFDELCMD < \begin{tikzpicture}
%DIFDELCMD < \begin{axis}
%DIFDELCMD < [
%DIFDELCMD < xlabel={duration [µs]},
%DIFDELCMD < y=2cm,
%DIFDELCMD < ytick={1},
%DIFDELCMD < yticklabels={Static Elements},
%DIFDELCMD < ]
%DIFDELCMD < \addplot+[
%DIFDELCMD < boxplot prepared={
%DIFDELCMD < 	lower whisker=26,
%DIFDELCMD < 	lower quartile=34,
%DIFDELCMD < 	median=41.5,
%DIFDELCMD < 	upper quartile=47,
%DIFDELCMD < 	upper whisker=122,
%DIFDELCMD < 	box extend=1,
%DIFDELCMD < 	whisker extend=0.5,
%DIFDELCMD < },
%DIFDELCMD < ] coordinates {};
%DIFDELCMD < \end{axis}
%DIFDELCMD < \end{tikzpicture} 
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [µs]},
		y=2cm,
		ytick={1},
		yticklabels={Constant Elements},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=26,
			lower quartile=34,
			median=41.5,
			upper quartile=47,
			upper whisker=122,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture} 
	\DIFaddendFL \end{figure}
	\item \textbf{Dependencies of the programming language} \\
	\DIFdelbegin \DIFdel{The dependencies of the EODC back end are retrieved by a }\DIFdelend \DIFaddbegin \DIFadd{A }\DIFaddend "pip freeze" execution \DIFaddbegin \DIFadd{retrieves the dependencies of the EODC backend}\DIFaddend . It is independent on the complexity of the job, hence has a constant duration time. The duration in the test cases executions is between 92 µs and 289 µs with \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend median of 133 µs. Figure \ref{fig:evaluation_perf_python} shows the box-plot of the test case execution.
	\begin{figure}[!h]
		\centering
		\caption{Box-plot of the duration time of the test cases to retrieve the modules of python.}
		\label{fig:evaluation_perf_python}		
		\begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [µs]},
		y=2cm,
		ytick={1},
		yticklabels={python modules},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=92,
			lower quartile=112,
			median=133,
			upper quartile=150.5,
			upper whisker=289,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture}
	\end{figure}
	\item \textbf{Result hash} \\
	The duration of the \DIFdelbegin \DIFdel{result }\DIFdelend \DIFaddbegin \DIFadd{resulting }\DIFaddend hash is dependent on the length of the resulting image. The \DIFdelbegin \DIFdel{sha-256 }\DIFdelend \DIFaddbegin \DIFadd{SHA-256 }\DIFaddend operation has a complexity of $O(n)$, where n is the size of the output image. In the test cases\DIFaddbegin \DIFadd{, }\DIFaddend the duration time of the result hash calculation is between 1.924 ms and 106.270 ms with a median of 2.521 ms. Table \ref{Tab:result_hash} shows the test cases and their result size \DIFdelbegin \DIFdel{in relation to }\DIFdelend \DIFaddbegin \DIFadd{concerning }\DIFaddend the average duration time of the result hash calculation. In the experiment setup\DIFaddbegin \DIFadd{, }\DIFaddend every 100 kByte of output data resulted in average in an additional duration of 371 µs. The test cases 9, 11, 14\DIFaddbegin \DIFadd{, }\DIFaddend and 16 had the \DIFdelbegin \DIFdel{biggest }\DIFdelend \DIFaddbegin \DIFadd{most prominent }\DIFaddend result file and therefore needed the most time in the result hash calculation.     

	\begin{table}[]
		\DIFaddbeginFL \centering
		\DIFaddendFL \caption{Result hash performance of the test cases depending on the result size.}
		\begin{tabular}{c|c|c}
			\textbf{Test Case} & \textbf{Result size [kByte]} & \textbf{Result hash duration [ms]}  \\ \hline
			1 & 587  & 2.184 \\ \hline 
			2 & 721 & 2.684 \\ \hline
			3 & 600 & 2.233 \\ \hline
			4 & 556 & 2.068 \\ \hline
			5 & 570 & 2.119 \\ \hline
			6 & 553 & 2.058 \\ \hline
			7 & 578 & 2.149 \\ \hline
			8 & 638 & 2.374 \\ \hline
			9 & 27 999 & 104.182 \\ \hline
			10 & 621 & 2.313 \\ \hline
			11 & 7 969 & 29.654 \\ \hline
			12 & 649 & 2.415 \\ \hline
			13 & 1 376 & 5.121 \\ \hline
			14 & 6 026 & 22.422 \\ \hline
			15 & 777 & 2.891 \\ \hline
			16 & 7 978 & 29.685 \\ \hline
			17 & 733 & 2.727 \\ \hline
			18 & 706 & 2.627 \\ 
		\end{tabular}
		\label{Tab:result_hash}
	\end{table}

	\item \textbf{Database UPDATE operation} \\
	\DIFdelbegin \DIFdel{To }\DIFdelend \DIFaddbegin \DIFadd{The job entry gets updated to }\DIFaddend store the context model in the \DIFdelbegin \DIFdel{meta-database, the job entry gets updated. The }\DIFdelend \DIFaddbegin \DIFadd{database by an }\DIFaddend SQL UPDATE statement. It is independent on the job configuration. The duration time at the test case execution is between 3.631 ms and 20.099 ms\DIFaddbegin \DIFadd{, }\DIFaddend with a median of 5.493 ms. Figure \ref{fig:evaluation_perf_database} shows the box-plot of the test case execution. 

	\begin{figure}[!h]
		\centering
		\caption{Box-plot of the duration time of the test cases to retrieve the modules of python.}
		\label{fig:evaluation_perf_database}		
		\begin{tikzpicture}
		\begin{axis}
		[
		xlabel={duration [ms]},
		y=2cm,
		ytick={1},
		yticklabels={UPDATE operation},
		]
		\addplot+[
		boxplot prepared={
			lower whisker=3.631,
			lower quartile=4.595,
			median=5.493,
			upper quartile=7.237,
			upper whisker=20.099,
			box extend=1,
			whisker extend=0.5,
		},
		] coordinates {};
		\end{axis}
		\end{tikzpicture}
	\end{figure}

\end{enumerate}

The \DIFdelbegin \DIFdel{Context Model }\DIFdelend \DIFaddbegin \DIFadd{context model }\DIFaddend creation performance is not affected much by the complexity of the test cases. The captured data for the context model is not related to the complexity of the execution. The "Increase" column of Table \ref{Tab:eva_performance} shows that simple test cases are affected the most in terms of relative performance loss \DIFdelbegin \DIFdel{, because the Query Handler and the Context Model }\DIFdelend \DIFaddbegin \DIFadd{because the }\textit{\DIFadd{Query Handler}} \DIFadd{and the context model }\DIFaddend have elements that need the same duration time independent of the test case configuration. In conclusion\DIFaddbegin \DIFadd{, }\DIFaddend it can be seen that the execution of the \DIFdelbegin \DIFdel{Query Handler and the Context Model }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFadd{and the context model }\DIFaddend is less affecting complex test cases \DIFdelbegin \DIFdel{, }\DIFdelend since there is a minimum duration time needed for the data creation and just a small additional duration time depending on the size for the resulting files. \DIFdelbegin \DIFdel{In addition }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend it should be mentioned that the \DIFdelbegin \DIFdel{Query Handler and the Context Model }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query Handler}} \DIFadd{and the context model }\DIFaddend creation happens after the processing. So that the result files of the job execution can be accessed by the user even if both modules have not finished yet. \DIFaddbegin \\
\DIFadd{Table \ref{Tab:eva_performance} summarizes the result of the mean duration time of the 50 runs of each the test case. The second column presents the duration of the reference backend. The other columns are measurements of the solution system. Including the total execution duration of the solution, the duration of the }\textit{\DIFadd{Query Handler}} \DIFadd{and the duration of the context model creation. The last column consists of the additional time that the solution systems needs in comparison to the reference system. The last row of the table shows the mean duration overall test cases. It shows that the solution adds between 20 ms and 175 ms to the reference backend, depending on the result sizes and not on the execution duration of the processing.
}\DIFaddend 

\DIFaddbegin \begin{table}[]
	\caption{\DIFaddFL{Mean duration time over 50 runs of the solution and the reference backend by executing the test cases}}
	\begin{tabular}{r|r|r|r|r|r}

		\textbf{} & \textbf{\DIFaddFL{Reference Backend}} & \multicolumn{3}{c|}{\textbf{Solution Backend}} &  \\ \hline \textbf{\DIFaddFL{Test}} & \textbf{} & \textbf{\DIFaddFL{Comparison}} & \textbf{\DIFaddFL{Query}} & \textbf{\DIFaddFL{Context}} & \textbf{\DIFaddFL{Solution}} \\ 
		\textbf{\DIFaddFL{Case}} & \textbf{\DIFaddFL{Total}} & \textbf{\DIFaddFL{Total}} & \textbf{\DIFaddFL{Handler}} & \textbf{\DIFaddFL{Model}} & \textbf{\DIFaddFL{Addition}} \\ \hline
		\DIFaddFL{1 }& \DIFaddFL{322.946 ms }& \DIFaddFL{345.127 ms }& \DIFaddFL{14.187 ms }& \DIFaddFL{7.994 ms }& \DIFaddFL{22.181 ms }\\ 
		\DIFaddFL{2 }& \DIFaddFL{369.066 ms }& \DIFaddFL{393.505 ms }& \DIFaddFL{16.342 ms }& \DIFaddFL{8.097 ms }& \DIFaddFL{24.439 ms }\\ 
		\DIFaddFL{3 }& \DIFaddFL{281.657 ms }& \DIFaddFL{305.407 ms }& \DIFaddFL{15.669 ms }& \DIFaddFL{8.081 ms }& \DIFaddFL{23.750 ms }\\ 
		\DIFaddFL{4 }& \DIFaddFL{276.324 ms }& \DIFaddFL{298.954 ms }& \DIFaddFL{14.015 ms }& \DIFaddFL{8.615 ms }& \DIFaddFL{22.630 ms }\\ 
		\DIFaddFL{5 }& \DIFaddFL{312.150 ms }& \DIFaddFL{334.802 ms }& \DIFaddFL{13.925 ms }& \DIFaddFL{8.727 ms }& \DIFaddFL{22.652 ms }\\ 
		\DIFaddFL{6 }& \DIFaddFL{314.571 ms }& \DIFaddFL{337.290 ms }& \DIFaddFL{13.985 ms }& \DIFaddFL{8.734 ms }& \DIFaddFL{22.719 ms }\\ 
		\DIFaddFL{7 }& \DIFaddFL{320.081 ms }& \DIFaddFL{343.552 ms }& \DIFaddFL{14.555 ms }& \DIFaddFL{8.916 ms }& \DIFaddFL{23.471 ms }\\ 
		\DIFaddFL{8 }& \DIFaddFL{304.998 ms }& \DIFaddFL{328.633 ms }& \DIFaddFL{14.742 ms }& \DIFaddFL{8.893 ms }& \DIFaddFL{23.635 ms }\\ 
		\DIFaddFL{9 }& \DIFaddFL{565.289 ms }& \DIFaddFL{740.751 ms }& \DIFaddFL{48.766 ms }& \DIFaddFL{126.696 ms }& \DIFaddFL{175.462 ms }\\ 
		\DIFaddFL{10 }& \DIFaddFL{401.922 ms }& \DIFaddFL{425.026 ms }& \DIFaddFL{14.874 ms }& \DIFaddFL{8.230 ms }& \DIFaddFL{23.104 ms }\\ 
		\DIFaddFL{11 }& \DIFaddFL{521.022 ms }& \DIFaddFL{605.185 ms }& \DIFaddFL{34.660 ms }& \DIFaddFL{49.503 ms }& \DIFaddFL{84.163 ms }\\ 
		\DIFaddFL{12 }& \DIFaddFL{387.536 ms }& \DIFaddFL{412.079 ms }& \DIFaddFL{15.711 ms }& \DIFaddFL{8.832 ms }& \DIFaddFL{24.543 ms }\\ 
		\DIFaddFL{13 }& \DIFaddFL{510.517 ms }& \DIFaddFL{538.784 ms }& \DIFaddFL{17.070 ms }& \DIFaddFL{11.197 ms }& \DIFaddFL{28.267 ms }\\ 
		\DIFaddFL{14 }& \DIFaddFL{657.989 ms }& \DIFaddFL{706.329 ms }& \DIFaddFL{19.010 ms }& \DIFaddFL{29.330 ms }& \DIFaddFL{48.340 ms  }\\ 
		\DIFaddFL{15 }& \DIFaddFL{345.806 ms }& \DIFaddFL{371.984 ms }& \DIFaddFL{17.027 ms }& \DIFaddFL{9.151 ms }& \DIFaddFL{26.178 ms }\\ 
		\DIFaddFL{16 }& \DIFaddFL{585.730 ms }& \DIFaddFL{658.493 ms }& \DIFaddFL{23.956 ms }& \DIFaddFL{48.807 ms }& \DIFaddFL{72.763 ms }\\ 
		\DIFaddFL{17 }& \DIFaddFL{563.755 ms }& \DIFaddFL{589.776 ms }& \DIFaddFL{16.778 ms }& \DIFaddFL{9.243 ms }& \DIFaddFL{26.021 ms }\\ 
		\DIFaddFL{18 }& \DIFaddFL{836.377 ms }& \DIFaddFL{862.271 ms }& \DIFaddFL{16.801 ms }& \DIFaddFL{9.093 ms }& \DIFaddFL{25.894 ms }\\ \hline
		\textbf{\DIFaddFL{Avg.}} & \textbf{\DIFaddFL{437.652 ms}} & \textbf{\DIFaddFL{477.664 ms}} & \textbf{\DIFaddFL{19.004 ms}} & \textbf{\DIFaddFL{21.008 ms}} & \textbf{\DIFaddFL{40.012 ms}} \\ 
	\end{tabular}
	\label{Tab:eva_performance}
\end{table}

\DIFaddend % The result hash is the only element that is dependent on the complexity of the test case. Most of the complexity comes from the temporal extent of the test cases, which do not affect the size of the resulting file. The reason for this is that the output file is always a composite of the values throughout the time range.
%DIF < \todo{ What is the variance across several runs?}
%DIF < \todo{Context Model: mock-up of result hash calculation possible ?}

\subsection*{Storage}\label{Evaluation:impact_stor}
This section describes the storage needed for the captured \DIFdelbegin \DIFdel{meta-data}\DIFdelend \DIFaddbegin \DIFadd{data}\DIFaddend . Since all the captured data is in the PostgreSQL database, the storage needed by it can be estimated using the PostgreSQL interface. Listing \ref{lst:eva_imact_1} presents the commands used to get the size of the \DIFdelbegin \DIFdel{data base }\DIFdelend \DIFaddbegin \DIFadd{database }\DIFaddend entries. The id of the data record is inserted into "''" (e.g. job id in the first one).  

\begin{listing}[ht]
	\begin{minted}[fontsize=\small]{sql}
-- Context Model 
SELECT sum(pg_column_size(context_model)) as filesize, count(*) as filerow 
FROM jobs as t WHERE id = '{}';
-- Query Table
SELECT sum(pg_column_size(t)) as filesize, count(*) as filerow 
FROM query as t WHERE query_pid = '{}';
-- QueryJob Table
SELECT sum(pg_column_size(t)) as filesize, count(*) as filerow 
FROM queryjob as t WHERE job_id = '{}';
	\end{minted}
	\caption{PostgreSQL commands to get the size of one data record in the tables \DIFdelbegin \DIFdel{Job}\DIFdelend \DIFaddbegin \DIFadd{job}\DIFaddend , \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \DIFadd{query }\DIFaddend and \DIFdelbegin \DIFdel{QueryJob}\DIFdelend \DIFaddbegin \DIFadd{queryob}\DIFaddend .}
	\label{lst:eva_imact_1}
\end{listing}	

%\begin{table}[]
%\caption{Mean storage over 50 runs of the solution by executing the test cases. All measured %values are in Bytes.}
%\centering
%\begin{tabular}{r|r|r|r|r}

%\textbf{Test Case} & \textbf{Context Model} & \textbf{Query} & \textbf{QueryJob} & \textbf{Sum}
%\\ \hline
%		1 & 1043 & 1529.659 & 113 & 2685.659 \\ 
%		2 & 1043 & 1530.583 & 113 & 2686.583 \\ 
%		3 & 1043 & 1525.585 & 113 & 2681.585 \\ 
%		4 & 1043 & 1533.649 & 113 & 2689.649 \\ 
%		5 & 1043 & 1524.622 & 113 & 2680.622 \\ 
%		6 & 1043 & 1520.267 & 113 & 2676.267 \\ 
%		7 & 1043 & 1524.800 & 113 & 2680.800 \\ 
%		8 & 1043 & 1527.614 & 113 & 2683.614 \\ 
%		9 & 1043 & 1532.814 & 113 & 2688.814 \\ 
%		10 & 1043 & 1523.184 & 113 & 2679.184 \\ 
%		11 & 1043 & 1530.667 & 113 & 2686.667 \\ 
%		12 & 1043 & 1524.825 & 113 & 2680.825 \\ 
%		13 & 1043 & 1529.923 & 113 & 2685.923 \\ 
%		14 & 1043 & 1525.614 & 113 & 2681.614 \\ 
%		15 & 1043 & 1524.683 & 113 & 2680.683 \\ 
%		16 & 1043 & 1530.422 & 113 & 2686.422 \\ 
%		17 & 1043 & 1524.846 & 113 & 2680.846 \\ 
%		18 & 1043 & 1521.432 & 113 & 2677.432 \\ \hline
%		\textbf{All} & \textbf{1043} & \textbf{1526.955} & \textbf{113} & \textbf{2682.955} \\
%	\end{tabular}
%	\label{Tab:eva_storage}
%\end{table}

The resulting storage need \DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend the evaluation is quite \DIFdelbegin \DIFdel{static}\DIFdelend \DIFaddbegin \DIFadd{constant}\DIFaddend . The mean storage of the 50 runs per test case\DIFaddbegin \DIFadd{, }\DIFaddend and the average storage size of all test cases are calculated. Three parts of the implementation are storing additional information. First\DIFaddbegin \DIFadd{, }\DIFaddend the context model is stored in an additional column named "context\_model" in the \DIFdelbegin \DIFdel{Job }\DIFdelend \DIFaddbegin \textit{\DIFadd{Job}} \DIFaddend table. There is no element in the context model that can vary in size, except for the list of packages of python, which has not changed during the evaluation. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend results in an additional size of 1.043 kByte for each job entry. The same occurs at the \DIFdelbegin \DIFdel{QueryJob }\DIFdelend \DIFaddbegin \textit{\DIFadd{QueryJob}} \DIFaddend table, which \DIFdelbegin \DIFdel{just maps the Query }\DIFdelend \DIFaddbegin \DIFadd{maps the }\textit{\DIFadd{Query}} \DIFaddend table and the \DIFdelbegin \DIFdel{Job }\DIFdelend \DIFaddbegin \textit{\DIFadd{Job}} \DIFaddend table and therefore contains the needed identifiers and timestamps for creation and modification. Every record of the \DIFdelbegin \DIFdel{QueryJob }\DIFdelend \DIFaddbegin \textit{\DIFadd{QueryJob}} \DIFaddend table needs 0.113 kBytes of storage space.
The Query data records have a varying size \DIFdelbegin \DIFdel{, }\DIFdelend because of the string length of the parameters in the original query. The remaining parts of the \DIFdelbegin \DIFdel{Query table are static }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFadd{table are constant }\DIFaddend in storage usage \DIFdelbegin \DIFdel{. Each }\DIFdelend \DIFaddbegin \DIFadd{— each }\DIFaddend Query record of the test cases needs between 1.520 kByte and 1.533 kByte storage. The original query makes up the most size needed in the \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table (e.g. 959 Bytes of 1521.432 Bytes in Testcase 18), because of the XML annotations. In summary\DIFaddbegin \DIFadd{, }\DIFaddend the average additional storage needed by the reference \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend per Job with a new Query entry is 2.677 kBytes. If the used query is already in the \DIFdelbegin \DIFdel{Query }\DIFdelend \DIFaddbegin \textit{\DIFadd{Query}} \DIFaddend table, only an additional 1.043 kBytes of storage is needed by the solution.

\section{Summary}
\DIFdelbegin \DIFdel{This chapter presented the evaluation of the implementation of Chapter \ref{Implementation} at a local machine. It includes the evaluation of the system behavior on data changes at the back end. Therefore, implementing test cases to determine, if the back end has a consistent data identification behavior. The code of the test cases are presented in this chapter and also available on GitHub}\footnote{\DIFdel{https://github.com/bgoesswein/dataid\_openeo/tree/master/}}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{. The }\DIFdelend \DIFaddbegin \DIFadd{The evaluation in this chapter showed how the solution tackles the goals of the research questions. First, it summarizes how the RDA recommendations are implemented in the solution, except for the migration recommendations (R14 and R15). The data identification implementation is then tested against exceptional test cases regarding data updates and data deletions at the backend. The evaluation shows that the solution can re-execute queries properly by returning old versions of updated files. The test cases show that the }\DIFaddend usage of the \DIFdelbegin \DIFdel{query }\DIFdelend \DIFaddbegin \DIFadd{data }\DIFaddend PID as input data \DIFdelbegin \DIFdel{for a different job can lead to misleading results for the user, if the original data has changed and the back end uses new data to replace the missing files. 
In the next section of this chapter, the captured data }\DIFdelend \DIFaddbegin \DIFadd{of a new job is superior to the current way of re-executing a job with the same process graph at EODC.
The reason for this is that the process graph does not have the original execution timestamp and therefore, does not use the same input data after an update occurs. In the evaluation of deleted data at the backend, the solution happens to be not capable of showing the exact missing files, since not the whole file list result of the query is persisted. The test case on job capturing showed that the solution is capable of identifying the backend version and therefore, the environment }\DIFaddend of the job execution\DIFdelbegin \DIFdel{is discussed, by executing a test case on how an job can be executed }\DIFdelend \DIFaddbegin \DIFadd{. Still, to run a new job }\DIFaddend in the same \DIFdelbegin \DIFdel{way as the original execution. The data to re-execute the job is captured, but the reproduction of the old back end is not done in an automatic manner and is up to the EODC back end provider. In the last section of this chapter, the impact of the solution on the EODC back end is evaluated. Due to the local environment of the evaluation setup, }\DIFdelend \DIFaddbegin \DIFadd{environment, EODC has to provide it manually. The evaluation also contains a section about the performance and storage impact on the EODC backend, by running 18 test cases derived from past publications that used data from EODC. The results show that, except for the result hashes, }\DIFaddend the \DIFdelbegin \DIFdel{outcome is limited to relative changes on performance, by comparing the solution with an reference back end. The solution back end has a lower impact on complex test cases, due to the high impact of the execution duration of the processing. Other test cases need up to a fifth more time for the }\DIFdelend \DIFaddbegin \DIFadd{calculation of the data identification and the context model are independent of the complexity of the job. The time of the result hashing used for the data identification is dependent on the size of the query result, and the resulting hash of the context model is dependent on the size of the output file of the job }\DIFaddend execution. The \DIFdelbegin \DIFdel{additional storage needed by the solution per execution is almost static and can be estimated good by the back end provider. The results of this chapter lead to further discussion in the next chapter, which is the conclusion and future work of the thesis}\DIFdelend \DIFaddbegin \DIFadd{evaluation of storage impact results in the conclusion that the additional needed space per job is unrelated to the job configuration. It depends on if there is a new query entry added to the database or not}\DIFaddend .

\chapter{Conclusion and future Work}\label{Conclusion}

\section{Conclusion}
In this thesis, the challenges of providing reproducibility in earth observation science have been explored. The solution consists of data identification and code identification for \DIFdelbegin \DIFdel{work-flows }\DIFdelend \DIFaddbegin \DIFadd{workflows }\DIFaddend in the context of geosciences. The implementation within the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project proves the concept defined in Section \ref{Design}. It makes it clear that reproducibility can be achieved in earth observation \DIFdelbegin \DIFdel{work-flows }\DIFdelend \DIFaddbegin \DIFadd{workflows }\DIFaddend by providing data identification and process identification \DIFaddbegin \DIFadd{to existing backends}\DIFaddend . One reason why earth observation researchers lack \DIFdelbegin \DIFdel{on }\DIFdelend reproducibility is the additional time needed to achieve it. With the provided solution\DIFaddbegin \DIFadd{, }\DIFaddend researchers can run experiments on \DIFdelbegin \DIFdel{EODC }\DIFdelend \DIFaddbegin \DIFadd{their backend of choice }\DIFaddend and automatically generate a data identifier that other scientists can use \DIFdelbegin \DIFdel{directly }\DIFdelend in their research. \DIFdelbegin \DIFdel{In addition the }\DIFdelend \DIFaddbegin \DIFadd{Besides, }\DIFaddend scientists can cite the version of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend and describe the execution environment. \DIFdelbegin \DIFdel{On the other hand, reproducible processing is not a prior feature of the back end providers at the moment, therefore the solution had to be simple to apply to existing systems. The suggested solution consists of light-weight recommendations to the back end design. The provided meta-data in the presented prototype }\DIFdelend \DIFaddbegin \DIFadd{The captured data in the context model }\DIFaddend is capable of reproducing experiments\DIFdelbegin \DIFdel{theoretically, but are }\DIFdelend \DIFaddbegin \DIFadd{. Nevertheless, it is }\DIFaddend not supported by the EODC \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend in an automated fashion. \DIFdelbegin \DIFdel{The design introduces a feature to compare different job executions to bring more transparency to the users. Applying the suggested additions is an advantage for back end providers, since they can provide execution information to scientists. Furthermore, the concept of easy to apply data citation and re-use may attract more users to use the back ends that provide the features of the concept. }\DIFdelend In the case of the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project, the \DIFdelbegin \DIFdel{development of it will lead to adoptions in the future. Therefore, the prototype of this thesis can be seen as a starting point of reproducibility in the OpenEO projectrather than a final solution}\DIFdelend \DIFaddbegin \DIFadd{solution concludes in suggestions to openEO for all components of the project}\DIFaddend .  

\subsection{Research questions revisited}\label{research question revisited}

This section revisits the research questions defined in Section \ref{research question} to discuss how the concept design \DIFdelbegin \DIFdel{suit }\DIFdelend \DIFaddbegin \DIFadd{suits }\DIFaddend the questions.

\begin{itemize}
	\item \textbf{How can an earth observation job re-execution be applied like the initial execution?}
	\begin{itemize}
		\item \textbf{How can the used data be identified after the initial execution?} \\
		The input data is defined by the satellite data identifier and the filter operations used in the \DIFdelbegin \DIFdel{work-flow}\DIFdelend \DIFaddbegin \DIFadd{workflow}\DIFaddend . This information with the hash of the resulting file list defines a \DIFdelbegin \DIFdel{unique }\DIFdelend \DIFaddbegin \DIFadd{single }\DIFaddend query identified by a \DIFdelbegin \DIFdel{pid}\DIFdelend \DIFaddbegin \DIFadd{PID}\DIFaddend . The query data record consists of the original execution \DIFdelbegin \DIFdel{time-stamp }\DIFdelend \DIFaddbegin \DIFadd{timestamp }\DIFaddend to enable the retrieval of the same data versions. By assigning persistent identifiers to every unique \DIFdelbegin \DIFdel{used }\DIFdelend input data, the data can be accessed after the \DIFdelbegin \DIFdel{initial }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution.   
		\item \textbf{How can the used software of the initial execution be reproduced?} \\
		The original software is defined by the used code, the programming language\DIFaddbegin \DIFadd{, }\DIFaddend and the installed libraries. \DIFdelbegin \DIFdel{To identify the used code a }\DIFdelend \DIFaddbegin \DIFadd{A }\DIFaddend version control system is needed \DIFaddbegin \DIFadd{to identify the code}\DIFaddend . In the implementation\DIFaddbegin \DIFadd{, }\DIFaddend Git and GitHub \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend used to identify and persist the used code version. The version of the programming language and the installed packages are stored in the job dependent context model. \DIFdelbegin \DIFdel{To reproduce the initial execution the back end }\DIFdelend \DIFaddbegin \DIFadd{The backend }\DIFaddend has to recreate the captured versions of code and state of the programming language \DIFaddbegin \DIFadd{to reproduce the first execution}\DIFaddend .       
		\item \textbf{What data has to be captured when?} \\
		The input data has to be captured at the query execution of the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend to ensure the equality of the persisted query and the \DIFdelbegin \DIFdel{original }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution. The capturing of the code version has to be executed at the time of the execution, as well as the environment information of the execution. The hash of the resulting output file is calculated right after the creation of it.   
		\item \textbf{How can the result of a re-execution in future software versions be verified?} \\
		The solution persists a hash of the resulting output file. The result hash value of the re-execution is compared with the original execution output hash. If the hash values are not equal\DIFaddbegin \DIFadd{, }\DIFaddend it can be assumed that the result of the re-execution is different from the \DIFdelbegin \DIFdel{initial }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend execution.  
	\end{itemize}
	\item \textbf{How can the equality of an earth observation job re-execution results be validated?}
	\begin{itemize}
		\item \textbf{What are the validation requirements?} \\
		The validation requirements are the equality of the captured data. \DIFdelbegin \DIFdel{There are three parts that }\DIFdelend \DIFaddbegin \DIFadd{Three parts }\DIFaddend have to be considered by the validation. First\DIFaddbegin \DIFadd{, }\DIFaddend the persistent input data identifier has to be the same between \DIFaddbegin \DIFadd{the }\DIFaddend two executions. Second the code version. Third the hash value of the output file.
		\item \textbf{How can the data be compared?} \\
		The data is compared \DIFdelbegin \DIFdel{by }\DIFdelend \DIFaddbegin \DIFadd{to }\DIFaddend the equality of all components of the context model. Every item of the context model is compared with the same item of the re-execution context model. There are only three states at the comparison defined: "EQUAL" or the difference of the context model element.
		\item \textbf{How can changes of the earth observation \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend environment be recognized?} \\
		The timestamps of the \DIFdelbegin \DIFdel{back end provenance version is }\DIFdelend \DIFaddbegin \DIFadd{backend provenance version are }\DIFaddend used to determine the version of the \DIFdelbegin \DIFdel{back end. Changes on }\DIFdelend \DIFaddbegin \DIFadd{backend. Changes in }\DIFaddend job dependent environment \DIFdelbegin \DIFdel{results }\DIFdelend \DIFaddbegin \DIFadd{result }\DIFaddend in different entries at the job dependent context model.   
		\item \textbf{How can differences in the environment between the executions be discovered?} \\
		The users can compare two job executions at the \DIFdelbegin \DIFdel{back end. The }\DIFdelend \DIFaddbegin \DIFadd{backend. A }\DIFaddend result is a JSON object that consists of all elements of the context model with the result of the comparison. In the resulting JSON\DIFaddbegin \DIFadd{, }\DIFaddend it is evident, which part of the environment differs between the executions and how they are different.
	\end{itemize}
\end{itemize}

\section{Future Work}\label{FutureWork}
This thesis proposed a prototype for applying reproducibility in the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project. Still\DIFdelbegin \DIFdel{there are major }\DIFdelend \DIFaddbegin \DIFadd{, there are significant }\DIFaddend issues to be addressed in the development of the \DIFdelbegin \DIFdel{OpenEO project. 
OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO project. 
openEO }\DIFaddend is still evolving until the official end of the project in 2021 and will be continued \DIFdelbegin \DIFdel{afterwards. There are a lot of features that }\DIFdelend \DIFaddbegin \DIFadd{afterward. Many features }\DIFaddend have to be defined and may interfere with the concept proposed in this thesis. User defined functions (UDF) are part of the project, but not well defined yet. Reproducing executed UDFs may lead to adaptations of the concept. Another main issue of the \DIFdelbegin \DIFdel{OpenEO }\DIFdelend \DIFaddbegin \DIFadd{openEO }\DIFaddend project is the billing concept that has to be apply-able to a diverse set of \DIFdelbegin \DIFdel{back ends}\DIFdelend \DIFaddbegin \DIFadd{backends}\DIFaddend . Since the capturing process and the generation of the context model needs resources, it has to be considered in the billing plan of the \DIFdelbegin \DIFdel{back ends}\DIFdelend \DIFaddbegin \DIFadd{backends}\DIFaddend . The growing diversity of the \DIFdelbegin \DIFdel{back ends applying the OpenEO }\DIFdelend \DIFaddbegin \DIFadd{backends applying the openEO }\DIFaddend standard is a huge challenge to remain the design of the context model suitable in the future. \DIFdelbegin %DIFDELCMD < \\
%DIFDELCMD < %%%
\DIFdel{The validation of the result with the hash of the result may lead to high duration times, so a better solution for validating the resulting file need to be developed. }\DIFdelend \DIFaddbegin \DIFadd{The concept needs to be improved by implementing it on other backend types. }\\
\DIFaddend In the solution of this work\DIFdelbegin \DIFdel{the meta-data is captured for identifying }\DIFdelend \DIFaddbegin \DIFadd{, the data is captured to identify }\DIFaddend how jobs are executed and to be able to identify the provenance. A step further is a way to \DIFdelbegin \DIFdel{automatically }\DIFdelend apply the reproduction of an already executed job \DIFaddbegin \DIFadd{automatically}\DIFaddend . A finer granularity of the process capturing is \DIFdelbegin \DIFdel{an }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend possible addition to the concept, leading to a definition of scalability of the processing capturing. A concept to achieve this is presented in \DIFdelbegin \DIFdel{the }\DIFdelend Section \ref{Job:Benchmarking} below.

\subsection{Benchmarking Mode}\label{Job:Benchmarking}

So far\DIFaddbegin \DIFadd{, }\DIFaddend only the capturing of the whole process chain, including input and output data, are described. The general idea (presented in Section \ref{Implementation:Job dependent provenance}) of the job dependent provenance capturing is to capture the input data of the process graph, the whole process graph and the output of the process graph. \DIFdelbegin \DIFdel{This is a common concept the back end }\DIFdelend \DIFaddbegin \DIFadd{It is a universal concept the backend }\DIFaddend providers can agree on \DIFdelbegin \DIFdel{, }\DIFdelend since it is not affecting the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend providers implementations much. Moreover, it is capable of giving the user a simple overview of how the results are generated and what the differences between job executions are. \DIFdelbegin \DIFdel{To }\DIFdelend \DIFaddbegin \DIFadd{The granularity of the capturing can be improved to }\DIFaddend make the comparison of different execution behaviors more informative\DIFdelbegin \DIFdel{, the granularity of the capturing can be improved}\DIFdelend . \\
Therefore, the \DIFdelbegin \DIFdel{bench marking }\DIFdelend \DIFaddbegin \DIFadd{benchmarking }\DIFaddend context model gets introduced. The idea is that not only the input data of the whole process chain and the output data gets captured, but every data state in between \DIFdelbegin \DIFdel{of }\DIFdelend each process. For every process in the process graph the input data, the output data\DIFaddbegin \DIFadd{, }\DIFaddend and the code executing the process get captured. \DIFdelbegin \DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{It }\DIFaddend makes it easier for users to see where in the process chain the execution \DIFdelbegin \DIFdel{actually }\DIFdelend produced different results.\\ The concept leads to higher implementation and execution costs \DIFdelbegin \DIFdel{at the back end }\DIFdelend \DIFaddbegin \DIFadd{on the backend }\DIFaddend side. The effect on the performance of the execution is higher than the \DIFdelbegin \DIFdel{common }\DIFdelend \DIFaddbegin \DIFadd{standard }\DIFaddend context model. Whether the implementation of such \DIFdelbegin \DIFdel{a }\DIFdelend granularity is doable is highly dependent on the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend implementation. Not every \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend might be able to implement this into the \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend due to external tools where the execution and results of single processes are not distinguishable. If a \DIFdelbegin \DIFdel{back end }\DIFdelend \DIFaddbegin \DIFadd{backend }\DIFaddend can support it, the granularity can be \DIFdelbegin \DIFdel{finer}\DIFdelend \DIFaddbegin \DIFadd{more exceptional}\DIFaddend . In an extreme example, the data and code can be captured for every line in the code. The flexibility of capturing granularity may define the context model design in the future.

\chapter{Appendix}\label{Appendix}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{appendix_pidB}
	\caption{Original query of jobB from the evaluation in Section \ref{Evaluation:special_dataid}.}
	\label{fig:appendix_pidB} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{appendix_pidD}
	\caption{Original query of jobD from the evaluation in Section \ref{Evaluation:special_dataid}.}
	\label{fig:appendix_pidD} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\backmatter


% Use an optional list of figures.
\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
\cleardoublepage % Start list of tables on the next empty right hand page.
\listoftables % Starred version, i.e., \listoftables*, removes the toc entry.



%\renewcommand\listoflistingscaption{List of source codes}
\listoflistings



% Use an optional list of alogrithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
\printindex

% Add a glossary.
\printglossaries

% Add a bibliography.
\bibliographystyle{abbrv}
\bibliography{intro}

\end{document}